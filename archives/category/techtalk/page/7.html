<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="ru-RU">

<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<title>about NetApp   &raquo; techtalk</title>

<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/style.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/print.css" type="text/css" media="print" />
<link rel="alternate" type="application/rss+xml" title="about NetApp RSS Feed" href="../../../../feed" />
<link rel="pingback" href="../../../../xmlrpc.php.html" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 2.6" />

</head>

<body>
<div id="page">
<div id="header">
	<div id="headertitle">
		<h1><a href="../../../../index.html" title="about NetApp: Системы хранения данных как предмет разговора">about NetApp</a></h1>
		<p>Системы хранения данных как предмет разговора</p>
	</div> 
	<!-- Search box (If you prefer having search form as a sidebar widget, remove this block) -->
	<div class="search">
		<form method="get" id="searchform" action="../../../../index.html">
<input type="text" size="20" name="s" id="s" value="Поиск..."  onblur="if(this.value=='') this.value='Поиск...';" onfocus="if(this.value=='Поиск...') this.value='';"/>
</form>
	</div> 
	<!-- Search ends here-->
		
</div>

<div id="navbar">
<ul id="nav">
	<li><a href="../../../../index.html">Home</a></li>
	<li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul>
</div>
<div id="wrapper">

	<div id="content">

	
			<p>Archive for the &#8216;techtalk&#8217; Category.</p>

 				
		<div class="navigation">
			<div class="alignleft"><a href="http://blog.aboutnetapp.ru/archives/category/techtalk/page/8">&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="6.html">Next Entries &raquo;</a></div>
		</div>

						
			<div class="post" id="post-975">
				<h2 class="posttitle"><a href="../../../975/trackback.html" rel="bookmark" title="Permanent Link to RAID-5, RAID-6 или RAID-10?">RAID-5, RAID-6 или RAID-10?</a></h2>
				<div class="postmetadata">25 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Я уже не раз в этом блоге <a href="../../../345/trackback.html">касался</a> темы различных типов RAID, и того, как выбор между ними влияет на показатели надежности использующей их системы в целом.</p>
<p>Недавно попалась на глаза интересная <a href="http://www.linkedin.com/groups/RAID-risk-calculator-93470.S.61385307?view=&amp;srchtype=discussedNews&amp;gid=93470&amp;item=61385307&amp;type=member&amp;trk=eml-anet_dig-b_pd-pmt-cn">дискуссия</a>, в которой приводились следующие данные.</p>
<p>Допустим мы имеем массив из 20 дисков SATA по 1TB (без учета необходимых для RAID дисков mirror и parity) , скорость ребилда у которого – 50MB/s, и который заполнен данными на 75%.</p>
<p>Тогда вероятность потери данных (именно <strong>потери данных</strong>, не просто отказа отдельного диска) из за выхода из строя дисков в RAID составляет, по годам эксплуатации:</p>
<p>Year 1: </p>
<p>RAID-5 - 13.76%    <br />RAID-10 - 0.078%     <br />RAID-6 - 0.516% </p>
<p>Year 2: </p>
<p>RAID-5 - 25.6%    <br />RAID-10 - 0.156%     <br />RAID-6 - 1.03% </p>
<p>Year 3: </p>
<p>RAID-5 - 36.86%    <br />RAID-10 - 0.23%     <br />RAID-6 - 1.54 </p>
<p>Year 5: </p>
<p>RAID-5 - 53.30%    <br />RAID-10 - 0.38%     <br />RAID-6 - 2.56%</p>
<p>Раз уж мы находимся в блоге посвященном решениям NetApp, то не могу не отметить, что в случае использования RAID-DP, который хотя и является формально RAID-6, но вышеприведенные данные для него будут ближе к значениям RAID-10, так как важную роль в увеличении MTTDL (Mean Time To Data Loss – ожидаемое время до момента потери данных) играет скорость ребилда, на время которого, и до его окончания, показатели надежности любого RAID снижены, и которая, в случае RAID-DP, будет значительно выше (а время восстановления – короче), чем у “канонического” RAID-6.</p>
<p>Например в документе <a href="http://media.netapp.com/documents/tr-3574.pdf">TR-3574</a> (пусть вас не смутит его “прикладной” заголовок про Exchange 2007, строго говоря работа эта совсем мало прикладная, а, в значительной мере, научная, по крайней мере по дотошности своего подхода) приводится такой расчет:</p>
<table border="0" cellspacing="0" cellpadding="2" width="600">
<tbody>
<tr>
<td valign="top" width="148"><strong>RAID type</strong></td>
<td valign="top" width="214" align="center"><strong>Probability of Data Loss in 5 Years</strong></td>
<td valign="top" width="236" align="center"><strong>Risk of Data Loss Relative to RAID-DP</strong></td>
</tr>
<tr>
<td valign="top" width="149">RAID-10 (1 data disk)</td>
<td valign="top" width="213" align="center">0,33%</td>
<td valign="top" width="236" align="center">163</td>
</tr>
<tr>
<td valign="top" width="150">RAID-5 (7 data disks)</td>
<td valign="top" width="213" align="center">6%</td>
<td valign="top" width="235" align="center"><font color="#ff0000"><strong>3955</strong></font></td>
</tr>
<tr>
<td valign="top" width="150">RAID-6 (7 data disks)</td>
<td valign="top" width="213" align="center">0,002%</td>
<td valign="top" width="235" align="center">1,0</td>
</tr>
<tr>
<td valign="top" width="150">RAID-DP (7 data disks)</td>
<td valign="top" width="213" align="center">0,002%</td>
<td valign="top" width="235" align="center">1,0</td>
</tr>
</tbody>
</table>
<p>RAID-5 на 7 дисках данных (7d+1p) почти <strong>в четыре тысяч раз</strong> менее надежен, чем RAID-6, на тех же 7 дисках данных (7d+2p)!</p>
<p>Отсюда вы сами сможете ответить на часто возникающий вопрос, что более выгодно с точки зрения надежности: две группы RAID-5, допустим, по 5+1, или же одна RAID-6 10+2. Как вы видите, надежность RAID-6 в данном случае выше<strong> на порядки</strong>, даже не более длинной группе<strong>.</strong></p>
<p>&#160;</p>
<p>Не забывайте, в ряде случаев Mean Time To Data Loss может равняться <em>Mean Time To Job Loss</em> :)</p>
<p>&#160;</p>
<p>PS: Если захотите углубиться самостоятельно в дебри расчетов и в тему надежности в RAID, то, кроме вышеуказанной <a href="http://media.netapp.com/documents/tr-3574.pdf">TR-3574</a>, могу также порекомендовать прочитать научную работу, опубликованную на прошлогоднем <a href="http://www.usenix.org/event/hotstorage10/">USENIX Hot Storage’10</a>: <a href="http://www.usenix.org/event/hotstorage10/tech/full_papers/Greenan.pdf">Mean time to meaningless - MTTDL, Markov models, and storage system reliability</a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/raid.html" rel="tag">raid</a>, <a href="../../../tag/raid-5.html" rel="tag">raid-5</a>, <a href="../../../tag/raid-dp.html" rel="tag">raid-dp</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../975/trackback.html#comments" title="Комментарий к записи RAID-5, RAID-6 или RAID-10?">Комментарии (10)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-971">
				<h2 class="posttitle"><a href="../../../971/trackback.html" rel="bookmark" title="Permanent Link to Что означает сообщение FCP Partner Path Misconfigured (и что с ним делать)?">Что означает сообщение FCP Partner Path Misconfigured (и что с ним делать)?</a></h2>
				<div class="postmetadata">14 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Я обратил внимание, что, довольно часто, пользователи сталкиваются с ошибкой конфгурации, которая индицируется в логах сообщением: <strong>FCP Partner Path Misconfigured</strong>. Несмотря на то, что ошибка эта довольно банальна, и исправление вызвавших ее причин тривиально, я заметил, что у многих пользователей возникают проблемы с диагностикой, да и вообще с пониманием того, что именно вызывает эту ошибку.</p>
<p>Поэтому я взял на себя труд перевести на русский прекрасно написанную статью из Knowledge Base с сайта NetApp, в которой подробно рассматривается эта ошибка, причины ее вызывающие, и методы устранения.</p>
<h2>Что означает сообщение FCP Partner Path Misconfigured</h2>
<p><a href="https://kb.netapp.com/support/index?page=content&amp;id=3010111&amp;actp=LIST_POPULAR">https://kb.netapp.com/support/index?page=content&amp;id=3010111</a></p>
<p>KB ID: 3010111 Version: 7.0    <br />Published date: 04/29/2011 </p>
<h3>Проблема</h3>
<p>AutoSupport message: FCP PARTNER PATH MISCONFIGURED</p>
<h3>Сообщения Syslog и EMS</h3>
<p><code>[hostname: scsitarget.partnerPath.misconfigured:error]: FCP Partner Path Misconfigured.      <br />[hostname: scsitarget.partnerPath.misconfigured:error]: FCP Partner Path Misconfigured - Host I/O access through a non-primary and non-optimal path was detected.</code></p>
<p>   <a href="../../../971/trackback.html#more-971" class="more-link"><small>Continue reading &#8216;Что означает сообщение FCP Partner Path Misconfigured (и что с ним делать)?&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/configuration.html" rel="tag">configuration</a>, <a href="../../../tag/fcp.html" rel="tag">fcp</a>, <a href="../../../tag/kb.html" rel="tag">kb</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/partner-path.html" rel="tag">partner path</a><br />					Раздел: <a href="../../howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../971/trackback.html#comments" title="Комментарий к записи Что означает сообщение FCP Partner Path Misconfigured (и что с ним делать)?">Комментарии (3)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-951">
				<h2 class="posttitle"><a href="../../../951/trackback.html" rel="bookmark" title="Permanent Link to Еще немного про autotiering">Еще немного про autotiering</a></h2>
				<div class="postmetadata">13 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Проглядывая <a href="http://community.netapp.com">community.netapp.com</a> обнаружил <a href="http://communities.netapp.com/message/51884">дискуссию о autotiering-е</a>, откуда выдернул интересное мнение уже известного вам Dimitris K. (recoverymonkey). Хотя в оригинале это были три реплики-ответа в дискуссии в форуме, я слил их оформил их как отдельную “статью”.</p>
<p>Дискуссия идет о реализации autotiering в EMC FAST, а также о системах хранения Compellent, которые, до недавнего времени, были главным игроком на рынке tiering-а, и реализация прозрачного tiering-а в них была сделана ранее всех. В России они почти неизвестны, хотя сейчас, как я понимаю, они могут начать попадать в страну через каналы Dell.</p>
<p><em>Dimitris Kriekouvkas (<a href="http://www.recoverymonkey.org">recoverymonkey</a>), сотрудник NetApp:</em></p>
<p>Autotiering это отличная концепция, но она также очень новая, и результаты ее работы не проверены и не подтверждены на подавляющем большинстве реальных нагрузок.</p>
<p>Посмотрите на опубликованные бенчмарки EMC – там нигде нет autotiering.</p>
<p>Вы также не найдете и показателей FASTcache. Все бенчмарки у EMC делаются на традиционных RAID-группах, без пулов.</p>
<p>Если вы посмотрите на руководство наилучших практик по производительности и доступности для EMC CLARiiON (ссылку на этот документ я давал в прошлом посте про &#8220;матчасть&#8221;), то вы увидите следующее:</p>
<ul>
<li>Вместо RAID-5 для больших пулов на SATA рекомендуется RAID-6 с размером группы в 10-12 дисков. </li>
<li>Thin Provisioning снижает производительность </li>
<li>Storage pools снижают производительность по сравнению с Traditional RAID </li>
<li>Данные и ввод-вывод не распределяются на все диски пула, как вы, возможно, предполагали (см <a href="http://virtualeverything.wordpress.com/2011/03/05/emc-storage-pool-deep-dive-design-considerations-caveats/">ссылку</a>). </li>
<li>Рекомендуется использовать drive ownership на только один контроллер </li>
<li>Нельзя смешивать разные типы RAID в одном пуле </li>
<li>Существуют ограничения по расширению пула дисками, в идеале расширение должно производиться увеличивая емкость пула вдвое (или, хотя бы, кратно количеству дисков в RAID-группе пула) </li>
<li>Для пула недоступны reallocate/rebalancing (для MetaLUN вы можете сделать restripe) </li>
<li>Процесс Tresspassing pool LUN (обычно при переключении LUN-а с одного контроллера на другой, например при выходе одного из них из строя, но, часто, и при других операциях) может приводить к снижению производительности, так как оба контроллера будут пытаться совершать операции ввода-вывода на LUN. Поэтому для pool LUN-ов важно оставаться закрепленными за тем контроллером, на котором они начали работать, иначе потребуется затратная миграция. </li>
<li>Не рекомендуется использовать thin LUN для задач, требующих высокой производительности по bandwidth. </li>
</ul>
<p>Я хочу еще раз привлечь внимание к простому факту: дьявол кроется в деталях. Читайте мелкий шрифт.</p>
<p>Вам говорят: Autotiering волшебным образом, автоматически, решит ваши проблемы, без вашего участия, не беспокойтесь об этом. В реальности все не совсем так.</p>
<p>При работе autotiering, значительная часть вашего working set, рабочего набора данных, находящегося в активном использовании в данный момент, должно быть перемещено на быстрое хранилище.</p>
<p>Допустим у вас есть хранилище ваших данных, емкостью 50TB. Правило оценки, которым руководствуются инженеры EMC,&#160; что 5% рабочего набора данных пользователя – “горячие данные”. Они перемещаются на SSD (в виде tier или cache). Таким образом вам нужно 2,5TB usable space на SSD, или примерно одна полку дисками SSD по 200GB, может быть больше, в зависимости от типа использованного RAID.</p>
<p>Принято считать, что объем “средней” нагрузки составляет 20% от объема, то есть 10TB, который размещается на дисках SAS или FC.</p>
<p>Остальное размещается на дисках SATA.</p>
<p><strong>Вопрос на 10 миллионов долларов:</strong></p>
<p>Что обойдется дешевле, autotiering и софт кэширования (он небесплатен) плюс 2,5TB SSD, плюс 10TB SAS, плюс 37,5TB SATA, или…</p>
<p>50TB SATA плюс NetApp FlashCache,или, например, 50TB SAS и Flash Cache?</p>
<p><strong>Вопрос на 20 миллионов долларов:</strong></p>
<p>Какая из этих двух конфигураций будет иметь более предсказуемую производительность?</p>
<p>&#160;</p>
<p>Compellent – это еще одна интересная история.</p>
<p>Большинство обсуждающих Compellent не задумывается о том, что tiering-у у него подвергаются “снэпшоты”, а не непосредственно рабочие данные!</p>
<p>То есть принцип там такой: берется снэпшот, данные делятся на страницы, размеров 2MB (по умолчанию, может быть меньше, но тогда нельзя будет увеличить емкость хранилища). Далее, если оценивается, что обращений на чтение с данной страницы мало, то она переносится на уровень SATA.</p>
<p>О чем не знают большинство интересующихся Compellent-ом:</p>
<p>Если вы изменяете содержимое данных на странице, то происходит это следующим образом:</p>
<ol>
<li>Перенесенная на SATA страница, содержащая данные, которые мы изменяем, остается на SATA.</li>
<li>Новая страница, объемом 2MB создается на Tier1 (SAS/FC), куда реплицируется содержимое страницы с SATA, и где делается запись изменения. Даже если меняется в данных один байт.</li>
<li>Когда с этой страницы будет сделан снэпшот, то он, в свою очередь, также может быть впоследствии перенесен на SATA, заменив прежнюю.</li>
<li>??того: 4MB занятого места для того, чтобы сохранить 2MB данных и один измененный байт.</li>
</ol>
<p>?? снова укажу: дьявол кроется в деталях. Если вы изменяете свои данные произвольным образом (random), вы получите множество “заснэпшоченных” страниц, и очень неэффективное использование пространства. Вот почему я всегда предупреждаю пользователей, которые интересуются Compellent-ом, задавать эти вопросы, и уяснить себе эти моменты, получив ясное описание от инженеров того, как используется пространство снэпшотов.</p>
<p>На NetApp мы имеем предельно гранулярное пространство, благодаря WAFL. Минимально возможный снэпшот по своему объему очень мал (это указатель, немного метаданных, плюс один измененный блок, размером 4KB). Поэтому на NetApp некоторые наши пользователи могут хранить сотни тысяч&#160; снэпшотов на одной системе (например именно так делает один всем известный банк, использующий наши системы хранения).</p>
<p>&#160;</p>
<p>Гранулярность, на самом деле, это только часть проблемы (производительность – другая ее часть). Сейчас страница у Compellent имеет размер 2MB (можно уменьшить до 512K, но это не позволит изменять размер стораджа). Если они перейдут, как обещают, на 64-битную арифметику в ПО, то они смогут получит&#160; размер страницы 64K (это пока не подтверждено), однако тут есть вот какая проблема. Запись этой страницы на RAID может быть двумя способами.</p>
<p>Если это RAID-1, то тогда мы записываем две копии страницы, размером 64KB на каждый из дисков.</p>
<p>Если это RAID-5/6, то тогда нам надо поделить объем записываемой страницы, размером 64KB, поровну между всеми дисками данных, входящих в RAID. Допустим используется RAID-5 в варианте 4d+1p. Тогда на каждый диск в операции записи получится всего 16KB (и меньше). ?? если для RAID-1 размер записи в 64KB это довольно типичный размер записи сегмента в RAID, и запись таким размером достаточно производительна, то для RAID-5/6 это очень маленький кусочек для операции записи, что будет неизбежно отражаться на производительности.</p>
<p>В Data ONTAP мы не перемещаем снэпшоты, поэтому у нас нет такой проблемы, у других же вендоров она очень остра. Получить предсказуемую производительность при использовании autotiering это очень, очень сложная задача. Всякий раз когда мы у клиентов меняем нашими системами сторадж от Compellent, это происходит не потому что им не хватает каких-то фич, а только оттого, что у клиентов с ними проблемы с производительностью. Всякий раз.</p>
<p>Мы ставим 1-2 вида дисков плюс Flash Cache, и проблема решена (в большинстве случаев производительность становится в 2-3 раза выше, как минимум). Часто это получается даже не дороже.</p>
<p>Вот такие дела.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/autotiering.html" rel="tag">autotiering</a>, <a href="../../../tag/compellent.html" rel="tag">compellent</a>, <a href="../../../tag/fast.html" rel="tag">fast</a>, <a href="../../../tag/fastcache.html" rel="tag">fastcache</a>, <a href="../../../tag/flash-cache.html" rel="tag">flash cache</a>, <a href="../../../tag/ssd/index.html" rel="tag">ssd</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../951/trackback.html#respond" title="Комментарий к записи Еще немного про autotiering">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-959">
				<h2 class="posttitle"><a href="../../../959/trackback.html" rel="bookmark" title="Permanent Link to Multi-path High Availaility (MPHA) FAQ">Multi-path High Availaility (MPHA) FAQ</a></h2>
				<div class="postmetadata">11 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Я давно собирался написать большой и подробный пост на тему что такое, для чего используется и как применяется режим подключения дисковых полок MPHA в системах NetApp, так как вокруг этой темы имеется, как я заметил, ряд принципиальных непониманий.</p>
<p>Однако мне на прошедшей и этой неделе пока некогда, поэтому пока позвольте представить вашему вниманию перевод официального FAQ на тему MPHA, от NetApp, а свои мысли на этот счет я оставлю до времени, когда я смогу подробнее “посидеть над столом” и сформулировать их.</p>
<p>Пока же – официальная позиция NetApp на тему использования MPHA в системах хранения NetApp.</p>
<p>   <a href="../../../959/trackback.html#more-959" class="more-link"><small>Continue reading &#8216;Multi-path High Availaility (MPHA) FAQ&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/cabling.html" rel="tag">cabling</a>, <a href="../../../tag/mpha.html" rel="tag">mpha</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/troubleshooting.html" rel="tag">troubleshooting</a><br />					Раздел: <a href="../../howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../959/trackback.html#comments" title="Комментарий к записи Multi-path High Availaility (MPHA) FAQ">Комментарии (4)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-950">
				<h2 class="posttitle"><a href="../../../950/trackback.html" rel="bookmark" title="Permanent Link to Учу матчасть :)">Учу матчасть :)</a></h2>
				<div class="postmetadata">7 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как заповедали старшие – изучаю вооружение потенциального противника :)</p>
<p>Нашел тут <a href="http://www.emc.com/collateral/hardware/white-papers/h5773-clariion-best-practices-performance-availability-wp.pdf">EMC CLARiiON Best Practices for Performance and Availability</a> (FLARE 30, 01.03.2011) и сижу, читаю.</p>
<p>Особо примечательные места выделяю. Ничего, что я сегодня без перевода? По-моему, в выделенном все достаточно понятно.</p>
<p>Уделяю особое внимание storage pool-ам и LUN-ам в них, так как только в pool-ах возможны новые фишечки, такие как FAST и thin provisioning.</p>
<p>   <a href="../../../950/trackback.html#more-950" class="more-link"><small>Continue reading &#8216;Учу матчасть :)&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/clariion.html" rel="tag">clariion</a>, <a href="../../../tag/emc/index.html" rel="tag">emc</a>, <a href="../../../tag/fast.html" rel="tag">fast</a>, <a href="../../../tag/pool.html" rel="tag">pool</a>, <a href="../../../tag/matchast.html" rel="tag">матчасть</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;" rel="category tag">цитаты</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../950/trackback.html#comments" title="Комментарий к записи Учу матчасть :)">Комментарии (19)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-942">
				<h2 class="posttitle"><a href="../../../942/trackback.html" rel="bookmark" title="Permanent Link to Project Mercury &ndash; эксперименты в области кэширования во flash">Project Mercury &ndash; эксперименты в области кэширования во flash</a></h2>
				<div class="postmetadata">30 Июнь 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Любопытная <a href="http://www.theregister.co.uk/2011/06/17/netapp_project_mercury/">статья</a> обнаружилась в веб-журнале The Register.</p>
<p>На конференции FAST’11 NetApp читал доклад о <a href="http://www.usenix.org/events/fast11/posters_files/Byan.pdf">экспериментах в рамках проекта Mercury</a>, где исследовалась интересная модель использования flash-памяти для кэширования данных серверов приложений.    <br />Аналогичным работами также занимаются и другие участники “топа вендоров”, например о Project Lightning недавно на своей конференции рассказывал EMC, кроме этого <a href="http://www.theregister.co.uk/2011/06/15/dell_app_servers_getting_flash/">аналогичные эксперименты</a> проделывал Dell, активно прорывающийся в “самую высшую лигу”.</p>
<p><img style="background-image: none; border-bottom: 0px; border-left: 0px; margin: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image104.png" width="409" height="366" /></p>
<p>Суть эксперимента состояла в возможнеости связать кэш во flash memory на сервере приложений, с централизованным дисковым хранилищем, чтобы позволить виртуальным машинам использовать сетевой сторадж не теряя преимуществ от использования объемного локального кэша на самом сервере.</p>
<p>Отмечу, что пока речь идет лишь о концепции, научной работе и экспериментах, и до воплощения в коммерческом продукте пока неблизко. Но идея интересная.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/cache.html" rel="tag">cache</a>, <a href="../../../tag/flash/index.html" rel="tag">flash</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a><br />					Раздел: <a href="../../review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>,  <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;" rel="category tag">цитаты</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../942/trackback.html#respond" title="Комментарий к записи Project Mercury &ndash; эксперименты в области кэширования во flash">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-936">
				<h2 class="posttitle"><a href="../../../936/trackback.html" rel="bookmark" title="Permanent Link to EMC FASTcache и NetApp Flash Cache">EMC FASTcache и NetApp Flash Cache</a></h2>
				<div class="postmetadata">23 Июнь 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как мне тут не раз уже попеняли, некорректно сравнивать <em>tiering-as-datamoving</em> и <em>tiering-as-caching</em>, то есть, например, NetApp Flash Cache и EMC FAST VP. Допустим, как я ни старался в <a href="../../../926/trackback.html">соответствующей статье</a>, я вас не убедил, что обе эти формы повышения эффективности системы хранения для пользователя есть суть одно. </p>
<p>Хорошо, давайте рассмотрим отдельно особенности, достоинства и недостатки только Flash Cache (PAM-II) и FASTcache.</p>
<p>Во первых, конечно, вы бы могли вместе со мной поехдничать о извилистом пути достижения цели у EMC. Сперва превратить flash-память в &quot;диски&quot; в форме SSD, а затем из этих дисков эмулировать &quot;память&quot; кэша. Ну ладно, допустим не смогли напрямую, оказалось проще через &quot;двойную эмуляцию&quot;.</p>
<p>Во-вторых если мы посмотрим на то, как у EMC предполагается использовать SSD диски под FASTcache, вы, уверен, вместе со мной поразитесь неффективности.</p>
<p>Допустим, мы разворачиваем систему хранения для 1000 рабочих мест под десктопную виртуализацию на XenDesktop. <a href="http://itzikr.wordpress.com/2011/06/08/citrix-xendesktopp-5-on-emc-vnx-match-made-in-heaven-part1/">Рекомендуемая схема</a> включает в себя три диска SSD, из которых один - hotspare, а два других образуют &quot;зеркало&quot; RAID-1. Таким образом, очевидно, что эффективность использования flash в такой конструкции будет примерно 33%, то есть одна треть от купленной емкости flash. Да, при увеличении объема FASTcache, кроме роста цены будет расти и эффективность использования, но она никогда не превысит 50%, за счет использования RAID-1 (плюс hotspare). Больше половины затраченных на SSD денег будут простаивать. По контрасту, если вы покупаете 256GB Flash Cache, вы можете использовать под кэширование и ускорение работы с данными все 256GB, сто процентов от затраченных на них денег.</p>
<p>В третьих, стоит обратит внимание, что использование SSD как дисков вынуждает EMC разместить этот кэш &quot;снаружи&quot; контроллера, в &quot;петле&quot; дискового ввода-вывода на интерфейсе SAS. В то время, как у NetApp плата Flash Cache располагается непосредственно на системной шине контроллера, на PCIe (PCIe v2.0 x8 в моделях 3200/6200, пропускная способность 32Gbit/s в каждом направлении). То есть взаимодействие контроллера с кэшем в случае FASTcache такое: данные пришли через ввод-вывод на контроллер, по каналу SAS к дискам, вышли через другой порт и записались на SSD по интерфейсу SAS. Затем, если к данным кэша обращение, они должны считаться через дисковый канал ввода-вывода по SAS обратно в контроллер, и отдаться через третий канал ввода-вывода, собственно инициатору запроса, по FC или iSCSI, или NFS/CIFS. Все это, безусловно, нагружает и так не бесконечные возможности дискового канала ввода-вывода к полкам, и, потенциально, может привести к ограничению производительности.</p>
<p>Наконец, стоит помнить, что, хотя в FASTcache удалось значительно снизить размер оперируемого &quot;чанка&quot; до 64KB, против гигабайта в FAST-&quot;просто&quot;, все же этот размер достаточно велик для многих задач, работающих в random read/write, размер блока кэша, значительно превышающий рабочий для соответствующей файловой системы или задачи, например блока базы данных, также снижает эффективность использования такого кэша, если, допустим, только 4KB из блока в 64KB нам действительно нужны (при 100% random это довольно обычная ситуация), то это значит, что эффективность кэша составляет лишь 1/16 от своего фактического объема.</p>
<p>Что же в плюсах? Очевидно, что это активно &quot;педалируемая&quot; EMC возможность работы такого кэша на запись. Особенно на это нажимают в сравнении с NetApp Flash Cache, который на запись не работает, и эта возможность действительно производит впечатление на тех людей, которые не особенно разбираются как там у NetApp все устроено, и знают только то, что &quot;что-то иметь это гораздо лучше чем не иметь&quot;, и уж все знают, что запись надо кэшировать, без кэширования запись на диски очень медленная, это знает даже начинающий пользователь, впервые покупающий кэш-контроллер в сервер.</p>
<p>Чем прежде всего занимается кэш на запись?   <br />Давайте рассмотрим на примере.</p>
<p>Когда клиент записывает блок данных на систему хранения, то, при наличии кэша на запись, этот блок максимально быстро в него помещается, и клиенту отдается сообщение “OK, принято”, клиент не ждет физического помещения блока в сектор жесткого диска, а может продолжать работать, словно записываемый блок уже записан, например отправить следующий блок.    <br />Запись ускоряется путем ее буферизации, и, в дальнейшем, сортировкой и упорядочением процессов записи.</p>
<p>Хотя современный жесткий диск и позволяет обращаться к произвольным фрагментам записанной на него информации, чем отличается от, например, магнитной ленты, которая позволяет считывать и записывать ее только последовательно, однако не позволяет эту произвольно размещенную (random) информацию считывать и записывать _одновременно_, поскольку это физически ограничено возможностями магнитных головок диска.</p>
<p>Если у нас есть жесткий диск, и мы не используем кэширование, то три клиента, пишущих на этот диск, будут вынуждены выстроиться в очередь. Сперва диск спозиционирует головки и дождется подхода нужного сектора для записи блока данных клиента A, а пославшие на запись свой блок клиенты B и C будут вынуждены ждать, затем головки будут переставлены в новое место, диск дождется, когда мимо головок проедет блок, который требуется перезаписать клиенту B, и перезапишет его, а если за это время у клиента A появился еще один блок на запись, то он поставится в очередь следом за блоком клиента C, и будет ожидать, пока выполнятся все операции перед ним.</p>
<p>Все эти процессы, как вы понимаете, механические, неспешны, и занимают не микросекунды, как операции в памяти, а миллисекунды, иногда десятки миллисекунд. Несмотря на то, что жесткий диск - устройство произвольной записи, эти произвольные записи не могут быть осуществлены одновременно.</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fca970c-pi" /></p>
<p>Ради создания хотя бы иллюзии одновременности и организуется кэш на запись. Приложения записывают свои данные в кэш записи, получают сразу же ответ &quot;готово&quot; и идут сочинять новые блоки на запись, не дожидаясь того, что данные физически поместятся на диск.</p>
<p>Кэш же внутри себя удерживает блок, ожидая подходящего момента на запись, а также оптимизирует записи, с тем, чтобы уменьшить &quot;пробег&quot; магнитных головок по диску, и возможно оптимальным образом перекомпонует записи так, чтобы уложить их максимально эффективным, с точки зрения head seek-а, способом.</p>
<p>Принципиальное отличие WAFL тут состоит в том, что WAFL не перезаписывает блоки уже записанные на диске, и значит при записи клиенты гораздо меньше ожидают seek-а. Вы помните, что в WAFL записи можно провести &quot;чохом&quot;, в выделенный сегмент, а не переписывать по одному блоку, мечась по всему диску, здесь и там, ожидая, когда подъедет под головку тот или иной блок. Поэтому даже без традиционного кэширования записи на WAFL клиентские записи быстро оказываются на дисках.</p>
<p>Строго говоря, большой кэш означает, что, используя транспортную аналогию, записи быстро встают в очередь на посадку, но совсем не значит, что при этом они быстро сядут и поедут.</p>
<p>WAFL оптимизирован именно на минимальное время от момента прихода данных &quot;на остановку&quot; и до входа их &quot;в автобус&quot; жесткого диска, превращая записи в записи последовательного порядка (sequental) из поступающих записей в произвольном порядке (random).</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef0133f2735a39970b-pi" width="467" height="307" /></p>
<p>Результат хорошо иллюстрируется <a href="http://blogs.netapp.com/dropzone/2010/07/does-your-data-wait-in-line.html">экспериментом</a>, в котором один aggregate, состоящий из трех дисков SATA 1TB 7200rpm, в RAID-DP (2p+1d), то есть, фактически, из одного действующего диска, показывает при random write блоком 4KB не типичные для SATA 70-80 IOPS, а более 4600!</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fe9970c-pi" /></p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485987007970c-pi" /></p>
<p>Объяснение этому простое - записи, поступающие на диск теряют свою &quot;рандомность&quot;, и &quot;секвентализируются&quot;. Около четырех с половиной тысяч IOPS random-записи на один диск SATA - это как раз то, отчего в системах NetApp нет свойственной для &quot;классических систем&quot; острой необходимости в кэше записи на уровне контроллера.</p>
<p>Таким образом запись действительно надо кэшировать для &quot;классических&quot; систем, да, безусловно, это так. Но это совсем не так безусловно для &quot;неклассических&quot; дисковых структур, используемых, например, в NetApp.</p>
<p>Вот почему NetApp Flash Cache не используется на запись данных, работая всем своим объемом только на чтение. Вот почему необходимость кэширования записи для WAFL не столь безоговорочна, как для <em>&quot;классических&quot;</em> дисковых структур.</p>
<p>Потому, что в WAFL необходимость обязательного кэширования данных при записи существенно ниже, чем для &quot;традиционных&quot; систем. </p>
<p>Это позволило, среди прочего, кстати, значительно упростить алгоритмическую составляющую процесса кэширования, ведь не секрет, что правильная обработка кэширования на запись часто создает значительные проблемы, особенно в наиболее эффективном режиме write-back.</p>
<p>А это, в свою очередь, снизило количество возможных ошибок (как программного кода реализации, так и ошибок при использовании) удешевило устройство и упростило использование.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/cache.html" rel="tag">cache</a>, <a href="../../../tag/emc/index.html" rel="tag">emc</a>, <a href="../../../tag/fastcache.html" rel="tag">fastcache</a>, <a href="../../../tag/flash/index.html" rel="tag">flash</a>, <a href="../../../tag/flashcache.html" rel="tag">flashcache</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/ssd/index.html" rel="tag">ssd</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../936/trackback.html#comments" title="Комментарий к записи EMC FASTcache и NetApp Flash Cache">Комментарии (25)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-926">
				<h2 class="posttitle"><a href="../../../926/trackback.html" rel="bookmark" title="Permanent Link to Про tiering: EMC FAST, FASTcache, NetApp Flash Cache">Про tiering: EMC FAST, FASTcache, NetApp Flash Cache</a></h2>
				<div class="postmetadata">26 Май 2011, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Несколько постов назад, в комментах, разгорелась нешуточная дискуссия о том, можно ли считать Flash Cache “истинно православным” средство tiering-а, и ставить его в ряд с множеством других аналогичных средств, например с EMC FAST.    <br />Для разъяснения моей позиции на этот счет я и написал этот пост.</p>
<p>Для начала давайте определим что такое tiering вообще.</p>
<p><strong>Tiering</strong>-ом (увы, русскоязычного термина пока не прижилось, будем использовать такое) принято называть механизм перемещения данных между “уровнями” (tiers) хранения, характеризующимися теми или иными свойствами, например ценой, быстродействием, защищенностью, и так далее. Обычно tiering-ом принято называть механизмы, для перемещения данных между дисками разных типов, или дисками и магнитными лентами, или же ROM-хранилищем, часто он используется для организации ILM – Information Lifecycling Management – хранилища данных в соответствии с их статусом и уровнями QoS.</p>
<p>Но давайте отвлечемся от физической реализации, и посмотрим на задачу с функциональной точки зрения, с точки зрения пользователя или приложения, использующего систему хранения.</p>
<p><strong>Что есть tiering с точки зрения приложения или пользователя? Зачем мы его применяем?</strong></p>
<p>Целью tiering-а для пользователя является возможность повысить эффективность (в первую очередь экономическую) использования его системы хранения. Когда “цена значения не имеет”, то все просто <strike>– надо купить Symmetrix</strike>. Однако в реальной жизни цена очень даже имеет значение, и пользователь вынужден идти на компромиссы между ценой решения и необходимой пользователю производительностью.</p>
<p>??спользуемые в системах хранения диски сегодня имеют различную производительность, емкость и цену, причем производительность и емкость обычно величины обратно пропорциональные: больше емкость – меньше производительность (SATA), меньше емкость – выше производительность&#160; (SAS), еще выше производительность и цена, и меньше емкость – Flash. Система хранения-же целом характеризуется соотношением <strong>IOPS/$</strong>, то есть количества единиц производительности с “вложенного доллара”.&#160; Повысить этот параметр стремится любой вендор и любой покупатель системы хранения.</p>
<p>Согласно широкоизвестному <a href="http://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%9F%D0%B0%D1%80%D0%B5%D1%82%D0%BE">Закону Парето</a> (“20 процентов сотрудников делает 80 процентов всей работы”) сравнительно небольшая часть данных ответственна за значительную долю быстродействия системы. Напротив, значительная часть данных относится к так называемым “холодным”данным, скорость доступа к которым в принципе не критична.     <br />Было бы неплохо, если бы эти 20% активных данных, скорость доступа к которым напрямую влияет на общую производительность, располагались на максимально быстром разделе хранилища, пусть он дорогой, но его емкость будет всего 20% от емкости хранилища, зато прирост мы получим во все 80%!</p>
<p>??менно эта идея лежала в основе идеи tiering-а. Если этот тип данных – активный, то автоматически перенесем его на более дорогие и быстродействующие диски, и повысим производительность доступа к ним, а менее активные данные – перенесем на менее производительные дешевый тип дисков. ?? настанет счастье, в виде повысившегося соотношения IOPS/$.</p>
<p>К такому, “классическому” типу tiering-а относится продукт EMC FAST – (Fully-Automated Storage Tiering). Он позволяет прозрачно для пользователя переносить фрагменты его данных между “уровнями” хранилища, например между разделами на дисках SAS и SATA.</p>
<p><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image99.png" width="186" height="318" /></p>
<p>Увы, дьявол кроется в деталях, и “всегда читайте написанное мелким шрифтом”. Первая версия FAST была весьма сырой, например, позволяла переносить только LUN-ы целиком, что, очевидно, неприемлемый на практике уровень “гранулярности” данных. Только в FASTv2 появилась возможность суб-LUN-овой миграции, впрочем, по-прежнему, мигрируемый минимальный фрагмент данных весьма велик (1GB!), да и еще окружен множеством ограничений использования (не поддерживается компрессия для данных, которые подлежат миграции, например).</p>
<p>К тому же, к сожалению, принадлежность данных к той или иной группе не фиксирована. Так, бухгалтерские проводки за прошлый квартал могут лежать “холодными”, до момента составления квартального или годового отчета, когда обращение к ним резко увеличится. Значит пока мы не распознаем и не смигрируем ставшие активными данные на быстрое хранилище, мы рискуем значительно терять в быстродействии, а если наш алгоритм выявления активных данных не обладает достаточным быстродействием, мы рискуем и вовсе не получить прироста, если “пики” доступа окажутся менее “разрешающей способности” анализирующего алгоритма, и не будут им распознаны как активность. Это же относится и к “устойчивости к помехам”, так, например, регулярный бэкап может свести с ума алгоритм анализа активности, ведь к данным обращаются каждую ночь!</p>
<p>Как вы видите, внешне очевидная и тривиальная задача анализа активности данных и переноса в соответствии с ними на те или иные типы дисков, обрастает сложностями.</p>
<p>Однако, почему задачу повышения быстродействия доступа к данным можно решать только лишь физическим перемещением данных с диска на диск?&#160; Вспомним про механизм, который называется “кэширование”. При кэшировании, копии данных, к которым осуществляется активный доступ, накапливаются в специальном высокоскоростном пространстве, доступ куда значительно быстрее (впрочем, см. выше, оно и значительно дороже, этим ограничивается расширение его размера), когда же активность доступа к данным падает (читай: снижается необходимость в этих данных), эти копии удаляются из кэша, при этом собственно содержимое данных по прежнему сохраняется в сравнительно недорогом и&#160; малопроизводительном, по сравнению с кэшем, “основном хранилище”.</p>
<p>Большой плюс метода кэширования заключается, во-первых, в полной “автоматизации процесса”, ни пользователю, ни его программе не надо предпринимать какие-то дополнительные действия при его использовании. Если они часто читают этот участок данных, он оказывается в кэше, и скорость доступа к нему повышается, если они перестают его читать, то он вытесняется более активными данными, и остается только в основном хранилище.</p>
<p><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image100.png" width="182" height="343" /></p>
<p>Во-вторых, в область дорогого, но высокопроизводительного хранения – “кэша” – попадают “по определению” только “горячие” данные. Место в кэше всегда занимают только данные, к которым сейчас идет активное обращение. Все 100% дорогостоящего объема кэша используются для повышения производительности системы, а не просто “хранения”. Следствием этого является более высокая эффективность работы кэша. Процессор Intel Xeon имеет всего 8Mb кэша, что не мешает ему работать с в тысячи раз большими объемами ОЗУ на сервере, и, за счет этого, сравнительно небольшого, по относительной величине, кэша, эффективно ускорять доступ к размещенным в обширной памяти данным.</p>
<p>В третьих – процесс ускорения доступа и улучшения результата IOPS/$ путем кэширования есть, с алгоритмической точки зрения, процесс тривиальный. А раз так, он имеет минимум побочных эффектов и ограничений использования, а работать (и давать результат в виде повышения производительности) начинает сразу же, а не когда накопится статистика использования и, наконец, в соответствии с ней произойдет миграция данных с одних дисков на другие.</p>
<p>Минусы? Ну как же в нашей жизни и без минусов. Эффективность кэша, действительно высокая при чтении, сильно падает на операциях записи. Ведь если мы изменяем данные в их копиях в кэше, нам надо регулярно и своевременно обновлять их непосредственно по месту их размещения (этот процесс имеет название “сброса содержимого кэша” или “flush”). Значит, если мы изменяем содержимое блока, нам надо это изменение распознать и переписать его содержимое в основном хранилище, чтобы, когда блок будет вытеснен по неактивности из кэша, его новое, измененное содержимое не потерялось. Кэширование записи, хоть и ускоряет собственно процесс записи, сильно усложняет ситуацию алгоритмически, и, как следствие, может вести к значительному снижению эффективности работы.</p>
<p>Таким образом, взглянув на задачу “со стороны пользователя”, мы видим два эквивалентных по результатам решения. Оба решают задачу в нужном пользователю русле, а именно, распределяют данные по “уровням” хранения с различной ценой и производительностью, в зависимости от их активности, улучшая, в результате, экономическую эффективность хранилища, повышая производительность и снижая затраты, то есть улучшая уже названный выше ключевой экономический&#160; параметр хранилища – <strong>IOPS/$</strong>.</p>
<p>Да, действительно, “метод кэширования” имеет в основе другой механизм, чем “метод переноса”, но если результат для пользователя – тот же самый: изменение характеристик доступа к данным, ведущее к улучшению параметра IOPS/$ и эффективности хранилища, то <em>“неважно, какого цвета кошка, если она хорошо ловит мышей”</em>. Будет ли это tiering как перенос данных между двумя типами дисков, или в виде переноса между диском и кэшем, для пользователя и его задач это, по большому счету, безразлично, если это дает эквивалентный прирост эффективности.</p>
<p>Вот почему я считаю, что как EMC FAST, так и EMC FASTcache и NetApp FlashCache, все они являются формами организации tiering-а, и их можно рассматривать вместе, как <em>формы</em> одного решения.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/emc/index.html" rel="tag">emc</a>, <a href="../../../tag/fast.html" rel="tag">fast</a>, <a href="../../../tag/fastcache.html" rel="tag">fastcache</a>, <a href="../../../tag/flashcache.html" rel="tag">flashcache</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/tiering.html" rel="tag">tiering</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../926/trackback.html#comments" title="Комментарий к записи Про tiering: EMC FAST, FASTcache, NetApp Flash Cache">Комментарии (38)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-881">
				<h2 class="posttitle"><a href="../../../881/trackback.html" rel="bookmark" title="Permanent Link to Откуда вообще берется фрагментация?">Откуда вообще берется фрагментация?</a></h2>
				<div class="postmetadata">21 Апрель 2011, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Так что же там на самом деле происходит с фрагментацией на NetApp?</p>
<p>Давайте, для начала, разберемся с основами. Я в этом блоге уже не раз писал о проблеме фрагментации, и желающих сошлю в более ранние посты, например <a href="../../../221/trackback.html">сюда</a> и <a href="../../../225/trackback.html">сюда</a>.</p>
<p>К сожалению тема фрагментации данных на WAFL до сих пор остается в некотором роде <a href="http://dic.academic.ru/dic.nsf/enc3p/333174">эзотерическим</a> знанием. Связано это, в первую очередь с тем, как мне представляется, что рассказ о фрагментации и необходимом противодействии ему, затрагивает ряд чувствительных особенностей функционирования WAFL, деталях, которые NetApp, по разным причинам (совсем не обязательно злонамеренным), разглашать пока не хочет.</p>
<p>Поэтому официальная позиция состоит в рекомендации, в случае, если влияние фрагментации в вашем конкретном случае, проявляется негативно (она, кстати, может и не проявляться), то включать wafl reallocate ( <code>reallocate on / reallocate start [/vol/volname]</code>) и спать счастливо.</p>
<p>Вообще же FUD вокруг <em>non-contiguous wafl blocks allocation</em> построен (впрочем, как и почти любой FUD) вокруг плохого представления технических деталей процесса и иногда чистосердечного, а чаще злонамеренного “непонимания” этих деталей. Давайте, для начала, разберем как же записываются данные в WAFL, по крайней мере на том уровне, на котором нам этот процесс показывает NetApp.</p>
<p>Но начать придется издалека.</p>
<p>   <a href="../../../881/trackback.html#more-881" class="more-link"><small>Continue reading &#8216;Откуда вообще берется фрагментация?&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/fragmentation.html" rel="tag">fragmentation</a>, <a href="../../../tag/fud/index.html" rel="tag">fud</a>, <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/wafl/index.html" rel="tag">wafl</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../881/trackback.html#comments" title="Комментарий к записи Откуда вообще берется фрагментация?">Комментарии (12)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-879">
				<h2 class="posttitle"><a href="../../../879/trackback.html" rel="bookmark" title="Permanent Link to Что такое SnapVault и куда это применить?">Что такое SnapVault и куда это применить?</a></h2>
				<div class="postmetadata">14 Апрель 2011, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>В новых Software Bundles, которые приходят новым покупателям систем NetApp часто включены лицензии на SnapVault (пример – Windows Bundle для распродажных FAS2020, о которых я <a href="../../../831/trackback.html">писал ранее</a>). Однако, к сожалению, всё еще не все хорошо представляют себе как это работает, как может применяться, и чем может быть полезно.</p>
<p>SnapVault это средство D2D Backup для систем хранения NetApp (а в случае использования Open Systems SnapVault – OSSV – и для обычных серверов), использующее их возможности по созданию и удаленному хранению снэпшотов.</p>
<p>Как вы знаете, в отличие от всех прочих систем хранения, использующих то, что у них также называется “снэпшоты”, NetApp не только не возражает, но даже поощряет хранить снэпшоты долговременно, непосредственно на дисках. Дело в том, что хранение снэпшотов на дисках, как своеобразной локальной “резервной копии”, вместе с собственно данными, не приводит к падению производительности всей системы хранения, как это происходит с COW (Copy-On-Write) “снэпшотами”. Поэтому на всех системах, кроме NetApp, снэпшоты используются лишь как “временное хранилище” состояния данных. Создали снэпшот в ночной тиши слабозагруженного датацентра, быстро слили на ленты или другое архивное хранилище, и скорее удалить.</p>
<p><a href="/pics//netapp-emc-snapshots.jpg"><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="NetApp-EMC-snapshots" border="0" alt="NetApp-EMC-snapshots" src="/pics//netapp-emc-snapshots-thumb.jpg" width="345" height="262" /></a></p>
<p>Не все <strike>йогурты</strike> снэпшоты одинаково полезны. :)</p>
<p>Но в случае NetApp вы можете локально хранить десятки, если не сотни снэпшотов, и почти мгновенно с них восстановиться в случае сбоя. Однако разумные требования безопасности рекомендуют не хранить резервную копию вместе с оригинальными данными, во избежание ее повреждения вместе с оригинальным датасетом (например при физическом повреждении системы хранения).    <br />Это не означает, что ее нельзя там хранить совсем, но локально сохраненная копия по крайней мере не должна быть единственной резервной копией ваших данных!</p>
<p>Было бы разумно если и хранить копию локально, для максимально быстрого восстановления данных, то отдельный набор – удаленно, на тот нечастый, но возможный случай, когда локальная копия будет повреждена, например вместе со всем хранилищем.</p>
<p>Однако копирование снэпшотов “наружу”, на внешний массив, обычным образом, лишает нас всех преимуществ механизма снэпшотов. Такая “обычная копия” займет место, равное физической емкости всего набора данных, для каждого скопированного снэпшота, а не привычного нам для снэпшотов “разностного” объема, когда каждый снэпшот, видясь как индивидуальный набор данных, занимает только разницу в измененных блоках, в сравнении с предшествующим ему снэпшотом.</p>
<p>Было бы неплохо и копировать снэпшоты наружу как-то “по-снэпшотному”, сохраняя эту их эффективность использования места.    <br />??менно это и делает SnapVault.     <br />SnapVault – это способ хранить снэпшоты рабочих данных на внешнем хранилище, но “по-снэпшотному”, экономя пространство хранения.</p>
<p>Лицензии на SnapVault существуют в двух “ролях”. Одна называется <em>SnapVault Primary</em>, а вторая – <em>SnapVault Secondary</em>.</p>
<p><strong>SnapVault Primary</strong> – это лицензия, которая нужна для системы, служащей <em>источником данных</em>, это та система хранения, на которой хранятся непосредственно “боевые” данные, копию которых мы хотим сохранить.</p>
<p><strong>SnapVault Secondary</strong> – это лицензия, устанавливаемая на <em>получателя резервных копий</em>. На систему, которая хранит резервные копии от одной или нескольких SnapVault Primary систем.</p>
<p>В качестве SnapVault Primary может также работать так называемый <strong>Open Systems SnapVault (OSSV)</strong> – программный продукт (его для NetАpp написала компания Syncsort), устанавливаемый непосредственно на сервера под управлением Windows/Linux/etc и позволяющий им играть роль SnapVault Primary систем, делать на них снэпшоты (разумеется с определенными ограничениями, присущими то или иной OS), передавать их на хранение на систему SnapVault Secondary (В роли SV-Sec может быть только NetApp, нет OSSV Secondary), и восстанавливать их с таких снэпшотов, при необходимости.</p>
<p>Как и для чего можно применить SnapVault?</p>
<p>Например можно централизованно хранить резервные копии данных от множества отделений основного бизнеса, где, очень часто, нет достаточного бюджета и ресурсов для организации полноценного бэкап-решения. В случае использования SnapVault вам нужно только раз настроить расписание на удаленных системах и политики хранения резервных копий на центральном хранилище (это может быть выделенная система, или же пространство на основной системе хранения NetApp головной компании, с лицензией SV-Secondary на ней. А с использованием OSSV это можно делать и без систем NetApp в качестве Primary. ?? в дальнейшем резервные копии от всех систем будут автоматически, по расписанию, приходить на ваше хранилище с ролью SV-Secondary в главном датацентре.</p>
<p><img style="background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px" title="SnapVault1" border="0" alt="SnapVault1" src="/pics//snapvault1.png" width="626" height="436" /></p>
<p>При этом только в первый раз будет передан полный объем хранения (так называемая baseline copy) для создания первоначального “снимка”, в дальнейшем через каналы передачи данных будет передаваться уже только “разностная” информация, например только измененные за день работы блоки данных.    <br />Допустим, что общий объем данных на вашей удаленной системе хранения равен 100GB. Но в течении суток меняется только около гигабайта.     <br />Тогда при первом, baseline transfer, будет передано все 100GB (это можно сделать, например, локально, перед тем, как отвезти систему хранения в удаленный офис), а затем, при ежедневном копировании, будет передаваться всего 1GB. ?? целый месяц ежедневных резервных копирований полной системы на Secondary-системе в центральном офисе займет всего около 130GB.</p>
<p>Также можно использовать SnapVault и для организации удаленной копии данных для Primary-системы для “квази-DR” решения. При этом в качестве резервного хранилища для мощной SnapVault Primary может служить недорогая и с невысокой производительностью, достаточной только для приема и хранения реплики данных, система с лицензией SnapVault Secondary.</p>
<p><img style="background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px" title="SnapVault2" border="0" alt="SnapVault2" src="/pics//snapvault2.png" width="575" height="332" /></p>
<p>Совместно и интегрированно с NetApp SnapVault могут работать такие программные продукты, как CommVault Sympana Suite и Syncsort Backup Express.</p>
<p>Настраивать и управлять работой SnapVault можно как из командной строки, так и с помощью отдельного программного продукта NetApp – Protection Manager, который также сейчас поставляется в составе многих бандлов, например в ONTAP Essential для FAS3200/6200.</p>
<p>Наилучший способ углубиться в тему – прочесть <a href="http://www.netapp.com/us/library/technical-reports/tr-3487.html">TR-3487 SnapVault Best Practices Guide</a>,     <br />и <a href="http://www.netapp.com/us/library/technical-reports/tr-3466.html">TR-3466 Open Systems SnapVault Best Practices Guide</a>.</p>
<p><i>Я помню, что обещал вам &#8220;исследование&#8221; по истории фрагментации в файловых системах, но что-то у меня пока куражу нет его дописать на нужном уровне, придется подождать. Читайте пока про SnapVault, а на следущих две недели у меня спланирована большая программа по углубленному &#8220;полосканию косточек&#8221; EMC VNX/VNXe. Не переключайте ваши браузеры, будет интересно. :)</i></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../../tag/netapp/index.html" rel="tag">netapp</a>, <a href="../../../tag/ossv.html" rel="tag">ossv</a>, <a href="../../../tag/snapshots.html" rel="tag">snapshots</a>, <a href="../../../tag/snapvault.html" rel="tag">snapvault</a><br />					Раздел: <a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../879/trackback.html#comments" title="Комментарий к записи Что такое SnapVault и куда это применить?">Комментарии (3)</a>									 </div>
			</div>
	
		
		<div class="navigation">
			<div class="alignleft"><a href="http://blog.aboutnetapp.ru/archives/category/techtalk/page/8">&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="6.html">Next Entries &raquo;</a></div>
		</div>
		
	
	</div>
	<div id="sidebar">
		<ul>
			
			
			<!-- Author information is disabled per default. Uncomment and fill in your details if you want to use it.
			<li><h2>Автор</h2>
			<p>A little something about you, the author. Nothing lengthy, just an overview.</p>
			</li>
			-->

			<li class="pagenav"><h2>Страницы</h2><ul><li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul></li>
			<li><h2>Рубрики</h2>
				<ul>
					<li class="cat-item cat-item-89"><a href="../../commands/index.html" title="Просмотреть все записи в рубрике &laquo;commands&raquo;">commands</a>
</li>
	<li class="cat-item cat-item-37"><a href="../../howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;">howto</a>
</li>
	<li class="cat-item cat-item-52"><a href="../../justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;">justread</a>
</li>
	<li class="cat-item cat-item-51"><a href="../../review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;">review</a>
</li>
	<li class="cat-item cat-item-3 current-cat"><a href="../index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;">techtalk</a>
</li>
	<li class="cat-item cat-item-71"><a href="../../tricks/index.html" title="Просмотреть все записи в рубрике &laquo;tricks&raquo;">tricks</a>
</li>
	<li class="cat-item cat-item-95"><a href="../../utilities/index.html" title="Просмотреть все записи в рубрике &laquo;utilities&raquo;">utilities</a>
</li>
	<li class="cat-item cat-item-44"><a href="../../whoisho/index.html" title="Просмотреть все записи в рубрике &laquo;whoisho&raquo;">whoisho</a>
</li>
	<li class="cat-item cat-item-1"><a href="../../news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;">новости</a>
</li>
	<li class="cat-item cat-item-387"><a href="../../opros.html" title="Просмотреть все записи в рубрике &laquo;опрос&raquo;">опрос</a>
</li>
	<li class="cat-item cat-item-8"><a href="../../translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;">переводы</a>
</li>
	<li class="cat-item cat-item-40"><a href="../../citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;">цитаты</a>
</li>
				</ul>
			</li>

			<li><h2>Архивы</h2>
				<ul>
					<li><a href='../../../date/2018/01.html' title='Январь 2018'>Январь 2018</a></li>
	<li><a href='../../../date/2015/10.html' title='Октябрь 2015'>Октябрь 2015</a></li>
	<li><a href='../../../date/2015/04.html' title='Апрель 2015'>Апрель 2015</a></li>
	<li><a href='../../../date/2015/03.html' title='Март 2015'>Март 2015</a></li>
	<li><a href='../../../date/2015/01.html' title='Январь 2015'>Январь 2015</a></li>
	<li><a href='../../../date/2014/12.html' title='Декабрь 2014'>Декабрь 2014</a></li>
	<li><a href='../../../date/2014/11.html' title='Ноябрь 2014'>Ноябрь 2014</a></li>
	<li><a href='../../../date/2014/10.html' title='Октябрь 2014'>Октябрь 2014</a></li>
	<li><a href='../../../date/2014/09.html' title='Сентябрь 2014'>Сентябрь 2014</a></li>
	<li><a href='../../../date/2014/08.html' title='Август 2014'>Август 2014</a></li>
	<li><a href='../../../date/2014/07.html' title='Июль 2014'>Июль 2014</a></li>
	<li><a href='../../../date/2014/06.html' title='Июнь 2014'>Июнь 2014</a></li>
	<li><a href='../../../date/2014/05.html' title='Май 2014'>Май 2014</a></li>
	<li><a href='../../../date/2014/04.html' title='Апрель 2014'>Апрель 2014</a></li>
	<li><a href='../../../date/2014/03.html' title='Март 2014'>Март 2014</a></li>
	<li><a href='../../../date/2014/02.html' title='Февраль 2014'>Февраль 2014</a></li>
	<li><a href='../../../date/2014/01.html' title='Январь 2014'>Январь 2014</a></li>
	<li><a href='../../../date/2013/12.html' title='Декабрь 2013'>Декабрь 2013</a></li>
	<li><a href='../../../date/2013/11.html' title='Ноябрь 2013'>Ноябрь 2013</a></li>
	<li><a href='../../../date/2013/10.html' title='Октябрь 2013'>Октябрь 2013</a></li>
	<li><a href='../../../date/2013/09.html' title='Сентябрь 2013'>Сентябрь 2013</a></li>
	<li><a href='../../../date/2013/08.html' title='Август 2013'>Август 2013</a></li>
	<li><a href='../../../date/2013/07.html' title='Июль 2013'>Июль 2013</a></li>
	<li><a href='../../../date/2013/06.html' title='Июнь 2013'>Июнь 2013</a></li>
	<li><a href='../../../date/2013/05.html' title='Май 2013'>Май 2013</a></li>
	<li><a href='../../../date/2013/04.html' title='Апрель 2013'>Апрель 2013</a></li>
	<li><a href='../../../date/2013/03.html' title='Март 2013'>Март 2013</a></li>
	<li><a href='../../../date/2013/02.html' title='Февраль 2013'>Февраль 2013</a></li>
	<li><a href='../../../date/2013/01.html' title='Январь 2013'>Январь 2013</a></li>
	<li><a href='../../../date/2012/12.html' title='Декабрь 2012'>Декабрь 2012</a></li>
	<li><a href='../../../date/2012/11.html' title='Ноябрь 2012'>Ноябрь 2012</a></li>
	<li><a href='../../../date/2012/10.html' title='Октябрь 2012'>Октябрь 2012</a></li>
	<li><a href='../../../date/2012/09.html' title='Сентябрь 2012'>Сентябрь 2012</a></li>
	<li><a href='../../../date/2012/08.html' title='Август 2012'>Август 2012</a></li>
	<li><a href='../../../date/2012/07.html' title='Июль 2012'>Июль 2012</a></li>
	<li><a href='../../../date/2012/06.html' title='Июнь 2012'>Июнь 2012</a></li>
	<li><a href='../../../date/2012/05.html' title='Май 2012'>Май 2012</a></li>
	<li><a href='../../../date/2012/04.html' title='Апрель 2012'>Апрель 2012</a></li>
	<li><a href='../../../date/2012/03.html' title='Март 2012'>Март 2012</a></li>
	<li><a href='../../../date/2012/02.html' title='Февраль 2012'>Февраль 2012</a></li>
	<li><a href='../../../date/2012/01.html' title='Январь 2012'>Январь 2012</a></li>
	<li><a href='../../../date/2011/12.html' title='Декабрь 2011'>Декабрь 2011</a></li>
	<li><a href='../../../date/2011/11.html' title='Ноябрь 2011'>Ноябрь 2011</a></li>
	<li><a href='../../../date/2011/10/index.html' title='Октябрь 2011'>Октябрь 2011</a></li>
	<li><a href='../../../date/2011/09/index.html' title='Сентябрь 2011'>Сентябрь 2011</a></li>
	<li><a href='../../../date/2011/08.html' title='Август 2011'>Август 2011</a></li>
	<li><a href='../../../date/2011/07/index.html' title='Июль 2011'>Июль 2011</a></li>
	<li><a href='../../../date/2011/06/index.html' title='Июнь 2011'>Июнь 2011</a></li>
	<li><a href='../../../date/2011/05/index.html' title='Май 2011'>Май 2011</a></li>
	<li><a href='../../../date/2011/04/index.html' title='Апрель 2011'>Апрель 2011</a></li>
	<li><a href='../../../date/2011/03/index.html' title='Март 2011'>Март 2011</a></li>
	<li><a href='../../../date/2011/02.html' title='Февраль 2011'>Февраль 2011</a></li>
	<li><a href='../../../date/2011/01.html' title='Январь 2011'>Январь 2011</a></li>
	<li><a href='../../../date/2010/12.html' title='Декабрь 2010'>Декабрь 2010</a></li>
	<li><a href='../../../date/2010/11/index.html' title='Ноябрь 2010'>Ноябрь 2010</a></li>
	<li><a href='../../../date/2010/10/index.html' title='Октябрь 2010'>Октябрь 2010</a></li>
	<li><a href='../../../date/2010/09/index.html' title='Сентябрь 2010'>Сентябрь 2010</a></li>
	<li><a href='../../../date/2010/08.html' title='Август 2010'>Август 2010</a></li>
	<li><a href='../../../date/2010/07/index.html' title='Июль 2010'>Июль 2010</a></li>
	<li><a href='../../../date/2010/06.html' title='Июнь 2010'>Июнь 2010</a></li>
	<li><a href='../../../date/2010/05.html' title='Май 2010'>Май 2010</a></li>
	<li><a href='../../../date/2010/04/index.html' title='Апрель 2010'>Апрель 2010</a></li>
	<li><a href='../../../date/2010/03/index.html' title='Март 2010'>Март 2010</a></li>
	<li><a href='../../../date/2010/02.html' title='Февраль 2010'>Февраль 2010</a></li>
	<li><a href='../../../date/2010/01.html' title='Январь 2010'>Январь 2010</a></li>
	<li><a href='../../../date/2009/12/index.html' title='Декабрь 2009'>Декабрь 2009</a></li>
	<li><a href='../../../date/2009/11/index.html' title='Ноябрь 2009'>Ноябрь 2009</a></li>
	<li><a href='../../../date/2009/10.html' title='Октябрь 2009'>Октябрь 2009</a></li>
	<li><a href='../../../date/2009/09.html' title='Сентябрь 2009'>Сентябрь 2009</a></li>
	<li><a href='../../../date/2009/08/index.html' title='Август 2009'>Август 2009</a></li>
	<li><a href='../../../date/2009/07/index.html' title='Июль 2009'>Июль 2009</a></li>
	<li><a href='../../../date/2009/06.html' title='Июнь 2009'>Июнь 2009</a></li>
	<li><a href='../../../date/2009/05.html' title='Май 2009'>Май 2009</a></li>
	<li><a href='../../../date/2009/04.html' title='Апрель 2009'>Апрель 2009</a></li>
	<li><a href='../../../date/2009/03.html' title='Март 2009'>Март 2009</a></li>
	<li><a href='../../../date/2009/02.html' title='Февраль 2009'>Февраль 2009</a></li>
	<li><a href='../../../date/2009/01.html' title='Январь 2009'>Январь 2009</a></li>
	<li><a href='../../../date/2008/12.html' title='Декабрь 2008'>Декабрь 2008</a></li>
	<li><a href='../../../date/2008/11.html' title='Ноябрь 2008'>Ноябрь 2008</a></li>
	<li><a href='../../../date/2008/10.html' title='Октябрь 2008'>Октябрь 2008</a></li>
	<li><a href='../../../date/2008/09.html' title='Сентябрь 2008'>Сентябрь 2008</a></li>
	<li><a href='../../../date/2008/08.html' title='Август 2008'>Август 2008</a></li>
	<li><a href='../../../date/2008/03.html' title='Март 2008'>Март 2008</a></li>
	<li><a href='../../../date/2008/02.html' title='Февраль 2008'>Февраль 2008</a></li>
	<li><a href='../../../date/2007/12.html' title='Декабрь 2007'>Декабрь 2007</a></li>
	<li><a href='../../../date/2007/11.html' title='Ноябрь 2007'>Ноябрь 2007</a></li>
	<li><a href='../../../date/2007/10.html' title='Октябрь 2007'>Октябрь 2007</a></li>
	<li><a href='../../../date/2007/09.html' title='Сентябрь 2007'>Сентябрь 2007</a></li>
	<li><a href='../../../date/2007/08.html' title='Август 2007'>Август 2007</a></li>
	<li><a href='../../../date/2007/07/index.html' title='Июль 2007'>Июль 2007</a></li>
	<li><a href='../../../date/2007/06.html' title='Июнь 2007'>Июнь 2007</a></li>
	<li><a href='../../../date/2007/05.html' title='Май 2007'>Май 2007</a></li>
				</ul>
			</li>

			
					</ul>
	</div>

</div> <!-- wrapper -->
<div id="footer">
	<a href="../../../../feed">Entries (RSS)</a> and <a href="../../../../comments/feed">Comments (RSS)</a>. Valid <a href="http://validator.w3.org/check/referer" title="This page validates as XHTML 1.0 Transitional"><abbr title="eXtensible HyperText Markup Language">XHTML</abbr></a> and <a href="http://jigsaw.w3.org/css-validator/check/referer"><abbr title="Cascading Style Sheets">CSS</abbr></a>.<br />
	Powered by <a href="http://wordpress.org/" title="Powered by WordPress.">WordPress</a> and <a href="http://srinig.com/wordpress/themes/fluid-blue/">Fluid Blue theme</a>.<br />
	<!-- 15 queries. 0.108 seconds. -->
	</div>
</div> <!-- page -->
</body>
</html>
