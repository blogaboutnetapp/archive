<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	>
<channel>
	<title>Комментарии к записи: Так ли незаменимы магнитные ленты для бэкапа? Часть 4</title>
	<atom:link href="http://blog.aboutnetapp.ru/archives/623/feed" rel="self" type="application/rss+xml" />
	<link>http://blog.aboutnetapp.ru/archives/623</link>
	<description>Системы хранения данных как предмет разговора</description>
	<pubDate>Wed, 07 Apr 2021 12:07:06 +0000</pubDate>
	<generator>http://wordpress.org/?v=2.6</generator>
		<item>
		<title>Автор: bbk</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-2794</link>
		<dc:creator>bbk</dc:creator>
		<pubDate>Sat, 23 Jun 2012 12:55:23 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-2794</guid>
		<description>deepblue&#62;??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже?
К примеру одно ПО может дедуплицировать, другое нет, в рамках стораджа так хранить данные не эффективно: одни данные дедуплицированны, другие нет. Что вас подвигло на этот _странный_ вопрос не пойму, ответ очевиден.</description>
		<content:encoded><![CDATA[<p>deepblue&gt;??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже?<br />
К примеру одно ПО может дедуплицировать, другое нет, в рамках стораджа так хранить данные не эффективно: одни данные дедуплицированны, другие нет. Что вас подвигло на этот _странный_ вопрос не пойму, ответ очевиден.</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: romx</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-554</link>
		<dc:creator>romx</dc:creator>
		<pubDate>Sat, 26 Jun 2010 11:27:36 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-554</guid>
		<description>&gt; Если делать дедупликацию на хосте, то запись идёт быстрее

Зато нагружается процессор хоста _в момент бэкапа_, что снижает его производительность, и, как следствие, скорость записи. От чего ушли - к тому пришли.

&gt; Это особенно актуально для VMWare, где сетевой ресурс часто ограничивает коэффициент консолидации.

Если датасторы VMware хранятся на NetApp FAS, то они обычно уже дедуплицированы на момент бэкапа (и весьма эффективно, обычно, по практическому опыту, 70-90%. Если использовать нетапповское же решение репликации Volume SnapMirror, то трафик оказывается дедуплицированным.

&gt; Низкий коэффициент дедупликации (fixed-length block алгоритм)

На VTL есть и variable block.

&gt; обычно не удаётся добиться больше, чем 10-20% дедупликации

У NetApp на этот счет есть другие сведения.

&gt; Дедупликация делается по расписанию ПОСЛЕ того, как данные записаны на диск.

Да, и это не минус, повторюсь, это огромный плюс! Процесс дедупликации никак не влияет на производительность системы, так как асинхронен с процессами. А место... Что место? Это просто диски. Диски купить куда как проще и дешевле, чем производительность и время сохранния/восстановления. Как я уже говорил по другому поводу: получить производительность по цене дисков это весьма неплохой deal по нынешним временам.

&gt; Для бэкапов эта функция почти бесполезна именно по причинам перечисленным выше.

Я ответил?</description>
		<content:encoded><![CDATA[<p>> Если делать дедупликацию на хосте, то запись идёт быстрее</p>
<p>Зато нагружается процессор хоста _в момент бэкапа_, что снижает его производительность, и, как следствие, скорость записи. От чего ушли - к тому пришли.</p>
<p>> Это особенно актуально для VMWare, где сетевой ресурс часто ограничивает коэффициент консолидации.</p>
<p>Если датасторы VMware хранятся на NetApp FAS, то они обычно уже дедуплицированы на момент бэкапа (и весьма эффективно, обычно, по практическому опыту, 70-90%. Если использовать нетапповское же решение репликации Volume SnapMirror, то трафик оказывается дедуплицированным.</p>
<p>> Низкий коэффициент дедупликации (fixed-length block алгоритм)</p>
<p>На VTL есть и variable block.</p>
<p>> обычно не удаётся добиться больше, чем 10-20% дедупликации</p>
<p>У NetApp на этот счет есть другие сведения.</p>
<p>> Дедупликация делается по расписанию ПОСЛЕ того, как данные записаны на диск.</p>
<p>Да, и это не минус, повторюсь, это огромный плюс! Процесс дедупликации никак не влияет на производительность системы, так как асинхронен с процессами. А место&#8230; Что место? Это просто диски. Диски купить куда как проще и дешевле, чем производительность и время сохранния/восстановления. Как я уже говорил по другому поводу: получить производительность по цене дисков это весьма неплохой deal по нынешним временам.</p>
<p>> Для бэкапов эта функция почти бесполезна именно по причинам перечисленным выше.</p>
<p>Я ответил?</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: deepblue</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-552</link>
		<dc:creator>deepblue</dc:creator>
		<pubDate>Fri, 25 Jun 2010 18:29:46 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-552</guid>
		<description>Раз уж перечислили все "за", стоит перечислить и все "против" использования НетАпп дедупликации для бэкапных данных:

- Если делать дедупликацию на хосте, то запись идёт быстрее, так как данные по сети передаются в уже дедуплицированном виде и сеть загружается намного меньше. Это особенно актуально для VMWare, где сетевой ресурс часто ограничивает коэффициент консолидации. Чаще всего при миграции с физических на виртуальные хосты приходится переделывать всю схему бэкапа и дисковая дедупликация здесь к сожалению никак не помогает.
- Низкий коэффициент дедупликации (fixed-length block алгоритм), обычно не удаётся добиться больше, чем 10-20% дедупликации, в интернете полно материала на эту тему. Решения специально заточенные под эту задачу дают намного лучший результат.
- Дедупликация делается по расписанию ПОСЛЕ того, как данные записаны на диск. Даже если можно 100Гб дедуплицировать до 90Гб, первоначально всё равно нужно будет 100Гб свободного места.

Вообще моё личное субъективное мнение - самое лучшее место для такой дедупликации - "файловые шары". "Включил и забыл" и место экономится.

Для бэкапов эта функция почти бесполезна именно по причинам перечисленным выше.</description>
		<content:encoded><![CDATA[<p>Раз уж перечислили все &#8220;за&#8221;, стоит перечислить и все &#8220;против&#8221; использования НетАпп дедупликации для бэкапных данных:</p>
<p>- Если делать дедупликацию на хосте, то запись идёт быстрее, так как данные по сети передаются в уже дедуплицированном виде и сеть загружается намного меньше. Это особенно актуально для VMWare, где сетевой ресурс часто ограничивает коэффициент консолидации. Чаще всего при миграции с физических на виртуальные хосты приходится переделывать всю схему бэкапа и дисковая дедупликация здесь к сожалению никак не помогает.<br />
- Низкий коэффициент дедупликации (fixed-length block алгоритм), обычно не удаётся добиться больше, чем 10-20% дедупликации, в интернете полно материала на эту тему. Решения специально заточенные под эту задачу дают намного лучший результат.<br />
- Дедупликация делается по расписанию ПОСЛЕ того, как данные записаны на диск. Даже если можно 100Гб дедуплицировать до 90Гб, первоначально всё равно нужно будет 100Гб свободного места.</p>
<p>Вообще моё личное субъективное мнение - самое лучшее место для такой дедупликации - &#8220;файловые шары&#8221;. &#8220;Включил и забыл&#8221; и место экономится.</p>
<p>Для бэкапов эта функция почти бесполезна именно по причинам перечисленным выше.</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: romx</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-551</link>
		<dc:creator>romx</dc:creator>
		<pubDate>Fri, 25 Jun 2010 17:39:56 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-551</guid>
		<description>&gt; ??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже?

По-моему очевидно. Потому что в таком случае это прозрачно для приложений, работает с любой системой резервного копирования, и грузит этой задачей (притом оффлайново) только сторадж, снимая нагрузку с сервера резервного копирования, которому и так обычно нелегко.

&gt; чтобы снизить нагрузку на сеть, особенно при бэкапах на расстоянии.

В решении NetApp "на расстояние" ( на вторичное, удаленное хранилище) передаются уже дедуплицированные на первичном хранилище данные. То есть сперва оно бэкапится локально (не слишком разумно сразу бэкапить удаленно, помним про неободимость быстрого рестора оперативного бэкапа "только что"), дедуплицируется в процессе жизни, после чего реплицируется уже дедуплицированным.</description>
		<content:encoded><![CDATA[<p>> ??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже?</p>
<p>По-моему очевидно. Потому что в таком случае это прозрачно для приложений, работает с любой системой резервного копирования, и грузит этой задачей (притом оффлайново) только сторадж, снимая нагрузку с сервера резервного копирования, которому и так обычно нелегко.</p>
<p>> чтобы снизить нагрузку на сеть, особенно при бэкапах на расстоянии.</p>
<p>В решении NetApp &#8220;на расстояние&#8221; ( на вторичное, удаленное хранилище) передаются уже дедуплицированные на первичном хранилище данные. То есть сперва оно бэкапится локально (не слишком разумно сразу бэкапить удаленно, помним про неободимость быстрого рестора оперативного бэкапа &#8220;только что&#8221;), дедуплицируется в процессе жизни, после чего реплицируется уже дедуплицированным.</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: deepblue</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-550</link>
		<dc:creator>deepblue</dc:creator>
		<pubDate>Fri, 25 Jun 2010 17:08:41 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-550</guid>
		<description>&#62; Ну тут я могу лишь сказать, что считаю возложение задачи дедупликации на сторадж, выбранный NetApp абсолютно правильным.

??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже? Понятно что у NetApp выбора особенно не было, но в зависимости от задачи будут разные аргументы "за" и "против". Раз уж мы взялись за бэкап, то здесь всё совсем неоднозначно. Традиционно бэкап - это самые большие потоки данных и логично их дедуплицировать на стороне хоста, чтобы снизить нагрузку на сеть, особенно при бэкапах на расстоянии. DataDomain, вот, придумал DD Boost, который дедуплицирует данные на стороне бэкап сервера, хотя на сторадже уже давно это умеют делать, как с этим дела у НетАппа?</description>
		<content:encoded><![CDATA[<p>&gt; Ну тут я могу лишь сказать, что считаю возложение задачи дедупликации на сторадж, выбранный NetApp абсолютно правильным.</p>
<p>??нтересно, на основании чего вы думаете, что дедуплицировать лучше на сторадже? Понятно что у NetApp выбора особенно не было, но в зависимости от задачи будут разные аргументы &#8220;за&#8221; и &#8220;против&#8221;. Раз уж мы взялись за бэкап, то здесь всё совсем неоднозначно. Традиционно бэкап - это самые большие потоки данных и логично их дедуплицировать на стороне хоста, чтобы снизить нагрузку на сеть, особенно при бэкапах на расстоянии. DataDomain, вот, придумал DD Boost, который дедуплицирует данные на стороне бэкап сервера, хотя на сторадже уже давно это умеют делать, как с этим дела у НетАппа?</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: romx</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-549</link>
		<dc:creator>romx</dc:creator>
		<pubDate>Fri, 25 Jun 2010 16:47:06 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-549</guid>
		<description>&gt; Только сейчас вендоры начали добовлять фичи в бэкап-софт типа той же дедупликации, которые делают задачу бекапа на диск экономомически оправданной.

Ну тут я могу лишь сказать, что считаю возложение задачи дедупликации на сторадж, выбранный NetApp абсолютно правильным.

&gt; иначе бы NetApp не предлагал за них $ 1.9 миллиарда. :)

Мое мнение (которое может не совпадать с мнением NetApp) что в истории с покупкой DDUP NetApp покупал в первую очередь клиентскую базу.

&gt; NetApp VTL, как отдельный продукт, вообще собираются прибить

Там вообще запутанная история какая-то. Официально это звучт так: прекращена дальнейшая разработка VTL как продукта. Продолжается его продажа по состоянию на сентябрь прошлого года. Почему-отчего - комментариев получить не удалось, увы.</description>
		<content:encoded><![CDATA[<p>> Только сейчас вендоры начали добовлять фичи в бэкап-софт типа той же дедупликации, которые делают задачу бекапа на диск экономомически оправданной.</p>
<p>Ну тут я могу лишь сказать, что считаю возложение задачи дедупликации на сторадж, выбранный NetApp абсолютно правильным.</p>
<p>> иначе бы NetApp не предлагал за них $ 1.9 миллиарда. :)</p>
<p>Мое мнение (которое может не совпадать с мнением NetApp) что в истории с покупкой DDUP NetApp покупал в первую очередь клиентскую базу.</p>
<p>> NetApp VTL, как отдельный продукт, вообще собираются прибить</p>
<p>Там вообще запутанная история какая-то. Официально это звучт так: прекращена дальнейшая разработка VTL как продукта. Продолжается его продажа по состоянию на сентябрь прошлого года. Почему-отчего - комментариев получить не удалось, увы.</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: deepblue</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-548</link>
		<dc:creator>deepblue</dc:creator>
		<pubDate>Fri, 25 Jun 2010 16:46:42 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-548</guid>
		<description>Дабы не навлечь на себя потоков гнева на этом блоге, добавлю, что DataDomain и NetApp VTL - разного класса продукты в рамках конкретной задачи (backup 2 disk). Насколько я понял, NetApp VTL, как отдельный продукт, вообще собираются прибить, что, в принципе, логично:
http://www.enterprisestorageforum.com/continuity/news/article.php/3863086/NetApp-Halts-VTL-Deduplication-Development.htm</description>
		<content:encoded><![CDATA[<p>Дабы не навлечь на себя потоков гнева на этом блоге, добавлю, что DataDomain и NetApp VTL - разного класса продукты в рамках конкретной задачи (backup 2 disk). Насколько я понял, NetApp VTL, как отдельный продукт, вообще собираются прибить, что, в принципе, логично:<br />
<a href="http://www.enterprisestorageforum.com/continuity/news/article.php/3863086/NetApp-Halts-VTL-Deduplication-Development.htm" rel="nofollow">http://www.enterprisestorageforum.com/continuity/news/article.php/3863086/NetApp-Halts-VTL-Deduplication-Development.htm</a></p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: deepblue</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-547</link>
		<dc:creator>deepblue</dc:creator>
		<pubDate>Fri, 25 Jun 2010 16:38:17 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-547</guid>
		<description>&#62;Возможно вы будете шокированы внимательно прочитав пост, но то, что вы там
&#62;“прочитали” родилось в вашей голове, и я этого не говорил. Спасибо за доверие,
&#62;конечно, но если вы хотите говорить со мной, то давайте говорить о том, что я на
&#62;самом деле утверждал, а не о том, что вы мне приписали. Благодарю за внимание :)

ОК, просто ваша серия постов прочиталась иманно так. Раз посты называются часть 1,2,3,4, я за вас додумал, что вы рассматриваете вариант замены снэпшотами традиционной связки "бэкап софт-лента". "Бэкап софт-диск" - более равнозначная замена, это всё, что я хотел сказать.

Если я вас не так понял, то беру свои слова обратно, мир-дружба-жевачка.

&#62;Самое смешное, что в недавнем посте в параллельном треде (про VTL) меня чуть
&#62;не на британский флаг порвали, оспаривая необходимость эмуляции ленты на
&#62;VTL, “ведь все давно умеют писать на диски”. Строго говоря все это действительно
&#62;так. Например популярный Backup Exec 10d (и далее), появившийся в 2006, если
&#62;не ошибаюсь, году, именно потому и “d”, что умеет использовать диски нативно.
&#62;NetBackup 6.5 умеет это точно, про 6.0 не помню, 5.0 умел работать с дисками как
&#62;с “псевдолентами”.
&#62;Так кто из современного бэкап-софта не умеет писать на диски нативно или через
&#62;“псевдоленту” (без эмуляции) хотя бы?

??мелось в виду, что если использовать традиционные политики типа "полный бэкап раз в неделю/инкрементальный раз в день" с дисками, то дисковое решение быстро становится слишком дорогим, вы сами об этом писали. Только сейчас вендоры начали добовлять фичи в бэкап-софт типа той же дедупликации, которые делают задачу бекапа на диск экономомически оправданной.

&#62;Ну насчет “не получилось” это еще вопрос, кому больше хотелось. У NetApp есть
&#62;уже лет пять как есть и продается вполне неплохой свой собственный продукт этого
&#62;класса - NetApp VTL.

Ну NetApp VTL с DataDomain я сравнивать не стал бы, это разного класса продукты, иначе бы NetApp не предлагал за них $ 1.9 миллиарда. :)</description>
		<content:encoded><![CDATA[<p>&gt;Возможно вы будете шокированы внимательно прочитав пост, но то, что вы там<br />
&gt;“прочитали” родилось в вашей голове, и я этого не говорил. Спасибо за доверие,<br />
&gt;конечно, но если вы хотите говорить со мной, то давайте говорить о том, что я на<br />
&gt;самом деле утверждал, а не о том, что вы мне приписали. Благодарю за внимание :)</p>
<p>ОК, просто ваша серия постов прочиталась иманно так. Раз посты называются часть 1,2,3,4, я за вас додумал, что вы рассматриваете вариант замены снэпшотами традиционной связки &#8220;бэкап софт-лента&#8221;. &#8220;Бэкап софт-диск&#8221; - более равнозначная замена, это всё, что я хотел сказать.</p>
<p>Если я вас не так понял, то беру свои слова обратно, мир-дружба-жевачка.</p>
<p>&gt;Самое смешное, что в недавнем посте в параллельном треде (про VTL) меня чуть<br />
&gt;не на британский флаг порвали, оспаривая необходимость эмуляции ленты на<br />
&gt;VTL, “ведь все давно умеют писать на диски”. Строго говоря все это действительно<br />
&gt;так. Например популярный Backup Exec 10d (и далее), появившийся в 2006, если<br />
&gt;не ошибаюсь, году, именно потому и “d”, что умеет использовать диски нативно.<br />
&gt;NetBackup 6.5 умеет это точно, про 6.0 не помню, 5.0 умел работать с дисками как<br />
&gt;с “псевдолентами”.<br />
&gt;Так кто из современного бэкап-софта не умеет писать на диски нативно или через<br />
&gt;“псевдоленту” (без эмуляции) хотя бы?</p>
<p>??мелось в виду, что если использовать традиционные политики типа &#8220;полный бэкап раз в неделю/инкрементальный раз в день&#8221; с дисками, то дисковое решение быстро становится слишком дорогим, вы сами об этом писали. Только сейчас вендоры начали добовлять фичи в бэкап-софт типа той же дедупликации, которые делают задачу бекапа на диск экономомически оправданной.</p>
<p>&gt;Ну насчет “не получилось” это еще вопрос, кому больше хотелось. У NetApp есть<br />
&gt;уже лет пять как есть и продается вполне неплохой свой собственный продукт этого<br />
&gt;класса - NetApp VTL.</p>
<p>Ну NetApp VTL с DataDomain я сравнивать не стал бы, это разного класса продукты, иначе бы NetApp не предлагал за них $ 1.9 миллиарда. :)</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: romx</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-546</link>
		<dc:creator>romx</dc:creator>
		<pubDate>Fri, 25 Jun 2010 15:20:37 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-546</guid>
		<description>&gt; может создать впечатление, что снэпшоты - это самый правильный метод решения задачи бэкапа на диски. 

Знаете что самое сложное в "общении с читателями"?
Разгорваривать с "вчитывающими". Так я называю тех, которые вместо прочтения того, что написано автором, то есть мной, начинают читать (и спорить следом) то, что автор не писал, а то, что их самих "беспокоит" в даный момент.
За прошедшие полторы недели меня успели обвинить (по порядку):
1. В том, что я предлагаю выкинуть все ленты, и использовать только диски
2. В том что я пиарю NetApp и пощу проплаченне посты (на тот момент даже слово NetApp ни в одном посте из трех еще даже не упоминалось).
4. В том, что приведенные мной описания "наполнены однобокими расчетами" (при том что я вообще никаких ценовых расчетов кроме цены отдельного устрйства "по прайсру" не привел).

Может что и пропустил, лень бегать по комментам.

Можете смело добавить к этому ваше: "что снэпшоты - это самый правильный метод решения задачи бэкапа на диски".

Возможно вы будете шокированы внимательно прочитав пост, но то, что вы там "прочитали" родилось в вашей голове, и я этого не говорил. Спасибо за доверие, конечно, но если вы хотите говорить со мной, то давайте говорить о том, что я на самом деле утверждал, а не о том, что вы мне приписали. Благодарю за внимание :)

&gt; проблема всегда была в том, что мейнстрим бэкап софт не умел и до сих пор часто не умеет эффективно эти самые диски использовать.

Самое смешное, что в недавнем посте в параллельном треде (про VTL) меня чуть не на британский флаг порвали, оспаривая необходимость эмуляции ленты на VTL, "ведь все давно умеют писать на диски". Строго говоря все это действительно так. Например популярный Backup Exec 10d (и далее), появившийся в 2006, если не ошибаюсь, году, именно потому и "d", что умеет использовать диски нативно. NetBackup 6.5 умеет это точно, про 6.0 не помню, 5.0 умел работать с дисками как с "псевдолентами".
Так кто из современного бэкап-софта не умеет писать на диски нативно или через "псевдоленту" (без эмуляции) хотя бы?

&gt; Ну и про DataDomain с которым у НетАппа не получилось

Ну насчет "не получилось" это еще вопрос, кому больше хотелось. У NetApp есть уже лет пять как есть и продается вполне неплохой свой собственный продукт этого класса - NetApp VTL.</description>
		<content:encoded><![CDATA[<p>> может создать впечатление, что снэпшоты - это самый правильный метод решения задачи бэкапа на диски. </p>
<p>Знаете что самое сложное в &#8220;общении с читателями&#8221;?<br />
Разгорваривать с &#8220;вчитывающими&#8221;. Так я называю тех, которые вместо прочтения того, что написано автором, то есть мной, начинают читать (и спорить следом) то, что автор не писал, а то, что их самих &#8220;беспокоит&#8221; в даный момент.<br />
За прошедшие полторы недели меня успели обвинить (по порядку):<br />
1. В том, что я предлагаю выкинуть все ленты, и использовать только диски<br />
2. В том что я пиарю NetApp и пощу проплаченне посты (на тот момент даже слово NetApp ни в одном посте из трех еще даже не упоминалось).<br />
4. В том, что приведенные мной описания &#8220;наполнены однобокими расчетами&#8221; (при том что я вообще никаких ценовых расчетов кроме цены отдельного устрйства &#8220;по прайсру&#8221; не привел).</p>
<p>Может что и пропустил, лень бегать по комментам.</p>
<p>Можете смело добавить к этому ваше: &#8220;что снэпшоты - это самый правильный метод решения задачи бэкапа на диски&#8221;.</p>
<p>Возможно вы будете шокированы внимательно прочитав пост, но то, что вы там &#8220;прочитали&#8221; родилось в вашей голове, и я этого не говорил. Спасибо за доверие, конечно, но если вы хотите говорить со мной, то давайте говорить о том, что я на самом деле утверждал, а не о том, что вы мне приписали. Благодарю за внимание :)</p>
<p>> проблема всегда была в том, что мейнстрим бэкап софт не умел и до сих пор часто не умеет эффективно эти самые диски использовать.</p>
<p>Самое смешное, что в недавнем посте в параллельном треде (про VTL) меня чуть не на британский флаг порвали, оспаривая необходимость эмуляции ленты на VTL, &#8220;ведь все давно умеют писать на диски&#8221;. Строго говоря все это действительно так. Например популярный Backup Exec 10d (и далее), появившийся в 2006, если не ошибаюсь, году, именно потому и &#8220;d&#8221;, что умеет использовать диски нативно. NetBackup 6.5 умеет это точно, про 6.0 не помню, 5.0 умел работать с дисками как с &#8220;псевдолентами&#8221;.<br />
Так кто из современного бэкап-софта не умеет писать на диски нативно или через &#8220;псевдоленту&#8221; (без эмуляции) хотя бы?</p>
<p>> Ну и про DataDomain с которым у НетАппа не получилось</p>
<p>Ну насчет &#8220;не получилось&#8221; это еще вопрос, кому больше хотелось. У NetApp есть уже лет пять как есть и продается вполне неплохой свой собственный продукт этого класса - NetApp VTL.</p>
]]></content:encoded>
	</item>
	<item>
		<title>Автор: deepblue</title>
		<link>http://blog.aboutnetapp.ru/archives/623#comment-545</link>
		<dc:creator>deepblue</dc:creator>
		<pubDate>Fri, 25 Jun 2010 10:58:44 +0000</pubDate>
		<guid isPermaLink="false">http://blog.aboutnetapp.ru/?p=623#comment-545</guid>
		<description>Претензий у меня ни к кому нет :) Просто то, как мы перешли с темы бэкапа на диски (что есть гуд) к теме снэпшотов, может создать впечатление, что снэпшоты - это самый правильный метод решения задачи бэкапа на диски. Я попытался указать, почему во многих случаях это не так. ??дея бэкапа на диски не нова, проблема всегда была в том, что мейнстрим бэкап софт не умел и до сих пор часто не умеет эффективно эти самые диски использовать. Потом появились нишевые продукты, которые эту проблему решили с помощью дедупликации - PureDisk, Avamar. Сейчас с разным успехом эти функции встраиваются в мейнстрим продукты. Ну и про DataDomain с которым у НетАппа не получилось, тоже забывать не стоит. На мой взгляд снэпшотами заменить функционал полноценного бэкап софта нельзя.</description>
		<content:encoded><![CDATA[<p>Претензий у меня ни к кому нет :) Просто то, как мы перешли с темы бэкапа на диски (что есть гуд) к теме снэпшотов, может создать впечатление, что снэпшоты - это самый правильный метод решения задачи бэкапа на диски. Я попытался указать, почему во многих случаях это не так. ??дея бэкапа на диски не нова, проблема всегда была в том, что мейнстрим бэкап софт не умел и до сих пор часто не умеет эффективно эти самые диски использовать. Потом появились нишевые продукты, которые эту проблему решили с помощью дедупликации - PureDisk, Avamar. Сейчас с разным успехом эти функции встраиваются в мейнстрим продукты. Ну и про DataDomain с которым у НетАппа не получилось, тоже забывать не стоит. На мой взгляд снэпшотами заменить функционал полноценного бэкап софта нельзя.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
