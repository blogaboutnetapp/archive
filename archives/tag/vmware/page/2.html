<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="ru-RU">

<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<title>about NetApp   &raquo; vmware</title>

<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/style.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/print.css" type="text/css" media="print" />
<link rel="alternate" type="application/rss+xml" title="about NetApp RSS Feed" href="../../../../feed" />
<link rel="pingback" href="../../../../xmlrpc.php.html" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 2.6" />

</head>

<body>
<div id="page">
<div id="header">
	<div id="headertitle">
		<h1><a href="../../../../index.html" title="about NetApp: Системы хранения данных как предмет разговора">about NetApp</a></h1>
		<p>Системы хранения данных как предмет разговора</p>
	</div> 
	<!-- Search box (If you prefer having search form as a sidebar widget, remove this block) -->
	<div class="search">
		<form method="get" id="searchform" action="../../../../index.html">
<input type="text" size="20" name="s" id="s" value="Поиск..."  onblur="if(this.value=='') this.value='Поиск...';" onfocus="if(this.value=='Поиск...') this.value='';"/>
</form>
	</div> 
	<!-- Search ends here-->
		
</div>

<div id="navbar">
<ul id="nav">
	<li><a href="../../../../index.html">Home</a></li>
	<li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul>
</div>
<div id="wrapper">

	<div id="content">

	
			<p>Posts tagged &#8216;vmware&#8217;</p>

	 		
		<div class="navigation">
			<div class="alignleft"><a href="3.html">&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="../index.html">Next Entries &raquo;</a></div>
		</div>

						
			<div class="post" id="post-1221">
				<h2 class="posttitle"><a href="../../../1221/trackback.html" rel="bookmark" title="Permanent Link to Data ONTAP Edge-T">Data ONTAP Edge-T</a></h2>
				<div class="postmetadata">11 Октябрь 2012, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Совсем немного времени прошло с момтента выпуска <a href="../../../1210/trackback.html">Data ONTAP Edge</a> – “виртуального NetApp” в форме Virtual Appliance для VMware, как подоспел “апсайз”: Data ONTAP Edge-T.</p>
<p>Если кратко, то это <em>расширенная и углубленная</em> версия Edge:</p>
<p><img src="https://communities.netapp.com/servlet/JiveServlet/downloadImage/38-9286-17153/620-328/Edge-T.png" /></p>
<p>Кк видите, теперь Edge может быть не только источником данных для SnapVault, но и получателем (в том числе для серверов с Open System SnapVault, под “обычными” OS), поддерживает репликацию SnapMirror. Остальные параметры остались без изменений.</p>
<p>Если вы уже интересовались ценами на Edge, то уже, наверняка были удивлены фактом того, что <del datetime="2012-10-13T12:32:06+00:00">Edge продается в одном варианте - паком на 10 лицензий</del>  <em>(UPD: с сентября можно купить поштучно</em>). То есть нацелен он, прежде всего, на множественные филиалы, но цена, из-за за 10 штук, получилась не очень интересной тем, кому хочется купить и использовать всего один-два. Ждем более интересной и подходящей ценовой политики от Edge-T.</p>
<p>Берут “на попробовать” по-прежнему там: <a href="http://www.netapp.com/data-ontap-edge-eval">http://www.netapp.com/data-ontap-edge-eval</a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../appliance.html" rel="tag">appliance</a>, <a href="../../edge.html" rel="tag">edge</a>, <a href="../../edge-t.html" rel="tag">edge-t</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">vmware</a>, <a href="../../vsa.html" rel="tag">VSA</a><br />					Раздел: <a href="../../../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;" rel="category tag">новости</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1221/trackback.html#comments" title="Комментарий к записи Data ONTAP Edge-T">Комментарии (2)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1217">
				<h2 class="posttitle"><a href="../../../1217/trackback.html" rel="bookmark" title="Permanent Link to NetApp MetroCluster целиком сертфицирован под VMware">NetApp MetroCluster целиком сертфицирован под VMware</a></h2>
				<div class="postmetadata">24 Сентябрь 2012, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Я <a href="../../../61/trackback.html">уже писал</a> тут о NetApp MetroCluster, решении построения распределенного отказоустойчивого хранилища данных с “нулевым RPO/RTO”.</p>
<p>Этот продукт уже довольно давно был сертифицирован на совместимость по программе VMware vMSC (vSphere Metro Storage Cluster), но только как NFS хранилище, и включен в такой конфигурации в <a href="http://www.vmware.com/resources/compatibility/search.php?deviceCategory=san">VMwatre HCL</a>. Однако недавно была завершена и сертификация решения под блочные (iSCSI и FC) протоколы, и сейчас NetApp MetroCluster – единственное среди систем хранения в листе vMSC стораджевое решение географически распределенного кластера, сертифицированное по программе vMSC под блочные (iSCSI, FCP) и файловые (NFS) протоколы вместе, как в stretched-версии (до 500 метров разноса узлов), так и в switched (до 100 километров между узлами).</p>
<p>Про NetApp Metrocluster теперь есть статья в VMware Knowledgebase: <a href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=2031038">http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;cmd=displayKC&amp;externalId=2031038</a></p>
<p>Так что прошу любить и жаловать. Если вам нужно распределенное отказоустойчивое хранилище под задачи VMware vSphere, причем гарантированно поддерживаемое VMware, то обратите внимание на NetApp MetroCluster. Это недешевое решение, разумеется, но, в тех случаях, когда отказ и недоступность данных на хранилище недопустим абсолютно, это одно из наилучших решений в этой области, хорошо практически отработанное, существующее на рынке уже около 10 лет, и <a href="http://www.netapp.com/us/library/customer-stories.html">используемое в продакшне сотнями разнообразных клиентов NetApp в мире</a>.</p>
<p><img style="background-image: none; border-bottom: 0px; border-left: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image161.png" width="666" height="508" /></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../metrocluster.html" rel="tag">metrocluster</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../vmsc.html" rel="tag">vmsc</a>, <a href="../index.html" rel="tag">vmware</a>, <a href="../../vsphere/index.html" rel="tag">vsphere</a><br />					Раздел: <a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>,  <a href="../../../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;" rel="category tag">новости</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1217/trackback.html#respond" title="Комментарий к записи NetApp MetroCluster целиком сертфицирован под VMware">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1169">
				<h2 class="posttitle"><a href="../../../1169/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 3d &ndash; Трафик NFS и балансировка Load Based Teaming">VMware и использование NFS: часть 3d &ndash; Трафик NFS и балансировка Load Based Teaming</a></h2>
				<div class="postmetadata">14 Июнь 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Третья часть серии постов в блоге <a href="http://wahlnetwork.com/">Wahl Network</a>, посвященных вариантам использования NFS в Vmware и организаци балансировки трафика по нескольким физическим путям.</p>
<h2><a href="http://wahlnetwork.com/2012/04/27/nfs-on-vsphere-technical-deep-dive-on-multiple-subnet-storage-traffic/">NFS в vSphere – погружение в детали: часть 3, VMware Load Balancing Teaming</a></h2>
<p>&#160;</p>
<p>В предшествующих трех постах этой серии мы <a href="../../../1164/trackback.html">рассмотрели основные ошибки и заблуждения</a>, связанные с использованием NFS в VMware, а также рассмотрели два варианта построения сети хранерия с использованием NFS - <a href="../../../1165/trackback.html">с единой подсетью</a>, и <a href="../../../1166/trackback.html">с множественными подсетями</a>. Если вы не слишком хорошо знакомы с тем, как NFS работает в vSphere, рекомендую вам начать с чтения этих статей. Выводы, к которым мы пришли, состоят в том, что NFS в vSphere требует использовать множественные подсети, чтобы использовать множественные физические линки к системе хранения, за исключением, когда EtherChannel правильно настроен правильно используется на одной подсети. Однако, даже при использовании static EtherChannel, множественные таргеты на стороне стораджа, и правильно спланированные least significant bits в адресе необходимы для использования более одного физического пути к системе хранения.</p>
<p>   <a href="../../../1169/trackback.html#more-1169" class="more-link"><small>Continue reading &#8216;VMware и использование NFS: часть 3d &ndash; Трафик NFS и балансировка Load Based Teaming&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../lbt.html" rel="tag">lbt</a>, <a href="../../load-balancing.html" rel="tag">load balancing</a>, <a href="../../nas.html" rel="tag">nas</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1169/trackback.html#respond" title="Комментарий к записи VMware и использование NFS: часть 3d &ndash; Трафик NFS и балансировка Load Based Teaming">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1166">
				<h2 class="posttitle"><a href="../../../1166/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 3c &ndash; Трафик NFS в разных подсетях">VMware и использование NFS: часть 3c &ndash; Трафик NFS в разных подсетях</a></h2>
				<div class="postmetadata">7 Июнь 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Продолжаем публикацию переводов серии постов в блоге <a href="http://wahlnetwork.com/">Wahl Network</a>, посвященных вариантам использования NFS в VMware.</p>
<h2><a href="http://wahlnetwork.com/2012/04/27/nfs-on-vsphere-technical-deep-dive-on-multiple-subnet-storage-traffic/">NFS в vSphere – погружение в детали: часть 2, порты vmkernel и экспорты NFS в разных подсетях</a></h2>
<p>Apr 27, 2012 </p>
<p>Ранее мы уже рассмотрели некоторые ошибочные концепции относительно NFS в vSphere, а также убедились в том, как трафик NFS идет при использовании одной подсети. Сейчас давайте посмотрим, как трафик NFS в vSphere пойдет в случае использования множественных подсетей. Хотя мы говорим тут прежде всего о NFS, это все также применимо и в случае iSCSI, если для него не используется binding.</p>
<p> <a href="../../../1166/trackback.html#more-1166" class="more-link"><small>Continue reading &#8216;VMware и использование NFS: часть 3c &ndash; Трафик NFS в разных подсетях&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../../vlan.html" rel="tag">vlan</a>, <a href="../index.html" rel="tag">vmware</a>, <a href="../../vsphere/index.html" rel="tag">vsphere</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1166/trackback.html#respond" title="Комментарий к записи VMware и использование NFS: часть 3c &ndash; Трафик NFS в разных подсетях">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1165">
				<h2 class="posttitle"><a href="../../../1165/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 3b &ndash; Трафик NFS в одной подсети">VMware и использование NFS: часть 3b &ndash; Трафик NFS в одной подсети</a></h2>
				<div class="postmetadata">31 Май 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Для рассмотрения вопроса, как работает доступ к стораджу по NFS с хоста ESXi, я снова воспользуюсь серией постов блога <a href="http://wahlnetwork.com">Wahl Network</a>, переводы которых я публикую сегодня и в ближайшие несколько дней. Его автор провел экспериментальную работу, показав то, как работает NFS в сети хранения, когда датасторы и vmkernel расположены в одной общей подсети, в разных подсетях, и рассмотрел вариант использования Load-based teaming, доступный для пользователей версии vSphere уровня Enterprise Plus.</p>
<p>Я надеюсь, что эти статьи ответят на вопрос, как же все же работает NFS в сети хранения vSphere, и как стораджи с использованием этого протокола правильно использовать для VMware vSphere 5.0.</p>
<h2><a href="http://wahlnetwork.com/2012/04/23/nfs-on-vsphere-technical-deep-dive-on-same-subnet-storage-traffic/">NFS в vSphere – погружение в детали: часть 1, порты vmkernel и экспорты NFS в единой общей подсети</a></h2>
<p>Apr 23, 2012 </p>
<h3>Конфигурация</h3>
<p>Для эксперимента, показывающего, как vSphere направляет трафик NFS в одной подсети, я создал тестовый стенд, с использованием 2 серверов NFS (я использовал для этого NetApp Simulator) с каждого их которых выведено по 2 экспорта, суммарно 4 экспорта NFS. Весь трафик направлен в VLAN 5 (это подсеть 10.0.5.0/24 моего стенда) и идет на хост ESXi 5.0 update 1 (build 623860). Хост имеет 2 физических порта-аплинка и 4 порта vmkernel, дающих трафику NFS множество возможны путей. Для того, чтобы создать существенный трафик в сети хранения, я развернул 4 VM с VMware IO analyzer appliance – по одному на каждый экспорт. Это позволит мне быстро создать трафик с виртуальных машин на все экспорты разом.</p>
<p>   <a href="../../../1165/trackback.html#more-1165" class="more-link"><small>Continue reading &#8216;VMware и использование NFS: часть 3b &ndash; Трафик NFS в одной подсети&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../esx.html" rel="tag">esx</a>, <a href="../../etherchannel.html" rel="tag">etherchannel</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../index.html" rel="tag">vmware</a>, <a href="../../vsphere/index.html" rel="tag">vsphere</a><br />					Раздел: <a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1165/trackback.html#respond" title="Комментарий к записи VMware и использование NFS: часть 3b &ndash; Трафик NFS в одной подсети">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1164">
				<h2 class="posttitle"><a href="../../../1164/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 3а &ndash;Балансировка нагрузки по NFS">VMware и использование NFS: часть 3а &ndash;Балансировка нагрузки по NFS</a></h2>
				<div class="postmetadata">28 Май 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как вы помните, я обещал остановиться на вопросе как именно правильно настраивать работу NFS по нескольким физическим интерфейсам Ethernet. ??так, отдельный разоблачения заблуждений в отношении multipathing для NFS псто!</p>
<p>Как я уже вкратце показал, широкораспространенное заблуждение, что, использовав NFS, вы будете вынужденно ограничены ровно одним интерфейсом ethernet, так как “NFS не поддерживает multipathing” – не верно. К моему глубокому сожалению, это заблуждение местами поддерживает даже сама VMware, несмотря на то, что метод балансировки с помощью нескольких портов VMkernel и нескольких IP-алиасов описан и рекомендуется в их же Best Practices.</p>
<p>Я уже написал ранее, что принципы работы NFS таковы, что даже пустив трафик Ethernet по нескольким объединенным в etherchannel портам, вы не добьетесь не только равномерной их загрузки, но даже не загрузите порты кроме первого. Это по видимому связано с тем, что NFS образует ровно одну TCP/IP сессию для данной пары IP-адресов “источник-приемник”, и именно ее и пускает по первому же подходящему для этого порту. А одну сессию разбить на несколько интерфейсов нельзя. Видимо именно это явилось источником странного заблуждения, что трафик NFS нельзя распределить по нескольким интерфейсам. Это не так, можно. Но нужна некоторая хитрость, в общем-то очевидная. Нужно создать несколько пар IP “источник-получатель”, в разных подсетях, и их уже распределить по интерфейсам.&#160; Об этом подробнее далее в постах серии.</p>
<p>Впрочем, как вы уже видите, неверно и обратное убеждение. Совсем недостаточно объединить несколько портов в etherchannel (у NetApp он называется VIF или ifgrp), чтобы, автоматически, увеличить пропусную способность получившегося интерфейса в Х-раз.</p>
<p>Давайте разберем некоторые распространенные заблуждения на этот счет:</p>
<p><strong>1. Трафик NFS можно назначить на порт VMkernel также, как мы назначаем его для iSCSI.</strong></p>
<p>Это не так. К сожалению.</p>
<p><img src="http://wahlnetwork.files.wordpress.com/2012/04/virtual-adapter-settings.png" /></p>
<p>Как вы видите на рисунке, в обведенном оранжевой рамкой боксе, можно назначить данный порт для: <em>iSCSI, FT, Management</em> или <em>vMotion</em>. Но не для NFS. Трафик NFS пойдет по первому же порту, принадлежащему подсети IP-получателя. Если порта в подсети IP-получателя трафика нет, то трафик пойдет через порт Management, в направлении его default gateway (предсказуемо), и такого варианта следует всеми силами избегать.</p>
<p><strong>2. Трафик NFS равномерно распределяется по всем аплинкам (физическим vmnic) хоста, используемым для связи с системой хранения.</strong></p>
<p>Увы – нет. Когда ESXi нажодит и выбирает подходящий порт vmkernel (принцип выбора описан выше), следуюшим шагом он выбирает аплинк. Независимо от типа vSwitch, хост vSphere использует конфигурацию portgroup по умолчанию (только с использованием virtual port ID), и, при наличии нескольких активных аплинков в группе, будет выбран только один (первый подходящий) аплинк для трафика NFS к данному примонтированному экспорту. Порты в группе при этом работают на поддержку high availability, то есть при выходе из строя активного порта, трафик перейдет на ранее пассивный порт, но не для load balancing, то есть трафик NFS по портам vmnic в группе не распределяется.</p>
<p><strong>3. Достаточно включить несколько физических портов на стороне стораджа в VIF (etherchannel), и трафик магическим образом распределится по всем им, расширяя полосу пропускания интерфейса в Х-раз.</strong></p>
<p>Тоже, к сожалению, в общем случае, без дополнительных усилий, неверно. При наличии одной TCP/IP сессии NFS с одного IP-источника на один IP-получатель, нельзя “разложить” трафик на несколько портов. Но можно это сделать для нескольких портов, нескольких IP-алиасов для получателя и/или&#160; нескольких портов VMkernel. Тогда, создав, например 4 пары IP в разных подсетях, для четырех eth, можно гораздо более равномено загрузить их работой. Это не столь “магически-автоматически”, но это работает. Без такого разбиения и распределения&#160; etherchannel обеспечивает только отказостоустойчивость (при отказе активного порта, трафик перенаправится в другой, ранее неактивный, в той же группе), но не балансировку нагрузки и не расширение bandwidth.</p>
<p>На стороне хоста ESXi для использования балансировки вам необходимо создать <em>static Etherchannel </em>с <em>IPhash-based teaming policy</em>, и иметь у vmkernel IP уникальный, между другими vmk, так называемый <em><a href="http://en.wikipedia.org/wiki/Least_significant_bit">least significant bit</a></em>.</p>
<p>Если у вас на VMware лицензия Enterprise Plus, и вы используете vSphere Distributed Switch, вы также можете воспользоваться <em>Load-based Teaming</em> для распределения трафика по VLAN или подсетям. При этом вам также понадобятся несколько VLAN или подсетей на VIF стораджа. Подробнее о таком варианте – в одном из следующих постов.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../etherchannel.html" rel="tag">etherchannel</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../../vif.html" rel="tag">vif</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1164/trackback.html#comments" title="Комментарий к записи VMware и использование NFS: часть 3а &ndash;Балансировка нагрузки по NFS">Комментарии (4)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1159">
				<h2 class="posttitle"><a href="../../../1159/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 2">VMware и использование NFS: часть 2</a></h2>
				<div class="postmetadata">21 Май 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>В <a href="../../../1151/trackback.html">части первой</a> я написал небольшую вводную, что такое NFS, и почему вам стоит обратить на этот способ подключения системы хранения к хост-серверу виртуализации, какие достоинства, удобства и премущества есть у этого способа.</p>
<p>??так, перейдем к некоторым конкретным вопросам, на которые приодится отвечать, выбирая NFS в качестве протокола доступа к датастору в VMware. Впервые такой вариант появился еще в VMware ESX 3.0, и постепенно зарабатывает все большую популярность, потесняя “классический” блочный способ подключения по FCP или iSCSI. О преимуществах, и некоторых недостатках я писал в <a href="../../../1151/trackback.html">первой части данной серии</a>.</p>
<p>Какие же основные проблемы принято называть, когда речь идет о испоьзовании NFS для VMware?</p>
<p><strong>1. На NFS нет multipathing.</strong></p>
<p>“<em>?? значит, выбирая NFS, я ограничен производительностью только одного интерфейса Ethernet</em>” добавляется явно или подразумеваемо. Ну, на самом деле это “и так и не так”. Пока оставим “за скобками” <em>практическую </em>надобность расширения канала к дискам даже для Gigabit Ethernet NFS или iSCSI (во многих системах это расширение bandwidth имеет довольно спорную <em>практическую </em>ценность). На NFS действительно нет multipath в том виде, в котором он понимается в блочных протоколах, потому что <strong>multipathing (MPIO, Multipathed Input Output) – это фича исключительно блочных протоколов.</strong> А так как NFS это не блочный протокол, то <em>фичи блочных протоколов</em> в нем быть не может “по определению”. Это так. Однако, NFS, как протокол поверх TCP/IP, конечно же имеет возможности реализации отказоустойчивости, путем использования множественных путей (как его имеет сам нижележащий TCP/IP), а также использования нескольких параллельных каналов доступа к данным, для расширения bandwidth. </p>
<p>Но тут есть некотрая тонкость. Проблема связана с тем, что NFS, для связи с данным датастором, с данным IP, и всеми его файлами, использует только одну TCP-сессию. А одну сессию, имеющую один IP-source и один IP-destination никак нельзя забалансировать по нескольким физическим портам Ethernet, даже с использованием etherchannel, формально работающем уровнем ниже.</p>
<p>Но можно (и нужно) выйти из положения хитрым трюком. Дело в том, что можно создать для destination несколько так называемых IP-алиасов, а также несколько портов VMkernel в качестве IP-source. Датастор, тем самым может быть доступен по нескольким равноправным IP-адресам. Если мы подключим датастор таким образом, у нас уже могут быть несколько различных IP-destination, через разные IP-source в VMkernel и, значит, заработает балансировка по IP-хэшу. Такой трафик, вышедший из нескольких IP-source в своих подсетях, и пришедший на сторадж в разные IP-алиасы, <strong>можно</strong> распределить по нескольким физическим eth-интерфейсам. Просто это присходит не “мистически-автоматически”, как в случае iSCSI MPIO, а путем ручной настройки этой балансировки, и дальнейшей ее самостоятельной работы.</p>
<p>Подробнее о этом методе рассказывается в <em><a href="http://www.netwell.ru/docs/netapp/rus_tr-3802_ethernet_storage_bp.pdf">TR-3802 Ethernet для систем хранения: Наилучшие методы</a></em> (глава 4.6 IP-алиасы), и в <em><a href="http://netwell.ru/docs/netapp/TR-3749_Rus_NetApp_VMware_vSphere_v5.0/tr-3749_rus_netapp_vmware_vsphere_v5.0.pdf">TR-3749 Руководство по наилучшим способам использования систем NetApp с VMware vSphere</a></em> (глава 3.3 Основы сети хранения с использованием Ethernet, глава 3.6 Сеть хранения с использованием Multiswitch Link Aggregation).</p>
<p>Дабы не раздувать один блогопост, чуть подробнее предлагаю углубиться в тему в отдельном посте далее, посвященном тому, как именно организована балансировка методом нескольких IP-алиасов на стороне стораджа, и нескольких портов VMkernel на стороне хост-сервера.</p>
<p><strong>UPD:</strong> Если вы счастливый обладатель VMware в лицензии <em>Enterprise Plus</em>, то тогда вам доступен еше один вариант загрузки нескольких NFS-линков к стораджу - это режим <em>Load-based Teaming</em> для интерфейсов vnic. Об этом способе мы также поговорим чуть позднее в отдельном посте.</p>
<p><strong>2. NFS нестабилен в работе и имеет проблемы с призводительностью.</strong></p>
<p><em>“Мы видели это своими глазами, собрав сервер NFS на Linux”</em> – добавляется явно или подразумеваемо. ?? это снова “и так, и не так”. Да, действительно, реализация NFS на Linux давно страдает серьезными проблемами, ее обычно приходится в продакшне твикать и патчить, только чтобы поправить некоторые наиболее вопиющие проблемы. В vanilla code она в продакшн малопригодна. Но это не значит, что <em>любой</em> NFS server также непригоден, только потому что он – NFS! Реализация NFS от NetApp зарекомендовала себя во множестве систем крайне высокого класса, ей по плечу задачи от небольших систем, до масштабов Yahoo!, Oracle, Siemens и Deutsche Telecom. NetApp имеет опыт разработки и эксплуатации NFS-серверов уже около 20 лет, в самых жестких условиях и требованиях по производительности и надежности. </p>
<p>??так: Не все реализации NFS “одинаково полезны”. ?? реализацией NFS в Linux многообразие их не исчерпывается. Не нужно интерполировать на NetApp неудачные реализации одной отдельно взятой подсистемы, в одной конкретной OS (или группе OS, использующих общих прародителей данного кода).</p>
<p>В следующей части я попробую более подробно остановится на методах multipathing для NFS, о которых выше в посте я вкратце уже упомянул.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../iscsi/index.html" rel="tag">iscsi</a>, <a href="../../mpio.html" rel="tag">mpio</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1159/trackback.html#comments" title="Комментарий к записи VMware и использование NFS: часть 2">1 комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1151">
				<h2 class="posttitle"><a href="../../../1151/trackback.html" rel="bookmark" title="Permanent Link to VMware и использование NFS: часть 1">VMware и использование NFS: часть 1</a></h2>
				<div class="postmetadata">7 Май 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как вы знаете, я убежденный сторонник того, что системы хранения NetApp – это лучший выбор для использования в среде серверной и десктопной вируализации. А для самой этой виртуализации – использование протокола NFS, который для систем NetApp, более чем родной, в свою очередь, лучший способ подключить дисковое хранилище. Со мной согласны <a href="../../../992/trackback.html">уже 36% пользователей систем виртуализации</a> (согласно отчету Forrester за май прошлого года), причем процент использования NFS растет, и уже превысил процент использования iSCSI (23%) на этих задачах.</p>
<p>Я уже не раз писал в этом блоге про различные аспекты использования NFS (посмотрите старые статьи по <a href="../../nfs/index.html">тегу NFS</a>), и даже переводил на этут тему Best Practices (про <a href="../../../632/trackback.html">Ethernet Storage</a> вообще и <a href="../../../591/trackback.html">про VMware</a> в частности), однако все это были разрозненные публикации “к случаю”. Мне захотелось собрать основные темы вопроса в одном месте, и обсудить их, наконец, “раз и навсегда”, или пока тема не изменилась значительно.</p>
<p>Но для начала несколько вводных слов, для тех, только что подключился к блогу.</p>
<p><strong>NFS</strong> (Network File System) – это протокол “сетевой файловой системы” разработанной компанией Sun в глубокой исторически-компьютерной древности, и предназначенный для доступа к данным в файлах по сети, в том числе для совместного доступа к ним от нескольких клиентов. NFS, вследствие своей сравнительной простоты реализации, стал очень популярным в UNIX-мире, где работу по NFS поддерживают практически любые OS. Несмотря на то, что сегодня “файловой системой” обычно принято называть нечто иное, за протоколом доступа к файлам по сети, NFS, исторически закрепилось название “файловая система”.&#160; С момента своего изобретения, NFS прошел большой путь, и на сегодня достиг версии 4.2, обретя множество важных на сегодня возможностей, таких как использование не только UDP, как первоначально в исходной версии протокола, но и TCP (в v3), улучшенные технологии разграничения доступа и безопасности (v4), поддержка распределенных кластерных и объектных хранилищ (v4.1) и различные методы offload-а (v4.2).</p>
<p>К сожалению, за NFS водится два своеобразных недостатка. Во-первых, он так и не появился в OS семейства Windows (если не считать крайне проблемной реализации, вышедшей в составе продуктов MS Services for UNIX) и остался “чужим и непонятным” для win-админов. ?? второе, но более важное, не все его реализации “одинаково полезны”. Многие пользователи, познакомившиеся с NFS через имеющую кучу проблем с производительностью и стабильностью, широкораспространенной реализацией в Vanilla Linux, считают, что “весь NFS такой, глючный, тормозной, для продакшна не пригодный”. А это не так.</p>
<p>В третьих, наконец, вокруг NFS, и особенностей его работы, циркулирует множество различных недопониманий, вдобавок помноженных на специфики реализаций и долгий исторический путь от версии к версии. Вот разбором этих недопониманий мы и займемся для начала. Напомню, я не стану обнимать необъятное, и сосредоточусь только лишь на использовании NFS в VMware.</p>
<p>А теперь <strong>о достоинствах</strong>. Во-первых следует отметить сравнительную простоту использования NFS. Его <strong>использование не требует внедрения и освоения сложной, особенно для новичка, FC-инфраструктуры</strong>, непростых процессов настроек зонинга, или разбирательства с iSCSI. ??спользовать NFS для доступа к датастору также просто и тем, что <strong>гранулярность хранения при этом равна файлу VMDK, а не целиком датастору</strong>, как в случае блочных протоколов. Датастор NFS это обычная монтируемая на хост сетевая “шара” с файлами дисков виртуальных машин и их конфигами. Это, в свою очередь, облегчает, например, резервное копирование и восстановление, так как единицей копирования и восстановления является простой файл, отдельный виртуальный диск отдельной виртуальной машины. Нельзя сбрасывать со счетов и то, что при использовании NFS <strong>вы “автоматически” получаете thin provisioning</strong>, а <strong>дедупликация высвобождает вам пространство непосредственно на уровень датастора</strong>, оно становится доступно непосредственно администратору и пользователям VM, а не на уровень стораджа, как в случае использования LUN-а. Это все также выглядит крайне привлекательно с точки зрения использования виртуальной инфраструктуры.</p>
<p>Наконец, используя датастор по NFS, <strong>вы не ограничены лимитом в 2TB</strong>, даже с VMFS ранее 5, а это очень полезно, например, если вам приходится администрировать большое количество сравнительно слабонагруженных вводом-выводом машин. ??х всех можно поместить на один большой датастор, бэкапить, и управлять которым гораздо проще, чем десятком разрозненных VMFS LUN-ов по 2TB(-512bytes) каждый.</p>
<p>Кроме того, <strong>вы можете свободно не только увеличивать, но и уменьшать датастор</strong>. Это может быть очень полезной возможностью для динамичной инфраструтуры, с большим количеством разнородных VM, таких как, например, среды облачных провайдеров, где VM постоянно создаются и&#160; удаляются, и данный конкретный датастор, для размещения этих VM, может не только расти, но и, часто, уменьшаться.</p>
<p>Однако, какие у нас имеются минусы?</p>
<p>Ну, во-первых, это <strong>невозможность использовать RDM</strong> (Raw-device mapping), который может понадобиться, например, для реализации кластера MS Cluster Service, если вы его хотите использовать. <strong>С NFS нельзя загрузиться</strong> (по крайней мере простым и обычным способом, типа boot-from-SAN). <strong>??спользование NFS сопряжено с некоторым увеличением нагрузки на сторадж</strong>, так как ряд операций, которые, в случае блочного SAN, реализуются на стороне хоста, в случае NFS поддерживатся стораджем. Это всяческие блокировки, разграничение доступа, и так далее. Однако, практика показывает, что <a href="http://media.netapp.com/documents/tr-3808.pdf">оверхед отражается на производительности крайне незначительно</a>.</p>
<p>Большим плюсом использования NFS на стораджах NetApp является то, что выбор NFS там не диктует вам “жесткого выбора” для вашей инфраструктуры в целом. Если вам понадобится также и блочный протокол для RDM, или для крайне критичной даже к <em>возможному</em> 10-15% падению производительности виртуальной машины, вы можете использовать для нее iSCSI или даже FCP, все на том же сторадже, параллельно с NFS, и всеми его плюсами, для основного датастора.</p>
<p>В следующем посте этой серии мы перейдемь к разбирательству основных недопониманий и мифов, которые имеются вокруг использования NFS в VMware.</p>
<p>Рекомендуется также почитать по теме:</p>
<ul>
<li>VMware: <a href="http://vmware.com/files/pdf/VMware_NFS_BestPractices_WP_EN.pdf">Best Practices for running VMware vSphere on Network Attached Storage</a></li>
<li>NetApp (RU): <a href="http://netwell.ru/docs/netapp/TR-3749_Rus_NetApp_VMware_vSphere_v5.0/tr-3749_rus_netapp_vmware_vsphere_v5.0.pdf">Руководство по наилучшим способам использования систем NetApp        <br />с VMware vSphere</a></li>
<li>NetApp (RU): <a href="http://www.netwell.ru/docs/netapp/tr-3839_vmware_nfs.pdf">??спользование NFS в VMware</a></li>
<li>NetApp (RU): <a href="http://www.netwell.ru/docs/netapp/rus_tr-3802_ethernet_storage_bp.pdf">Ethernet для систем хранения: наилучшие методы</a></li>
<li>NetApp (EN): <a href="http://media.netapp.com/documents/tr-3880.pdf">tr-3880 CLI Configuration Processes for NetApp and VMware vSphere.pdf</a></li>
<li>NetApp (EN): <a href="http://media.netapp.com/documents/tr-3697.pdf">tr-3697 Multiprotocol Performance Test of VMware ESX 3.5 on NetApp</a></li>
<li>NetApp (EN): <a href="http://media.netapp.com/documents/tr-3808.pdf">tr-3808 VMware vSphere 4.1 and ESX 3.5 Multiprotocol Performance Comparison Using FC iSCSI and NFS</a></li>
</ul>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../nfs/index.html" rel="tag">nfs</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/whoisho/index.html" title="Просмотреть все записи в рубрике &laquo;whoisho&raquo;" rel="category tag">whoisho</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1151/trackback.html#comments" title="Комментарий к записи VMware и использование NFS: часть 1">Комментарии (10)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1147">
				<h2 class="posttitle"><a href="../../../1147/trackback.html" rel="bookmark" title="Permanent Link to Сравнительное тестирование HDS AMS2100 и NetApp FAS2240-2">Сравнительное тестирование HDS AMS2100 и NetApp FAS2240-2</a></h2>
				<div class="postmetadata">26 Апрель 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Нам пишут наши читатели:</p>
<p>Оригнал здесь, перепечатывается оттуда по согласию с автором: <a href="http://ua9uqb.livejournal.com/79872.html">http://ua9uqb.livejournal.com/79872.html</a></p>
<hr />
<p>Не так давно довелось мне протестировать две системы хранения данных, делюсь результатами. В тесте принимали участие системы начального уровня NetApp FAS2240 и Hitachi AMS2100.    <br />Надо понимать, что системы эти сравнимы довольно условно, так как AMS - обычное, бедное по функционалу блочное хранилище, в отличии от сильно мозговитого, &quot;всё-всё умеющего&quot; NetApp-а. Конфигурации системы, методики тестирования, и прочие тюнинги, были согласованны со специалистами вендоров. </p>
<p>Немного подробностей по методике:    <br />1. Тестирование проводилось программой IOMeter версии 2006.07.27 для Windows. </p>
<p>2. Подключение СХД Hitachi AMS2100 производится посредством FC4G, протокол FC    <br />Cостав LUN и томов СХД     <br />Raid10 8 x 450gb 15k 3.5’ Usable: 1.5Tb Lun: 700Gb     <br />Raid6 8+2 450gb 15k 3.5’ Usable: 3.1Tb Lun: 700Gb </p>
<p>3. Подключение СХД NetApp FAS2240 производится посредством 10Ge, протокол iscsi    <br />Состав LUN и томов СХД     <br />RaidDP 9+2 x 600gb 10k 2.5’ Usable: 4.33Tb Lun: 700Gb </p>
<p>4. Тесты проводится в режиме файловой системы NTFS 4к, выравнивание партишина 64к; </p>
<p>5. Настройки программы IOMeter:    <br />• Worker&#8217;s – 4     <br />• Target – 16 (в каждом из Worker’s)     <br />• Disk Size – 200000000 (100Gb)     <br />• Ramp Up Time – 600 сек     <br />• Run Time – 30 мин </p>
<p>6. Фиксация результатов каждого теста для IOMeter длиться 30 минут начиная с 10-й и заканчивая 40-й минутой теста; </p>
<p>7. Профили нагрузок(название профилей условное), используемые для тестирования IOMeter следующие:&#160; <br /><strong> Нагрузка характерная для приложений ORACLE&#160; <br /></strong>• размер блока 8КБ;     <br />• соотношение количества операций чтения/запись 30/70;     <br />• характер нагрузки – 100% случайный доступ; </p>
<p><strong> Нагрузка характерная для VMware: </strong>    <br />• размер блока 4КБ;     <br />• соотношение количества операций чтения/запись 40/60;     <br />• характер нагрузки – 45% линейный и 55% случайный доступ; </p>
<p><strong>Нагрузка характерная для операций Backup:      <br /></strong>• размер блока 64КБ;     <br />• 100% последовательное чтение; </p>
<p>??тог:    <br /><a href="http://pics.livejournal.com/ua9uqb/pic/000f0e30"><img alt="" src="http://pics.livejournal.com/ua9uqb/pic/000f0e30" width="466" height="268" /></a>     </p>
<p>Табличка довольно прозрачная, поэтому никаких выводов делать не буду.    <br />NetApp FAS2240(в центре, где куча вертикальных планочек, и жёлтая наклейка сверху): </p>
<p><a href="http://pics.livejournal.com/ua9uqb/pic/000ezadb"><img alt="" src="http://pics.livejournal.com/ua9uqb/pic/000ezadb" width="640" height="424" /></a>&#160;</p>
<p>Hitachi AMS2100 </p>
<p><a href="http://pics.livejournal.com/ua9uqb/pic/000eyraa"><img alt="" src="http://pics.livejournal.com/ua9uqb/pic/000eyraa" width="640" height="424" /></a>     </p>
<p>PS. Дополнительно проводилось тестирование программой IOZone, порядок цифр примерно тот же, если будут интересующиеся, выложу результаты. </p>
<p>PPS. Пост навеян коментом френдессы <a href="http://balorus.livejournal.com/profile"><img alt="[info]" src="http://l-stat.livejournal.com/img/userinfo.gif?v=91.1" width="16" height="16" /></a><a href="http://balorus.livejournal.com/">balorus</a> к <a href="http://ua9uqb.livejournal.com/79392.html">посту про сервер Sun T4-4</a> :-) </p>
<p>PPPS. Мой выбор был бы NetApp, если бы Хитача в те же деньги не уложила сильно больше шпинделей. А так же не ещё один сильно политический нюанс, который очень сложно перебить даже теми волшебными, и крайне необходимыми нам штуками, которые NetApp умеет делать очень хорошо, а AMS не умеет в принципе. </p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../2100.html" rel="tag">2100</a>, <a href="../../2240.html" rel="tag">2240</a>, <a href="../../2240-2.html" rel="tag">2240-2</a>, <a href="../../ams2100.html" rel="tag">AMS2100</a>, <a href="../../hds.html" rel="tag">hds</a>, <a href="../../hitachi.html" rel="tag">hitachi</a>, <a href="../../iometer.html" rel="tag">iometer</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../oracle/index.html" rel="tag">oracle</a>, <a href="../../performance/index.html" rel="tag">performance</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1147/trackback.html#comments" title="Комментарий к записи Сравнительное тестирование HDS AMS2100 и NetApp FAS2240-2">Комментарии (18)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1130">
				<h2 class="posttitle"><a href="../../../1130/trackback.html" rel="bookmark" title="Permanent Link to Установка NetApp VASA Provider">Установка NetApp VASA Provider</a></h2>
				<div class="postmetadata">19 Март 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Для начала несколько слов о том, что такое собственно этот VASA Provider.</p>
<p>VASA – это новая придумка VMware для больших сред виртуализации. Когда у вас всего один-два стораджа и всего несколько датасторов, то вам это не надо. Вы и так помните что у вас где. А вот когда систем хранения у вас будет с десяток, и несколько сотен датасторов,&#160; несколько человек админов в три смены, вот тогда вам захочется как-то разбираться, что у вас где, и, по-возможности, быстро. </p>
<p>   <a href="../../../1130/trackback.html#more-1130" class="more-link"><small>Continue reading &#8216;Установка NetApp VASA Provider&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../vasa.html" rel="tag">vasa</a>, <a href="../index.html" rel="tag">vmware</a><br />					Раздел: <a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1130/trackback.html#comments" title="Комментарий к записи Установка NetApp VASA Provider">1 комментарий</a>									 </div>
			</div>
	
		
		<div class="navigation">
			<div class="alignleft"><a href="3.html">&laquo; Previous Entries</a></div>
			<div class="alignright"><a href="../index.html">Next Entries &raquo;</a></div>
		</div>
		
	
	</div>
	<div id="sidebar">
		<ul>
			
			
			<!-- Author information is disabled per default. Uncomment and fill in your details if you want to use it.
			<li><h2>Автор</h2>
			<p>A little something about you, the author. Nothing lengthy, just an overview.</p>
			</li>
			-->

			<li class="pagenav"><h2>Страницы</h2><ul><li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul></li>
			<li><h2>Рубрики</h2>
				<ul>
					<li class="cat-item cat-item-89"><a href="../../../category/commands/index.html" title="Просмотреть все записи в рубрике &laquo;commands&raquo;">commands</a>
</li>
	<li class="cat-item cat-item-37"><a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;">howto</a>
</li>
	<li class="cat-item cat-item-52"><a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;">justread</a>
</li>
	<li class="cat-item cat-item-51"><a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;">review</a>
</li>
	<li class="cat-item cat-item-3"><a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;">techtalk</a>
</li>
	<li class="cat-item cat-item-71"><a href="../../../category/tricks/index.html" title="Просмотреть все записи в рубрике &laquo;tricks&raquo;">tricks</a>
</li>
	<li class="cat-item cat-item-95"><a href="../../../category/utilities/index.html" title="Просмотреть все записи в рубрике &laquo;utilities&raquo;">utilities</a>
</li>
	<li class="cat-item cat-item-44"><a href="../../../category/whoisho/index.html" title="Просмотреть все записи в рубрике &laquo;whoisho&raquo;">whoisho</a>
</li>
	<li class="cat-item cat-item-1"><a href="../../../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;">новости</a>
</li>
	<li class="cat-item cat-item-387"><a href="../../../category/opros.html" title="Просмотреть все записи в рубрике &laquo;опрос&raquo;">опрос</a>
</li>
	<li class="cat-item cat-item-8"><a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;">переводы</a>
</li>
	<li class="cat-item cat-item-40"><a href="../../../category/citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;">цитаты</a>
</li>
				</ul>
			</li>

			<li><h2>Архивы</h2>
				<ul>
					<li><a href='../../../date/2018/01.html' title='Январь 2018'>Январь 2018</a></li>
	<li><a href='../../../date/2015/10.html' title='Октябрь 2015'>Октябрь 2015</a></li>
	<li><a href='../../../date/2015/04.html' title='Апрель 2015'>Апрель 2015</a></li>
	<li><a href='../../../date/2015/03.html' title='Март 2015'>Март 2015</a></li>
	<li><a href='../../../date/2015/01.html' title='Январь 2015'>Январь 2015</a></li>
	<li><a href='../../../date/2014/12.html' title='Декабрь 2014'>Декабрь 2014</a></li>
	<li><a href='../../../date/2014/11.html' title='Ноябрь 2014'>Ноябрь 2014</a></li>
	<li><a href='../../../date/2014/10.html' title='Октябрь 2014'>Октябрь 2014</a></li>
	<li><a href='../../../date/2014/09.html' title='Сентябрь 2014'>Сентябрь 2014</a></li>
	<li><a href='../../../date/2014/08.html' title='Август 2014'>Август 2014</a></li>
	<li><a href='../../../date/2014/07.html' title='Июль 2014'>Июль 2014</a></li>
	<li><a href='../../../date/2014/06.html' title='Июнь 2014'>Июнь 2014</a></li>
	<li><a href='../../../date/2014/05.html' title='Май 2014'>Май 2014</a></li>
	<li><a href='../../../date/2014/04.html' title='Апрель 2014'>Апрель 2014</a></li>
	<li><a href='../../../date/2014/03.html' title='Март 2014'>Март 2014</a></li>
	<li><a href='../../../date/2014/02.html' title='Февраль 2014'>Февраль 2014</a></li>
	<li><a href='../../../date/2014/01.html' title='Январь 2014'>Январь 2014</a></li>
	<li><a href='../../../date/2013/12.html' title='Декабрь 2013'>Декабрь 2013</a></li>
	<li><a href='../../../date/2013/11.html' title='Ноябрь 2013'>Ноябрь 2013</a></li>
	<li><a href='../../../date/2013/10.html' title='Октябрь 2013'>Октябрь 2013</a></li>
	<li><a href='../../../date/2013/09.html' title='Сентябрь 2013'>Сентябрь 2013</a></li>
	<li><a href='../../../date/2013/08.html' title='Август 2013'>Август 2013</a></li>
	<li><a href='../../../date/2013/07.html' title='Июль 2013'>Июль 2013</a></li>
	<li><a href='../../../date/2013/06.html' title='Июнь 2013'>Июнь 2013</a></li>
	<li><a href='../../../date/2013/05.html' title='Май 2013'>Май 2013</a></li>
	<li><a href='../../../date/2013/04.html' title='Апрель 2013'>Апрель 2013</a></li>
	<li><a href='../../../date/2013/03.html' title='Март 2013'>Март 2013</a></li>
	<li><a href='../../../date/2013/02.html' title='Февраль 2013'>Февраль 2013</a></li>
	<li><a href='../../../date/2013/01.html' title='Январь 2013'>Январь 2013</a></li>
	<li><a href='../../../date/2012/12.html' title='Декабрь 2012'>Декабрь 2012</a></li>
	<li><a href='../../../date/2012/11.html' title='Ноябрь 2012'>Ноябрь 2012</a></li>
	<li><a href='../../../date/2012/10.html' title='Октябрь 2012'>Октябрь 2012</a></li>
	<li><a href='../../../date/2012/09.html' title='Сентябрь 2012'>Сентябрь 2012</a></li>
	<li><a href='../../../date/2012/08.html' title='Август 2012'>Август 2012</a></li>
	<li><a href='../../../date/2012/07.html' title='Июль 2012'>Июль 2012</a></li>
	<li><a href='../../../date/2012/06.html' title='Июнь 2012'>Июнь 2012</a></li>
	<li><a href='../../../date/2012/05.html' title='Май 2012'>Май 2012</a></li>
	<li><a href='../../../date/2012/04.html' title='Апрель 2012'>Апрель 2012</a></li>
	<li><a href='../../../date/2012/03.html' title='Март 2012'>Март 2012</a></li>
	<li><a href='../../../date/2012/02.html' title='Февраль 2012'>Февраль 2012</a></li>
	<li><a href='../../../date/2012/01.html' title='Январь 2012'>Январь 2012</a></li>
	<li><a href='../../../date/2011/12.html' title='Декабрь 2011'>Декабрь 2011</a></li>
	<li><a href='../../../date/2011/11.html' title='Ноябрь 2011'>Ноябрь 2011</a></li>
	<li><a href='../../../date/2011/10/index.html' title='Октябрь 2011'>Октябрь 2011</a></li>
	<li><a href='../../../date/2011/09/index.html' title='Сентябрь 2011'>Сентябрь 2011</a></li>
	<li><a href='../../../date/2011/08.html' title='Август 2011'>Август 2011</a></li>
	<li><a href='../../../date/2011/07/index.html' title='Июль 2011'>Июль 2011</a></li>
	<li><a href='../../../date/2011/06/index.html' title='Июнь 2011'>Июнь 2011</a></li>
	<li><a href='../../../date/2011/05/index.html' title='Май 2011'>Май 2011</a></li>
	<li><a href='../../../date/2011/04/index.html' title='Апрель 2011'>Апрель 2011</a></li>
	<li><a href='../../../date/2011/03/index.html' title='Март 2011'>Март 2011</a></li>
	<li><a href='../../../date/2011/02.html' title='Февраль 2011'>Февраль 2011</a></li>
	<li><a href='../../../date/2011/01.html' title='Январь 2011'>Январь 2011</a></li>
	<li><a href='../../../date/2010/12.html' title='Декабрь 2010'>Декабрь 2010</a></li>
	<li><a href='../../../date/2010/11/index.html' title='Ноябрь 2010'>Ноябрь 2010</a></li>
	<li><a href='../../../date/2010/10/index.html' title='Октябрь 2010'>Октябрь 2010</a></li>
	<li><a href='../../../date/2010/09/index.html' title='Сентябрь 2010'>Сентябрь 2010</a></li>
	<li><a href='../../../date/2010/08.html' title='Август 2010'>Август 2010</a></li>
	<li><a href='../../../date/2010/07/index.html' title='Июль 2010'>Июль 2010</a></li>
	<li><a href='../../../date/2010/06.html' title='Июнь 2010'>Июнь 2010</a></li>
	<li><a href='../../../date/2010/05.html' title='Май 2010'>Май 2010</a></li>
	<li><a href='../../../date/2010/04/index.html' title='Апрель 2010'>Апрель 2010</a></li>
	<li><a href='../../../date/2010/03/index.html' title='Март 2010'>Март 2010</a></li>
	<li><a href='../../../date/2010/02.html' title='Февраль 2010'>Февраль 2010</a></li>
	<li><a href='../../../date/2010/01.html' title='Январь 2010'>Январь 2010</a></li>
	<li><a href='../../../date/2009/12/index.html' title='Декабрь 2009'>Декабрь 2009</a></li>
	<li><a href='../../../date/2009/11/index.html' title='Ноябрь 2009'>Ноябрь 2009</a></li>
	<li><a href='../../../date/2009/10.html' title='Октябрь 2009'>Октябрь 2009</a></li>
	<li><a href='../../../date/2009/09.html' title='Сентябрь 2009'>Сентябрь 2009</a></li>
	<li><a href='../../../date/2009/08/index.html' title='Август 2009'>Август 2009</a></li>
	<li><a href='../../../date/2009/07/index.html' title='Июль 2009'>Июль 2009</a></li>
	<li><a href='../../../date/2009/06.html' title='Июнь 2009'>Июнь 2009</a></li>
	<li><a href='../../../date/2009/05.html' title='Май 2009'>Май 2009</a></li>
	<li><a href='../../../date/2009/04.html' title='Апрель 2009'>Апрель 2009</a></li>
	<li><a href='../../../date/2009/03.html' title='Март 2009'>Март 2009</a></li>
	<li><a href='../../../date/2009/02.html' title='Февраль 2009'>Февраль 2009</a></li>
	<li><a href='../../../date/2009/01.html' title='Январь 2009'>Январь 2009</a></li>
	<li><a href='../../../date/2008/12.html' title='Декабрь 2008'>Декабрь 2008</a></li>
	<li><a href='../../../date/2008/11.html' title='Ноябрь 2008'>Ноябрь 2008</a></li>
	<li><a href='../../../date/2008/10.html' title='Октябрь 2008'>Октябрь 2008</a></li>
	<li><a href='../../../date/2008/09.html' title='Сентябрь 2008'>Сентябрь 2008</a></li>
	<li><a href='../../../date/2008/08.html' title='Август 2008'>Август 2008</a></li>
	<li><a href='../../../date/2008/03.html' title='Март 2008'>Март 2008</a></li>
	<li><a href='../../../date/2008/02.html' title='Февраль 2008'>Февраль 2008</a></li>
	<li><a href='../../../date/2007/12.html' title='Декабрь 2007'>Декабрь 2007</a></li>
	<li><a href='../../../date/2007/11.html' title='Ноябрь 2007'>Ноябрь 2007</a></li>
	<li><a href='../../../date/2007/10.html' title='Октябрь 2007'>Октябрь 2007</a></li>
	<li><a href='../../../date/2007/09.html' title='Сентябрь 2007'>Сентябрь 2007</a></li>
	<li><a href='../../../date/2007/08.html' title='Август 2007'>Август 2007</a></li>
	<li><a href='../../../date/2007/07/index.html' title='Июль 2007'>Июль 2007</a></li>
	<li><a href='../../../date/2007/06.html' title='Июнь 2007'>Июнь 2007</a></li>
	<li><a href='../../../date/2007/05.html' title='Май 2007'>Май 2007</a></li>
				</ul>
			</li>

			
					</ul>
	</div>

</div> <!-- wrapper -->
<div id="footer">
	<a href="../../../../feed">Entries (RSS)</a> and <a href="../../../../comments/feed">Comments (RSS)</a>. Valid <a href="http://validator.w3.org/check/referer" title="This page validates as XHTML 1.0 Transitional"><abbr title="eXtensible HyperText Markup Language">XHTML</abbr></a> and <a href="http://jigsaw.w3.org/css-validator/check/referer"><abbr title="Cascading Style Sheets">CSS</abbr></a>.<br />
	Powered by <a href="http://wordpress.org/" title="Powered by WordPress.">WordPress</a> and <a href="http://srinig.com/wordpress/themes/fluid-blue/">Fluid Blue theme</a>.<br />
	<!-- 15 queries. 0.098 seconds. -->
	</div>
</div> <!-- page -->
</body>
</html>
