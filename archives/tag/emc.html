<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="ru-RU">

<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<title>about NetApp   &raquo; emc</title>

<link rel="stylesheet" href="../../wp-content/themes/fluid-blue/style.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../wp-content/themes/fluid-blue/print.css" type="text/css" media="print" />
<link rel="alternate" type="application/rss+xml" title="about NetApp RSS Feed" href="../../feed" />
<link rel="pingback" href="../../xmlrpc.php.html" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 2.6" />

</head>

<body>
<div id="page">
<div id="header">
	<div id="headertitle">
		<h1><a href="../../index.html" title="about NetApp: Системы хранения данных как предмет разговора">about NetApp</a></h1>
		<p>Системы хранения данных как предмет разговора</p>
	</div> 
	<!-- Search box (If you prefer having search form as a sidebar widget, remove this block) -->
	<div class="search">
		<form method="get" id="searchform" action="../../index.html">
<input type="text" size="20" name="s" id="s" value="Поиск..."  onblur="if(this.value=='') this.value='Поиск...';" onfocus="if(this.value=='Поиск...') this.value='';"/>
</form>
	</div> 
	<!-- Search ends here-->
		
</div>

<div id="navbar">
<ul id="nav">
	<li><a href="../../index.html">Home</a></li>
	<li class="page_item page-item-153"><a href="../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul>
</div>
<div id="wrapper">

	<div id="content">

	
			<p>Posts tagged &#8216;emc&#8217;</p>

	 		
		<div class="navigation">
			<div class="alignleft"><a href="emc/page/2.html">&laquo; Previous Entries</a></div>
			<div class="alignright"></div>
		</div>

						
			<div class="post" id="post-1281">
				<h2 class="posttitle"><a href="../1281/trackback.html" rel="bookmark" title="Permanent Link to Ждем EMC VNX2">Ждем EMC VNX2</a></h2>
				<div class="postmetadata">22 Август 2013, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Совсем немного времени остается до того момента, когда, по регулярно знакомой владельцам техники Apple схеме, гордые владельцы <em><a href="vnx.html">передовых систем хранения от лидера рынка</a></em> увидят, как их недавнее приобретение <del datetime="2013-08-21T06:03:29+00:00">превращается в тыкву</del> <a href="http://">становится вчерашним днем</a>.<br />
Ведь в VNX2, новом поколении midrange-систем EMC, и &#8220;в 4 раза выше производительность&#8221; (с) EMC), и новое кэширование во flash, и обновленный софт управления, и даже долгожданная блочная дедупликация.<br />
Вот только все это - для новых систем, которые придется заново купить. А старые&#8230; Старые уже будут не столь <em>передовыми </em>и не столь <em>ведущими</em>, на фоне новых. Они останутся теми, прежними, да, но вряд ли это обрадует клиентов EMC, наконец-то получивших свой долгожданный VNX полгода назад.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="vnx.html" rel="tag">vnx</a>, <a href="vnx2.html" rel="tag">vnx2</a><br />					Раздел: <a href="../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>&nbsp;&nbsp;|&nbsp;
					<a href="../1281/trackback.html#comments" title="Комментарий к записи Ждем EMC VNX2">Комментарии (17)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1262">
				<h2 class="posttitle"><a href="../1262/trackback.html" rel="bookmark" title="Permanent Link to Новинки на EMC World или всегда ли &ldquo;рулит софт&rdquo;?">Новинки на EMC World или всегда ли &ldquo;рулит софт&rdquo;?</a></h2>
				<div class="postmetadata">3 Июнь 2013, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как я и обещал, прошлым постом новости про VNX не ограничатся, тем более, что EMC World дал кое-какую интересную пищу для размышлений. В новом посте Dimitris Krekoukias кратко описывает своим мысли, посетившие его после интересного анонса на EMC World экспериментального контроллера, который, возмжно, будет в будущем моделью <em>VNX.Next</em>.</p>
<p>Напоминаю, что это не мой личный авторский текст, а перевод текста в его блоге, оригинальный текст которого вы можете найти по ссылке и там же, при желании, поучаствововать в обсуждении.</p>
<h2><a href="http://recoverymonkey.org/2013/05/16/how-to-decipher-emcs-new-vnx-pre-announcement-and-look-behind-the-marketing">Как расшифровать анонс EMC о новых VNX и заглянуть за маркетинговую завесу</a></h2>
<p><a href="http://recoverymonkey.org/2013/05/16/how-to-decipher-emcs-new-vnx-pre-announcement-and-look-behind-the-marketing"><u><font color="#0066cc">http://recoverymonkey.org/2013/05/16/how-to-decipher-emcs-new-vnx-pre-announcement-and-look-behind-the-marketing</font></u></a></p>
<p>Я с интересом посмотрел некоторые объявления, сделанные в ходе EMC World. Отчасти оттого, что это новости нашего конкурента, отчасти же потому, что я нерд, которому интересно увидеть и узнать что-нибудь по настоящему технически крутое.</p>
<p>BTW: Хочу сказать спасибо Mark Kulacz за его помощь в отношении пруфов. Марк, хоть это и трудно признать, даже больший нерд, чем я.</p>
<p>??так… EMC сделал что-то новое. Демонстрация возможного VNX следующего поколения (VNX2?), недоступна на момент написания поста (на самом деле было много суеты вокруг того, что это существует пока только в виде лабораторного стенда, и т.д.).</p>
<p>Один из результатов, который был показан, это увеличенная производительность по сравнению с их топовой на сегодня VNX7500.</p>
<p><strong>Задача данной статьи - показать, что продемонстрированное увеличение производительности не пропорционально тому, что на этот счет утверждает EMC, и что оно достигнуто не настолько за счет программного улучшения, как это утверждает EMC, и более того, это может означать, что текущие системы VNX внезапно &quot;устареют&quot; без разумной на то причины.      <br />Хотя это и позволит EMC опять поднять денег.</strong></p>
<p><strong>Много шумихи</strong> было создано вокруг того, что софт есть ключевой движущий элемент за всеми улучшениями производительности, и того, как теперь можно задействовать все ядра CPU, в то время как раньше, оказывается, это было не так. Софт - здесь, софт - там, и, в общем, это было основной темой всего мероприятия.</p>
<p>OK, убедили. Многоядерность и расширение ее поддержки в сегодняшнем IT-ландшафте это то, о чем говорят все. Параллелизация задач - ключевой момент сегодняшнего IT.</p>
<p>Затем они показали интересный график (я взял слайд из публичного видео):</p>
<p><img title="MCX_core_util_arrow[1]" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="MCX_core_util_arrow[1]" src="/pics/36a340e362d8_B0CD/MCX_core_util_arrow1.png" width="600" height="332" /></p>
<p>&#160;</p>
<p>Я добавил стрелки для уточнения наиболее интересных моментов.</p>
<p><strong>Отметьте, что на левом графике, согласно утверждению самой EMC, текущий VNX использует примерно 2,5 ядра из 6 имеющихся</strong>, если вы составите все столбики вместе (например, Core 0 загружен целиком, Core 1 на 50%, Cores 2-4 заняты немного, Core 5 не занят почти ничем). <em>Это важно, и мы к этому еще вернемся</em>. Но, в настоящий момент, <em>если это в самом деле правда</em>, это показывает крайнюю неэффективность использования многоядерности. Все выглядит так, будто ядра процессора фиксированно закреплены за процессами - Core 0 занимается только RAID, например, и так далее. Возможно это способ снизить накладные расходы на переключение контекстов?</p>
<p>Затем они показали, как работает новый контроллер, с 16 ядрами на контроллер (нынешний VNX7500 имеет 6 ядер на контроллер).</p>
<p>ОК, пока все нормально.</p>
<p>Затем они показали, как <em>Святым Духом Программного Обеспечения</em> (в оригинале <em>By The Holy Power Of Software</em>, прим <em>romx</em>), они могут равномерно задействовать все 16 ядер этого нового контроллера поровну (картинка справа).</p>
<p>Далее была интересная часть. Они показали тесты в IOmeter для этого нового контроллера (и только для него).</p>
<p><strong>Они упомянули, что текущий VNX 7500 достигает 170,000 8K операций произвольного чтения (random reads) с SSD</strong> (эти цифры сами по себе хороший подарок для разговора с продавцами EMC, обещающими безумные показатели VNX7500 IOPS). ?? что существующая в текущей модели проблема с производительностью связана с тем, что софт не может загрузить равномерно все ядра.</p>
<p>Затем был показан экспериментальный контроллер, продемонстрировавший показатели ввода-вывода в пять раз выше. Это, конечно, впечатляет, хотя вряд ли это <a href="http://recoverymonkey.org/2012/07/26/an-explanation-of-iops-and-latency/">реально демонстрирует производительность</a>, но я принимаю это как факт того, что они хотя бы попытались продемонстрировать то, насколько на операциях одного только чтения можно получить от дополнительных ядер, плюс это круто выглядящие маркетинговые цифры.</p>
<p>Записи это всегда отдельная сложность для дискового массива, конечно. Как мы <a href="http://recoverymonkey.org/2013/05/06/more-emc-vnx-caveats/">уже сталкивались с этим</a> производительность VNX в этом случае падает радикально.</p>
<p><strong>Однако, это все оставило несколько больших вопросов:</strong></p>
<ol>
<li>Если все это действительно результат оптимизации софта VNX, будет ли это также работать для VNX7500? </li>
<li>Почему бы не показать новый софт также и на VNX7500? Это, возможно, даст увеличение производительности более 2x раз, просто за счет лучшей (равной) загрузки ядер контроллера. <strong>Но если простой апргрейд софта на VNX7500 позволит стораджу заработать вдвое быстрее, разве не будет это сильнейшим подтверждением утверждения EMC, что “софт – рулит”?</strong> Почему нужно пропускать такую прекрасную возможность это доказать? </li>
<li>?? если с новым софтом на VNX7500 мы можем получить, допустим, 400 000 read IOPS на том же тесте, разница между старым контроллером и новым не будет столь радикальна, как ее показывает EMC… правильно? :)</li>
<li><strong>Но если загрузка ядер CPU VNX7500 все же НЕ столь плоха, как это показывает EMC в графике выше (зачем было бы заморачиваться с двумя дополнительными ядрами на VNX7500 в сравнении с VNX5700, если бы это было действительно так), тогда улучшение производительности вызвано в основном простой дополнительной мощностью нового железа контроллера. Что, опять же, противоречит заявленной идее про то, что “софт – рулит”!</strong> </li>
<li>Почему клиентам EMC нужен XtremeIO если новый VNX так быстр? Как насчет VMAX? ;)</li>
</ol>
<p>Пункт #4 наиболее важен. Например, EMC годами рекламирует улучшение многоядерной производительности. Текущий релиз VNX FLARE имеет якобы на 50% выше эффективность использования ядер, чем <a href="http://virtualgeek.typepad.com/virtual_geek/2012/05/vnx-inyo-is-going-to-blow-some-minds.html">ранее</a>. ??, ранее, в 2008 году, многоядерность <a href="http://www.ortra.com/emc-2008/present/store/Reem%20Morad%20(%20Matrix),%20CLARiiON%20CX4,%20Redefining%20the%20Mid%20Range%20Storage.pdf">рекламировалась</a> как дающая двукратное преимущество по сравнению с софтом, использовавшемся ранее. Однако, после всего последовательного улучшения, график выше показывает нам крайне низкую эффективность использования ядер. <strong>Кто же тут прав?</strong> </p>
<p><strong>??ли же, может быть, новый контроллер демонстрирует улучшение производительности не столько за счет волшебного нового софта, а, в основном, за счет значительно быстрее работающей аппаратной платформы – более быстрый процессор Intel (выше частота, не просто больше ядер, плюс более эффективные инструкции), более новый чипсет, быстрее память, быстрее SSD, шина, и т.д, и т.п. Потенциально <em>новый контроллер сам по себе в 3-5 раз быстрее, без всякого софта</em>, только за счет новой аппаратной платформы.</strong></p>
<p>Однако я, или, по крайней мере действующие клиенты VNX, скорее всего будут более всего обеспокоены пунктом 1. Разу уж, как нам пообещали, все это решается программным путем&#8230; :)</p>
<p><strong>Если новый софт так здорово помогает, будет ли он доступен на существующей платформе VNX? Это должно было бы пойти им на пользу, с учетом того, что, согласно EMC, часть ядер там не занимается ничем. Бесплатное улучшение производительности!</strong></p>
<p>Однако… Если они все же НЕ сделают это доступным, единственным рациональным объяснением тут будет то, что они намерены в очередной раз принудить своих клиентов купить новое железо, и сделать новый <em>forklift upgrade</em> (CX-&gt;VNX-&gt;”new box”).</p>
<p>??ли же, возможно, это улучшение требует аппаратных изменений в платформе. Но тогда это, вообще говоря, разрушает всю рассказанную историю про &quot;волшебный софт&quot;, который “рулит”.</p>
<p>Если все это про &quot;Software Defined Storage&quot;, то почему этот софт так привязан к железу?</p>
<p>У нас в лабе стоят старинные NetApp FAS3070. ??х контроллеры выпущены поколения назад (2006 год, древняя старина), и при этом <strong>они работают под текущей версией кода GA ONTAP</strong>. Они были произведены примерно 3-4 поколения контроллеров назад, и, на момент их выпуска, они работали изначально под управлением софта очень, очень отличающегося от существующего сегодня, и все равно нормально работающего на них.</p>
<p>??ногда я думаю, что мы тем самым портим наших клиентов :)</p>
<p>Могут ли CX3-80 (самая навороченная в линейке CX3, такая же древность, как NetApp FAS3070) взять и использовать код, показанный на EMC World? Могут ли они взять действующий GA-код для VNX? Могут ли они взять хотя бы код для CX4? Может ли CX4-960 (опять же, самая навороченная из CX4) взять софтовый код с VNX? Я могу продолжать. Но все это рисует только лишь угнетающую картину того, как EMC относится к защите инвестиций в железо для своих клиентов.</p>
<p>Но разговор про работу с устаревающим оборудованием и его поддержка это отдельная крутая история, о которой мы поговорим в другой раз.</p>
<p>D</p>
<p>??сточник &lt;<a href="http://recoverymonkey.org/">http://recoverymonkey.org/</a>&gt; </p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="netapp/index.html" rel="tag">netapp</a>, <a href="vnx.html" rel="tag">vnx</a><br />					Раздел: <a href="../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../1262/trackback.html#comments" title="Комментарий к записи Новинки на EMC World или всегда ли &ldquo;рулит софт&rdquo;?">Комментарии (6)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1257">
				<h2 class="posttitle"><a href="../1257/trackback.html" rel="bookmark" title="Permanent Link to Снова про EMC VNX">Снова про EMC VNX</a></h2>
				<div class="postmetadata">13 Май 2013, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Так как я теперь, стихийным образом (вернее будет сказать попустительством EMC, похоже положившим на продвижение VNX в русскоязычном интернете крупный дюймовый болт) являюсь, в некотором роде, неожиданно для себя, <a href="https://www.google.ru/search?hl=en&amp;as_q=EMC+VNX&amp;lr=lang_ru&amp;cr=countryRU">авторитетным источником информации</a> про <a href="http://yandex.ru/yandsearch?text=EMC+VNX&amp;lr=213">эту модель стораджей</a>, будем пользоваться сим фактом. ?? сегодня вам очередной перевод из блога Recoverymonkey, с очередными укусами за жёппу ;).</p>
<p>Напоминаю, что тексты, которые я публикую в этом блоге на правах переводов, являются переводами, а не моими текстами, поэтому спорить со мной, наезжать в коментах, или упрекать в чем-либо, например за тон изложения – бесполезно. Сходите по приведенной ссылке, и если испытываете той или иной степени попаболь от того, что автор пишет – скажите это <a href="http://recoverymonkey.org/2013/05/06/more-emc-vnx-caveats/">вот прямо там</a>.</p>
<h2><a href="http://recoverymonkey.org/2013/05/06/more-emc-vnx-caveats/">Еще несколько предостережений про EMC VNX</a></h2>
<p><a href="http://recoverymonkey.org/2013/05/06/more-emc-vnx-caveats/"><u><font color="#0066cc">http://recoverymonkey.org/2013/05/06/more-emc-vnx-caveats/</font></u></a>.</p>
<p>Недавно, когда мне по работе пришлось столкнуться с конкурентным предложением нашим клиентам в виде системы хранения EMC VNX, я заметил, что EMC использует несколько утверждений для доказательства своего превосходства (или, хотя бы не отставания).</p>
<p>Я уже писал в недалеком прошлом вот <a href="http://recoverymonkey.org/2011/01/13/questions-to-ask-emc-regarding-their-new-vnx-systems/">эту</a> статью (есть <a href="../868/trackback.html">перевод</a>, прим. romx), и сегодня я хочу углубленнее рассмотреть несколько аспектов. Обсуждаемые сегодя темы:</p>
<ol>
<li>Эффективность хранения на VNX </li>
<li>Утверждение, что LUN-ы могут обслуживаться обоими контроллерами </li>
<li>Утверждение, что autotiering помогает в большинстве задач </li>
<li>Утверждение, что сторадж-пулы – это просто </li>
<li>Производительность thin provisioning </li>
<li>Новые снэпшоты VNX </li>
</ol>
<p>Я буду указывать по каждому утверждению ссылки на актуальные документы EMC, а то иначе это ничем не будет отличаться от работы “маркетингового робота”.</p>
<h3>Эффективность хранения на VNX</h3>
<p>EMC любит утверждать, что на ее системах не требуется отдать 10% пространства в “налог” OS, как это требуется у NetApp <em>(WAFL reserve, прим. romx)</em>. Однако, как утверждается по&#160; <a href="https://community.emc.com/message/713193">ссылке</a> рекомендации best practices указывают, что на <em>autotiered pool</em>, как минимум 10% свободного места должны быть всегда доступны <strong>на каждом из tier-ов</strong>, для обеспечения его нормальной работы.</p>
<p>Также присутствует минимально 3GB оверхеда на каждом LUN, плюс оверхед на метаданные, который вычисляется по формуле, приведенной в статье по ссылке. Плюс дополнительно метаданные, если вы используете дедупликацию.</p>
<p>Моя позиция: Не бывает бесплатного сыра. Если вы хотите новых фич, использующих для своей работы пулы, за это есть определенная цена. В противном случае продолжайте использовать традиционные RAID-группы, там нет новых фич, но они дают гораздо более предсказуемую производительность и характер использования пространства дисков.</p>
<h3>LUN-ы могут обслуживаться обоими контроллерами, обеспечивая “load balancing”</h3>
<p>Вот тут пошло веселье. Утверждается, что <em>LUN ownership</em> может быть мгновенно переключен с одного контроллера VNX на другой, чтобы обеспечить балансировку их загрузки. Ну, как всегда тут есть тонкости. Важно также отметить здесь, что на момент написания этого поста, в VNX не было никаких средств для организации автоматической балансировки <em>LUN ownership</em> в зависимости от загрузки контроллеров.</p>
<ol>
<li>Если это делается с традиционными RAID LUN-ами: Передача <em>LUN ownership</em> делается без проблем. Это так, как было всегда. </li>
<li>Если же LUN находится в пуле – это история другая. <strong>Нет способа быстро переключить <em>LUN ownership</em> для такого LUN, без существенного влияния этого процесса на производительность системы. </strong></li>
</ol>
<p>Обильная информация по тому поводу может быть найдена <a href="https://community.emc.com/docs/DOC-22906">здесь</a>. Говоря коротко: Вы не можете менять <em>LUN ownership</em>, при использовании пулов, вместо этого вам придется перенести содержимое LUN, на другой LUN, на другом контроллере (именно на другой LUN, перенос LUN-а “как есть” может создать проблемы), в противном случае готовьтесь заплатить производительностью.</p>
<h3>Утверждение, что autotiering помогает на большинстве задач</h3>
<p>Не торопитесь. <em>(в оригинале: “Not so FAST”, прим. romx).</em> Собственные документы EMC, такие как “best practice guides” полны предупреждений и замечаний, относительно использования autotiering. Несмотря на то, что эта фича фактически пропагандируется в каждой кампании продаж, как создающее множество преимуществ гигантское достоинство.</p>
<p>Вот, например, очень подробный документ “<a href="https://community.emc.com/servlet/JiveServlet/previewBody/19006-102-3-67659/h10739-emc-scaling-performance-oracle-virtual-machine-wp.pdf">EMC Scaling performance for Oracle Virtual Machine</a>“, этот график найден там на странице 35:</p>
<p><img title="fastoracle[1]" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="fastoracle[1]" src="/pics/fce3d4f9ee55_DCDB/fastoracle1.png" width="600" height="403" /></p>
<p>Стрелки добавил я. Отметьте, что наибольшее преимущество в производительности достигается тогда, когда кэш правильно сконфигурирован с точки зрения сайзинга.<strong> Дополнительные 5 SSD для VNX tiering (FAST VP) почти не дают существенного повышения производительности на задаче с нагрузкой типа базы данных.</strong></p>
<p>Можно только гадать, какой прирост дали бы эти 4 SSD, если бы были установлены не в autotiering, а увеличили бы кэш… </p>
<p>Быть может, если бы все 8 SSD были бы использованы как all-flash-storage, это было бы даже еще быстрее, чем 4 cache SSD и 5 tier SSD, но это просто лишний раз продемонстрирует слабый “замах” autotiering-а FAST-VP и недостатки его маркетинга.</p>
<h3>Утверждение, что storage pools это упрощение работы со стораджем</h3>
<p>Типичный подход сэйла EMC к пользователю для продажи VNX это: используйте единый, суперпупер-простой пул дисков с автотирингом данных на нем. Однако вопреки тому, что утверждает маркетинговое шоу, к сожалению, с использованием пулов VNX сложность снижается совсем не так, как это обещается, наприме просто потому что пулы рекомендуются совсем не для любой задачи и не для любой рабочей нагрузки. Рассмотрим типовой сцеарий развертывания VNX, смоделированный по документам best practice:</p>
<ol>
<li>Журнальные LUN-ы RecoverPoint рекомедуется держать на отдельной группе RAID10 </li>
<li>LUN-ы SQL log на отдельной группе RAID10 </li>
<li>Exchange 2010 log LUN-ы на отдельной группе RAID10&#160; </li>
<li>Exchange 2010 data LUN-ы могут находиться в пуле, но лишь в случае, если он состоит из однотипных дисков, в противном случае нужно использовать несколько традиционных RAID-групп </li>
<li>SQL data могут быть помещены на autotiered pool </li>
<li>VM могут быть на отдельном пуле, или жить совместно на пуле SQL </li>
<li>Репозиторий VDI linked clone может использовать SSD в отдельной группе RAID10 </li>
</ol>
<p>Ну, отлично. Я понимаю все достоинства разделения ввода-вывода, которыми продиктованы рекомендации выше. Однако “продающая” позиция компании в отношении пулов и autotiering это то, что они призваны снизить сложность, общую стоимость решения, улучшить производительность и эффективность распределения пространства. Ясно, что вряд ли все приведенные варианты могут одновременно встретиться в одном стораже на практике. Но что за причина, отчего они не могут быть в одном, или двух пулах, и использовать какой-то вид QoS на массиве, чтобы обеспечить приоритезацию трафика?</p>
<p>?? что получится, если вы отдали лишнего на диски под традиционные RAID-группы в примере выше? Как насчет вернуть или перераспределить это неиспользуемое там пространство другим задачам?</p>
<p>А что насчет варианта, когда места не хватает, как просто вам будет добавить немножко пространства или производительности? (Не в 2-3 раза, допустим, а всего на 20% больше). Сможете вы расширить традиционные VNX RAID-группы на несколько дисков?</p>
<p><strong>?? как там насчет общей эффективности хранения, если все эти задачи нам придется держать отдельно друг от друга?</strong> Хмммм… </p>
<p>Для подробностей смотрите документы по дизайну хранилища под <a href="http://www.emc.com/collateral/hardware/white-papers/h8888-exch-2010-storage-best-pract-design-guid-emc-storage.pdf">Exchange</a> и <a href="http://www.emc.com/collateral/hardware/white-papers/h8987-mssql-vnx5700-wp.pdf">SQL</a>.</p>
<h2>Производительность thin provisioning</h2>
<p>О,это просто здорово. Производительность VNX thin provisioning сильно хуже в сравнении с <em>thick</em>, а они оба – в сравнении с традиционными RAID-группами. Проблема с производительнстью возникает прежде всего вследствие того, как VNX выделяет пространство при записи на thin LUN. При этом используются блоки, <font style="background-color: #ffff00"></font><font style="style">размером</font> 8KB. Хорошее описание того, как распределяется пространство на сторадж-пуле можно найти <a href="http://vjswami.com/2011/03/05/emc-storage-pool-deep-dive-design-considerations-caveats/">здесь</a>&#160;<em>(у меня есть </em><a href="../969/trackback.html"><em>перевод</em></a><em>, прим. romx)</em>. VNX пишет в пулы сементами, размером 1GB. Thick LUN-ы резервируют столько сегментов, размеров 1GB, сколько необходимо, и производительность получается вполне приемлемая. Thin LUN-ы не резервируют место, и, тем самым, не имеют возможности оптимизировать записи и чтения – в результате начинается фрагментация, вдобавок к более высокой загрузке CPU, дискового оверхеда, больше занимается памяти контроллера, все это для обслуживания thin LUN-ов. </p>
<p>Вновь смотрим документ по <a href="http://www.emc.com/collateral/hardware/white-papers/h8888-exch-2010-storage-best-pract-design-guid-emc-storage.pdf">дизайну хранилища под Exchange 2010</a>, страница 23:</p>
<p><img title="vnxthin[1]" style="border-left-width: 0px; border-right-width: 0px; background-image: none; border-bottom-width: 0px; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-top-width: 0px" border="0" alt="vnxthin[1]" src="/pics/fce3d4f9ee55_DCDB/vnxthin1.png" width="600" height="241" /></p>
<p>Снова, стрелки на скриншоте, выделяющие важные моменты, добавлены мной:</p>
<ol>
<li>Thin provisioning не рекомендован для высокой нагрузки на VNX </li>
<li>Конечно, он такой медленный сам по себе, что вы должны делать ваши thin pools хотя бы на RAID10!!! </li>
</ol>
<p>Но погодите, thin provisioning придуман для экономии места на дисках, а теперь я должен использовать его на RAID10, который это место просто пожирает?</p>
<p>Какой-то нонсенс.</p>
<p>А что если пользователь желает повышенную надежность и, поэтому, хочет использовать RAID6 на весь пул? Как быстро с этим будет работать thin provisioning?</p>
<p>А, вдобавок у VNX нет способа устранить фрагментацию, которая бесчинствует на их thin LUN-ах. Только через миграцию на другой LUN.</p>
<h3>Новые снэпшоты VNX</h3>
<p>В VNX появился способ облегчить удар по производительности, присущий традиционным снэпшотам FLARE, перейдя от метода создания снэпшота COFW (Copy On First Write) на ROFW (Redirect On First Write).</p>
<p>Проблема?</p>
<p>Новым снэпшотам VNX требуется использование пулов и thin LUN-ов. Это кажется только технической особеностью, но…</p>
<p>Это те самые две фичи VNX, которые существенно понижают его производительность.</p>
<p>Есть и другие существенные проблемы с новыми сэпшотами VNX, но это тема другого поста. Так что ничего удивительного, что EMC проталкивает RecoverPoint гораздо активнее, чем свои снэпшоты…</p>
<h3>??того</h3>
<p>Существует маркетинг, и существует “инженерная” реальность.</p>
<p>Поскольку VNX может работать как с пулами, так и с традиционными RAID-группами, маркетинг мудро выбрал не слишком вдаваться в детали о том, что с чем там у него работает.</p>
<p>Реальность же состоит в том, что все новые фичи работают только с пулами. Но тут есть несколько весьма существенных засад.</p>
<p>Если вы рассматриваете вариант покупки VNX – по меньшей мере убедитесь, что понимаете, какая из проталкиваемых маркетингом фич будет работать для конкретно вашей задачи. Попросите сделать полную схему разбиения системы на LUN-ы (LUN layout).</p>
<p>?? опять же, мы никак еще не говорили про использование RAID-6 для защиты данных в пуле, это опять же отдельная история для отдельного поста.</p>
<p>D</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="vnx.html" rel="tag">vnx</a><br />					Раздел: <a href="../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../1257/trackback.html#comments" title="Комментарий к записи Снова про EMC VNX">Комментарии (11)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1218">
				<h2 class="posttitle"><a href="../1218/trackback.html" rel="bookmark" title="Permanent Link to Сингапурская идиллия :)">Сингапурская идиллия :)</a></h2>
				<div class="postmetadata">29 Сентябрь 2012, 10:29 <!-- от  --></div>
				<div class="postentry">
					<p>Выходные – день оффтопиков. Вот такая идиллическая картина получилась в пятницу. Причем там еще два шкафа IBM-овских серверов сбоку на этой кухне. Не хватает только HP и Hitachi. :)</p>
<p> <a title="IMG_2700 by romx, on Flickr" href="http://www.flickr.com/photos/romx/8034590048/"><img alt="IMG_2700" src="http://farm9.staticflickr.com/8449/8034590048_c7f89822f0.jpg" width="500" height="375" /></a></p>
<p>&#160;</p>
<p>Посты в блоге скоро возобновлю, просто в этот четверг работа не давала времени причесать уже написанный пост. В том числе и та, что на фото выше :)</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="fas2040.html" rel="tag">fas2040</a>, <a href="netapp/index.html" rel="tag">netapp</a>, <a href="vnxe.html" rel="tag">vnxe</a><br />					Раздел: <a href="../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;" rel="category tag">новости</a>&nbsp;&nbsp;|&nbsp;
					<a href="../1218/trackback.html#comments" title="Комментарий к записи Сингапурская идиллия :)">Комментарии (7)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-982">
				<h2 class="posttitle"><a href="../982/trackback.html" rel="bookmark" title="Permanent Link to EMC VNX5100 &ndash; &ldquo;не совсем VNX&rdquo;">EMC VNX5100 &ndash; &ldquo;не совсем VNX&rdquo;</a></h2>
				<div class="postmetadata">4 Август 2011, 10:24 <!-- от  --></div>
				<div class="postentry">
					<p>Я тут уже <a href="vnx.html">несколько раз останавливался</a> на новых продуктах <a href="emc/index.html">EMC</a>, их достоинствах, недостатках, и скрытых, возможно, для кого-то, “подводных камнях”, но не планировал снова возвращаться к теме, по крайней мере, пока не появятся какие-нибудь новости. Однако столкнулся с рядом не очень приятных фактов в жизни моих коллег-кастомеров, и вынужден несколько подробнее остановиться на одной из моделей линейки VNX – VNX5100, самой “младшей” в серии VNX.</p>
<p>Дело в том, что я уже не в первый раз сталкиваюсь с попытками сейлов партнеров EMC продавать EMC VNX5100 клиентам, не указывая ясно им на ряд ограничений конкретно этой модели. Скорее всего это не злонамеренное действие, а просто недостаток информации, или, возможно, просто нехватка компетенции в новых продуктах. Так или иначе, я хочу остановиться на этой тонкости для кастомеров чуть более подробно.</p>
<p>EMC VNX5100 это самая “младшая” в “старшей” линейке VNX (напомню, что VNXe, несмотря на схожесть аббревиатуры, это совсем иная система), а если учесть и весьма низкую цену, по которой она предлагается, неудивительно, что ее активно продвигают на российском рынке. Разумеется с VNX5100 частенько приходится сталкиваться и предложениям NetApp. Однако, сравнивая, стоит помнить, что VNX5100 – это “не совсем VNX”, и вот почему.</p>
<p>Все мы видели эффектный маркетинговый launch EMC, все мы помним что именно на нем было “выкачено” в качестве основных, ключевых возможностей систем новой линейки VNX. Это:</p>
<ul>
<li>Мультипротокольность (EMC называет это Unified Storage)</li>
<li> Порты 10Gb Ethernet и iSCSI “в базе”</li>
<li>Файловые протоколы (NAS), доступные “из коробки”, на той же системе, без допзатрат</li>
<li>Дедупликация</li>
<li>FAST VP (tiering) и FASTcache</li>
</ul>
<p>Это то, что, в первую очередь, ассоциируется у пользователя с понятием “VNX”, о чем говорится во всех маркетинговых материалах, и что клиент логично надеется получить, покупая систему с названием VNX. </p>
<p>Проблема в том, что в VNX 5100 ничего этого нет.</p>
<p>EMC VNX 5100 это “чисто FC” сторадж, этакий “Clariion CX4 с дисками SAS”, поэтому:</p>
<ul>
<li>Только FC (нет &#8216;unified&#8217; даже в понимании EMC), следовательно:</li>
<ul>
<li>Нет файловых протоколов NFS, CIFS.</li>
<li>Нет дедупликации (даже в ее ограниченном &quot;файловом&quot; понимании EMC)</li>
<li>Нет iSCSI и FCoE</li>
</ul>
<li>Нет FAST VP (<a href="../926/trackback.html">tiering</a>)</li>
<li>Поддерживает использование в FASTcache только одного SSD емкостью 100GB (93,1GB эффективной, форматированной емкости. Плюс необходимые mirror и spare).</li>
<li>Апгрейд на другой, настоящий VNX (5300/5500/5700) возможен только полной заменой системы (нет data in place upgrade). Это вообще отдельная система, отличающаяся по организации данных от всех остальных VNX/VNXe.</li>
</ul>
<p>Является конкурентом NetApp в сегменте FAS2040 или FAS3210, но:</p>
<ul>
<li>Максимальное количество дисков невелико - 75 (против 136 у 2040 или 240 у 3210), то есть всего около 30 дисков данных, при использовании RAID-10, при нехватке дисков - см выше про невозможность апгрейда на 5300/5500/5700.</li>
<li>Только 4 FC-порта для хостов/фабрик и 2 порта SAS для полок (на контроллер, на пару - 8/4 ) без возможности расширения.</li>
<li>Очень ограниченное предложение по софтовой части решения (за счет отсутствия NAS-компоненты и связанных с ним софтовых продуктов EMC).</li>
</ul>
<p>Да, VNX 5100 дешев. <a href="../884/trackback.html">Ранее я уже говорил</a>, почему “цена ниже рыночной” должна вас сперва настораживать, а потом уже радовать. Обычно за этим скрывается ряд засад, подобных указанным выше.</p>
<p>Сам по себе 5100 вполне себе достойный продукт, проблема лишь в том, что его продают как “VNX” которым, по крайней мере в глазах пользователя, он не является.</p>
<p>“NetApp?” – говорит сэйл компании партнера EMC – “Мы вам предложим кое-что получше! Новый сторадж от лидера отрасли – VNX 5100! Вот, возьмите брошюру (<em>в которой рассказывается про 5300 и 5700, а пункты про 5100 сопровождены маленькой серой звездочкой-ссылкой на мелкий шрифт в конце: “not available for 5100”</em>), а главное: она в полтора раза дешевле этого NetApp! Что раздумывать, берите!”</p>
<p>В результате пользователь, заплативший заманчиво низкую цену за “новую систему хранения от лидера рынка” получает, фактически, систему предыдущего поколения, без всех рекламируемых новых функциональных фич, да еще и без возможность апгрейда, оказываясь разом на “боковой тупиковой ветке” продукта.    <br />Зато задешево. :-|</p>
<p>Остерегайтесь “заманчиво низкой цены”. Внимательно читайте мелкий шрифт. ?? не забывайте, что, очень часто, сэйлы, и даже пресэйлы партнеров, особенно многопрофильных интеграторов, могут не вполне твердо владеть темой в новых продуктах вендора.</p>
<p>Такой вот вам Caveat Emptor*</p>
<p>PS. Я, как и ранее, должен отметить, что я не являюсь специалистом в системах EMC, и если в перечисленных выше фактах о 5100 в моих источниках ненамеренно вкралась ошибка, то, если вы куда больший специалист, и у вас есть данные о том, что я по ошибке написал что-то неверное про EMC, обязательно сообщите мне это, я обязательно поправлю это место в тексте.</p>
<p>PPS. Благодаря Сергею из комментариев было поправлено несколько не вполне ясно написанных мест и неточностей.
</p>
<p><em><font color="#cccccc">* </font></em><a href="http://en.wikipedia.org/wiki/Caveat_emptor"><em><font color="#cccccc">Caveat Emptor</font></em></a><em><font color="#cccccc"> (лат.) – “Покупатель – остерегись!”, приписка традиционно делаемая при изложении чувствительной для покупателя информации, особенно в области технических характеристик или условий покупки.</font></em></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="5100.html" rel="tag">5100</a>, <a href="emc/index.html" rel="tag">emc</a>, <a href="vnx.html" rel="tag">vnx</a>, <a href="vnx5100.html" rel="tag">vnx5100</a>, <a href="warning.html" rel="tag">warning</a><br />					Раздел: <a href="../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>&nbsp;&nbsp;|&nbsp;
					<a href="../982/trackback.html#comments" title="Комментарий к записи EMC VNX5100 &ndash; &ldquo;не совсем VNX&rdquo;">Комментарии (16)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-969">
				<h2 class="posttitle"><a href="../969/trackback.html" rel="bookmark" title="Permanent Link to EMC Storage Pools &ndash; как это работает &ldquo;под крышкой&rdquo;.">EMC Storage Pools &ndash; как это работает &ldquo;под крышкой&rdquo;.</a></h2>
				<div class="postmetadata">27 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Продолжаю свои изыскания в теме технологий EMC. На этот&#160; раз я заинтересовался механизмом, лежащим в основе всех новых фишечек EMC CLARiiON/VNX – так называемым Storage Pool. Самым интересным и понятным для меня объяснением оказалась статья блоггера <a href="http://virtualeverything.wordpress.com">virtualeverything</a> (он не сотрудник EMC, а, как я понял, работает в партнере-интеграторе, и имеет довольно широкое поле зрения, в котором и продукты EMC, и NetApp, и, например, Hitachi). Незначительно сокращенный перевод этой его заметки я предлагаю вашему вниманию.</p>
<h2>Глубокое погружение в тему EMC Storage Pool.</h2>
<p>Posted by <a href="http://virtualeverything.wordpress.com">veverything</a> on March 5, 2011</p>
<p>??спользование Storage Pools это довольно частая тема для дискуссии с моими коллегами и пользователями. Многая информация о устройстве и механизмах работы не слишком распространена или известна, поэтому я решил провести мое собственное исследование на этот счет, и поделиться его результатами.</p>
<p>Некоторое время назад EMC представила в своей линейке систем хранения CLARiiON новый принцип так называемого Virtual Provisioning и Storage Pools. Основная идея заключалась в упрощении администрирования системы хранения. Традиционный метод управления хранилищем это взять дисковый массив, наполненный дисками, создать из них дискретные группы RAID, и затем нарезать из этих групп LUNы, которые, затем, предоставляются хостам. Дисковый массив, в зависимости от своего размера, при этом может содержать до нескольких сотен таких групп, и часто превращается в архипелаг разрозненных &quot;островков хранения&quot; на этих RAID-группах. Это может вызывать серьезные затруднения при планировании пространства хранилища, чтобы устранить проблему нерационально потраченного при таком разбиении места, так как у большинства пользователей их потребности к хранилищу, и планы как разбить под задачи, меняются со временем, и, обычно, еще не ясны до конца в &quot;день 1&quot;. Нужны были средства гибкого и простого администрирования, и родилась идея Storage Pools.</p>
<p>Storage pools, как следует из его имени, позволяет администраторам создавать &quot;общий массив&quot; (pool) хранения. В ряде случаев вы даже можете создать один большой, общий пул, куда входят все диски системы хранения, что значительно упрощает процессы администрирования. Больше нет отдельных пространств, не нужно углубляться в &quot;тонкие материи&quot; устройства и организации RAID group, схем разбиения, и так далее. Кроме того, появилась также технология FAST VP, которая позволяет перемещать блоки данных в соответствии с их активностью, по уровням хранения, например на более емкие и дешевые, или более быстрые и дорогие диски. Просто выделите и назначьте пространство из такого &quot;пула&quot; вашей задаче, динамически, гибко, и при этом еще и позволяя использовать auto tiering. Звучит здорово так? По крайней мере так утверждает маркетинг. Давайте разберемся с тем, как это работает &quot;физически&quot;, и нет ли не вполне очевидных &quot;подводных камней&quot;.</p>
<p>   <a href="../969/trackback.html#more-969" class="more-link"><small>Continue reading &#8216;EMC Storage Pools &ndash; как это работает &ldquo;под крышкой&rdquo;.&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="fast.html" rel="tag">fast</a>, <a href="fast-vp.html" rel="tag">fast vp</a>, <a href="storage-pool.html" rel="tag">storage pool</a><br />					Раздел: <a href="../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;" rel="category tag">howto</a>,  <a href="../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../969/trackback.html#comments" title="Комментарий к записи EMC Storage Pools &ndash; как это работает &ldquo;под крышкой&rdquo;.">Комментарии (11)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-950">
				<h2 class="posttitle"><a href="../950/trackback.html" rel="bookmark" title="Permanent Link to Учу матчасть :)">Учу матчасть :)</a></h2>
				<div class="postmetadata">7 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как заповедали старшие – изучаю вооружение потенциального противника :)</p>
<p>Нашел тут <a href="http://www.emc.com/collateral/hardware/white-papers/h5773-clariion-best-practices-performance-availability-wp.pdf">EMC CLARiiON Best Practices for Performance and Availability</a> (FLARE 30, 01.03.2011) и сижу, читаю.</p>
<p>Особо примечательные места выделяю. Ничего, что я сегодня без перевода? По-моему, в выделенном все достаточно понятно.</p>
<p>Уделяю особое внимание storage pool-ам и LUN-ам в них, так как только в pool-ах возможны новые фишечки, такие как FAST и thin provisioning.</p>
<p>   <a href="../950/trackback.html#more-950" class="more-link"><small>Continue reading &#8216;Учу матчасть :)&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="clariion.html" rel="tag">clariion</a>, <a href="emc/index.html" rel="tag">emc</a>, <a href="fast.html" rel="tag">fast</a>, <a href="pool.html" rel="tag">pool</a>, <a href="matchast.html" rel="tag">матчасть</a><br />					Раздел: <a href="../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../category/citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;" rel="category tag">цитаты</a>&nbsp;&nbsp;|&nbsp;
					<a href="../950/trackback.html#comments" title="Комментарий к записи Учу матчасть :)">Комментарии (19)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-940">
				<h2 class="posttitle"><a href="../940/trackback.html" rel="bookmark" title="Permanent Link to Некоторые данные о производительности EMC FASTcache и FAST VP">Некоторые данные о производительности EMC FASTcache и FAST VP</a></h2>
				<div class="postmetadata">4 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Несмотря на то, что EMC наотрез отказывается публиковать результаты производительности систем с FASTcache и FAST VP, храня по этому поводу многозначительное и загадочное молчание, невозможно запретить частным лицам анализировать и делиться своими результатами приобретенных ими систем.</p>
<p>Недавно в интернете удалось раскопать вот такие <a href="http://storagesavvy.com/2011/03/26/real-world-emc-fastvp-and-fastcache-results/">результаты</a>.</p>
<p><a href="http://storagesavvy.com/2011/03/26/real-world-emc-fastvp-and-fastcache-results/"><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image102.png" width="490" height="245" /></a></p>
<p>Некто проследил и опубликовал результаты real world-работы системы NS480 (CX4-480), объемом 480 SAS, SATA и EFD дисков, на 28TB block и 100TB NAS data, оснащенной как FAST VP, так и FASTcache (4 диска EFD по 100GB, общей usable capacity в FASTcache – 183GB).</p>
<p>Пожалуйста, примите во внимание, что это Real World data, то есть реальная производительность системы, используемой под реальную пользовательскую работу (в рассмотренном случае – ERP, VMware, SQL server databases, CIFS shares), а не какие-то специальные тестовые условия. В этом и сила и слабость приведенных результатов. Сила в том, что это реальные, практические данные. Слабость – в том, что, вполне вероятно, мы не видим всех возможностей FAST/FASTcache.</p>
<p><img style="background-image: none; border-bottom: 0px; border-left: 0px; margin: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top: 0px; border-right: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image103.png" width="490" height="245" /></p>
<p>Тем не менее “чем богаты – тем и рады”. Пока EMC хранит загадочное молчание, приходится перебиваться тем, что подарят сердобольные пользователи. Подробно о системе и полученных результатах – по ссылке выше.</p>
<p>Напомню, что, в отличие от EMC, NetApp результаты своих систем с Flash Cache не только не таит, а наоборот, активно пропагандирует и публикует, потому что, по справедливости, там есть чем гордиться.</p>
<p>UPD: В комментариях к посту также нашлось упоминание еще одних результатов.</p>
<p><a href="http://sudrsn.wordpress.com/2011/03/19/storage-efficiency-with-awesome-fast-cache/">http://sudrsn.wordpress.com/2011/03/19/storage-efficiency-with-awesome-fast-cache/</a>    <br /><a href="http://sudrsn.wordpress.com/2011/04/16/emc-fast-cache-increase-performance-while-reducing-disk-drive-count/">http://sudrsn.wordpress.com/2011/04/16/emc-fast-cache-increase-performance-while-reducing-disk-drive-count/</a></p>
<p>Правда там человек темнит о конфигурации.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="fast.html" rel="tag">fast</a>, <a href="fast-vp.html" rel="tag">fast vp</a>, <a href="fastcache.html" rel="tag">fastcache</a>, <a href="netapp/index.html" rel="tag">netapp</a><br />					Раздел: <a href="../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>&nbsp;&nbsp;|&nbsp;
					<a href="../940/trackback.html#comments" title="Комментарий к записи Некоторые данные о производительности EMC FASTcache и FAST VP">Комментарии (7)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-936">
				<h2 class="posttitle"><a href="../936/trackback.html" rel="bookmark" title="Permanent Link to EMC FASTcache и NetApp Flash Cache">EMC FASTcache и NetApp Flash Cache</a></h2>
				<div class="postmetadata">23 Июнь 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как мне тут не раз уже попеняли, некорректно сравнивать <em>tiering-as-datamoving</em> и <em>tiering-as-caching</em>, то есть, например, NetApp Flash Cache и EMC FAST VP. Допустим, как я ни старался в <a href="../926/trackback.html">соответствующей статье</a>, я вас не убедил, что обе эти формы повышения эффективности системы хранения для пользователя есть суть одно. </p>
<p>Хорошо, давайте рассмотрим отдельно особенности, достоинства и недостатки только Flash Cache (PAM-II) и FASTcache.</p>
<p>Во первых, конечно, вы бы могли вместе со мной поехдничать о извилистом пути достижения цели у EMC. Сперва превратить flash-память в &quot;диски&quot; в форме SSD, а затем из этих дисков эмулировать &quot;память&quot; кэша. Ну ладно, допустим не смогли напрямую, оказалось проще через &quot;двойную эмуляцию&quot;.</p>
<p>Во-вторых если мы посмотрим на то, как у EMC предполагается использовать SSD диски под FASTcache, вы, уверен, вместе со мной поразитесь неффективности.</p>
<p>Допустим, мы разворачиваем систему хранения для 1000 рабочих мест под десктопную виртуализацию на XenDesktop. <a href="http://itzikr.wordpress.com/2011/06/08/citrix-xendesktopp-5-on-emc-vnx-match-made-in-heaven-part1/">Рекомендуемая схема</a> включает в себя три диска SSD, из которых один - hotspare, а два других образуют &quot;зеркало&quot; RAID-1. Таким образом, очевидно, что эффективность использования flash в такой конструкции будет примерно 33%, то есть одна треть от купленной емкости flash. Да, при увеличении объема FASTcache, кроме роста цены будет расти и эффективность использования, но она никогда не превысит 50%, за счет использования RAID-1 (плюс hotspare). Больше половины затраченных на SSD денег будут простаивать. По контрасту, если вы покупаете 256GB Flash Cache, вы можете использовать под кэширование и ускорение работы с данными все 256GB, сто процентов от затраченных на них денег.</p>
<p>В третьих, стоит обратит внимание, что использование SSD как дисков вынуждает EMC разместить этот кэш &quot;снаружи&quot; контроллера, в &quot;петле&quot; дискового ввода-вывода на интерфейсе SAS. В то время, как у NetApp плата Flash Cache располагается непосредственно на системной шине контроллера, на PCIe (PCIe v2.0 x8 в моделях 3200/6200, пропускная способность 32Gbit/s в каждом направлении). То есть взаимодействие контроллера с кэшем в случае FASTcache такое: данные пришли через ввод-вывод на контроллер, по каналу SAS к дискам, вышли через другой порт и записались на SSD по интерфейсу SAS. Затем, если к данным кэша обращение, они должны считаться через дисковый канал ввода-вывода по SAS обратно в контроллер, и отдаться через третий канал ввода-вывода, собственно инициатору запроса, по FC или iSCSI, или NFS/CIFS. Все это, безусловно, нагружает и так не бесконечные возможности дискового канала ввода-вывода к полкам, и, потенциально, может привести к ограничению производительности.</p>
<p>Наконец, стоит помнить, что, хотя в FASTcache удалось значительно снизить размер оперируемого &quot;чанка&quot; до 64KB, против гигабайта в FAST-&quot;просто&quot;, все же этот размер достаточно велик для многих задач, работающих в random read/write, размер блока кэша, значительно превышающий рабочий для соответствующей файловой системы или задачи, например блока базы данных, также снижает эффективность использования такого кэша, если, допустим, только 4KB из блока в 64KB нам действительно нужны (при 100% random это довольно обычная ситуация), то это значит, что эффективность кэша составляет лишь 1/16 от своего фактического объема.</p>
<p>Что же в плюсах? Очевидно, что это активно &quot;педалируемая&quot; EMC возможность работы такого кэша на запись. Особенно на это нажимают в сравнении с NetApp Flash Cache, который на запись не работает, и эта возможность действительно производит впечатление на тех людей, которые не особенно разбираются как там у NetApp все устроено, и знают только то, что &quot;что-то иметь это гораздо лучше чем не иметь&quot;, и уж все знают, что запись надо кэшировать, без кэширования запись на диски очень медленная, это знает даже начинающий пользователь, впервые покупающий кэш-контроллер в сервер.</p>
<p>Чем прежде всего занимается кэш на запись?   <br />Давайте рассмотрим на примере.</p>
<p>Когда клиент записывает блок данных на систему хранения, то, при наличии кэша на запись, этот блок максимально быстро в него помещается, и клиенту отдается сообщение “OK, принято”, клиент не ждет физического помещения блока в сектор жесткого диска, а может продолжать работать, словно записываемый блок уже записан, например отправить следующий блок.    <br />Запись ускоряется путем ее буферизации, и, в дальнейшем, сортировкой и упорядочением процессов записи.</p>
<p>Хотя современный жесткий диск и позволяет обращаться к произвольным фрагментам записанной на него информации, чем отличается от, например, магнитной ленты, которая позволяет считывать и записывать ее только последовательно, однако не позволяет эту произвольно размещенную (random) информацию считывать и записывать _одновременно_, поскольку это физически ограничено возможностями магнитных головок диска.</p>
<p>Если у нас есть жесткий диск, и мы не используем кэширование, то три клиента, пишущих на этот диск, будут вынуждены выстроиться в очередь. Сперва диск спозиционирует головки и дождется подхода нужного сектора для записи блока данных клиента A, а пославшие на запись свой блок клиенты B и C будут вынуждены ждать, затем головки будут переставлены в новое место, диск дождется, когда мимо головок проедет блок, который требуется перезаписать клиенту B, и перезапишет его, а если за это время у клиента A появился еще один блок на запись, то он поставится в очередь следом за блоком клиента C, и будет ожидать, пока выполнятся все операции перед ним.</p>
<p>Все эти процессы, как вы понимаете, механические, неспешны, и занимают не микросекунды, как операции в памяти, а миллисекунды, иногда десятки миллисекунд. Несмотря на то, что жесткий диск - устройство произвольной записи, эти произвольные записи не могут быть осуществлены одновременно.</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fca970c-pi" /></p>
<p>Ради создания хотя бы иллюзии одновременности и организуется кэш на запись. Приложения записывают свои данные в кэш записи, получают сразу же ответ &quot;готово&quot; и идут сочинять новые блоки на запись, не дожидаясь того, что данные физически поместятся на диск.</p>
<p>Кэш же внутри себя удерживает блок, ожидая подходящего момента на запись, а также оптимизирует записи, с тем, чтобы уменьшить &quot;пробег&quot; магнитных головок по диску, и возможно оптимальным образом перекомпонует записи так, чтобы уложить их максимально эффективным, с точки зрения head seek-а, способом.</p>
<p>Принципиальное отличие WAFL тут состоит в том, что WAFL не перезаписывает блоки уже записанные на диске, и значит при записи клиенты гораздо меньше ожидают seek-а. Вы помните, что в WAFL записи можно провести &quot;чохом&quot;, в выделенный сегмент, а не переписывать по одному блоку, мечась по всему диску, здесь и там, ожидая, когда подъедет под головку тот или иной блок. Поэтому даже без традиционного кэширования записи на WAFL клиентские записи быстро оказываются на дисках.</p>
<p>Строго говоря, большой кэш означает, что, используя транспортную аналогию, записи быстро встают в очередь на посадку, но совсем не значит, что при этом они быстро сядут и поедут.</p>
<p>WAFL оптимизирован именно на минимальное время от момента прихода данных &quot;на остановку&quot; и до входа их &quot;в автобус&quot; жесткого диска, превращая записи в записи последовательного порядка (sequental) из поступающих записей в произвольном порядке (random).</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef0133f2735a39970b-pi" width="467" height="307" /></p>
<p>Результат хорошо иллюстрируется <a href="http://blogs.netapp.com/dropzone/2010/07/does-your-data-wait-in-line.html">экспериментом</a>, в котором один aggregate, состоящий из трех дисков SATA 1TB 7200rpm, в RAID-DP (2p+1d), то есть, фактически, из одного действующего диска, показывает при random write блоком 4KB не типичные для SATA 70-80 IOPS, а более 4600!</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fe9970c-pi" /></p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485987007970c-pi" /></p>
<p>Объяснение этому простое - записи, поступающие на диск теряют свою &quot;рандомность&quot;, и &quot;секвентализируются&quot;. Около четырех с половиной тысяч IOPS random-записи на один диск SATA - это как раз то, отчего в системах NetApp нет свойственной для &quot;классических систем&quot; острой необходимости в кэше записи на уровне контроллера.</p>
<p>Таким образом запись действительно надо кэшировать для &quot;классических&quot; систем, да, безусловно, это так. Но это совсем не так безусловно для &quot;неклассических&quot; дисковых структур, используемых, например, в NetApp.</p>
<p>Вот почему NetApp Flash Cache не используется на запись данных, работая всем своим объемом только на чтение. Вот почему необходимость кэширования записи для WAFL не столь безоговорочна, как для <em>&quot;классических&quot;</em> дисковых структур.</p>
<p>Потому, что в WAFL необходимость обязательного кэширования данных при записи существенно ниже, чем для &quot;традиционных&quot; систем. </p>
<p>Это позволило, среди прочего, кстати, значительно упростить алгоритмическую составляющую процесса кэширования, ведь не секрет, что правильная обработка кэширования на запись часто создает значительные проблемы, особенно в наиболее эффективном режиме write-back.</p>
<p>А это, в свою очередь, снизило количество возможных ошибок (как программного кода реализации, так и ошибок при использовании) удешевило устройство и упростило использование.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="cache.html" rel="tag">cache</a>, <a href="emc/index.html" rel="tag">emc</a>, <a href="fastcache.html" rel="tag">fastcache</a>, <a href="flash/index.html" rel="tag">flash</a>, <a href="flashcache.html" rel="tag">flashcache</a>, <a href="netapp/index.html" rel="tag">netapp</a>, <a href="ssd/index.html" rel="tag">ssd</a><br />					Раздел: <a href="../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../936/trackback.html#comments" title="Комментарий к записи EMC FASTcache и NetApp Flash Cache">Комментарии (25)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-926">
				<h2 class="posttitle"><a href="../926/trackback.html" rel="bookmark" title="Permanent Link to Про tiering: EMC FAST, FASTcache, NetApp Flash Cache">Про tiering: EMC FAST, FASTcache, NetApp Flash Cache</a></h2>
				<div class="postmetadata">26 Май 2011, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Несколько постов назад, в комментах, разгорелась нешуточная дискуссия о том, можно ли считать Flash Cache “истинно православным” средство tiering-а, и ставить его в ряд с множеством других аналогичных средств, например с EMC FAST.    <br />Для разъяснения моей позиции на этот счет я и написал этот пост.</p>
<p>Для начала давайте определим что такое tiering вообще.</p>
<p><strong>Tiering</strong>-ом (увы, русскоязычного термина пока не прижилось, будем использовать такое) принято называть механизм перемещения данных между “уровнями” (tiers) хранения, характеризующимися теми или иными свойствами, например ценой, быстродействием, защищенностью, и так далее. Обычно tiering-ом принято называть механизмы, для перемещения данных между дисками разных типов, или дисками и магнитными лентами, или же ROM-хранилищем, часто он используется для организации ILM – Information Lifecycling Management – хранилища данных в соответствии с их статусом и уровнями QoS.</p>
<p>Но давайте отвлечемся от физической реализации, и посмотрим на задачу с функциональной точки зрения, с точки зрения пользователя или приложения, использующего систему хранения.</p>
<p><strong>Что есть tiering с точки зрения приложения или пользователя? Зачем мы его применяем?</strong></p>
<p>Целью tiering-а для пользователя является возможность повысить эффективность (в первую очередь экономическую) использования его системы хранения. Когда “цена значения не имеет”, то все просто <strike>– надо купить Symmetrix</strike>. Однако в реальной жизни цена очень даже имеет значение, и пользователь вынужден идти на компромиссы между ценой решения и необходимой пользователю производительностью.</p>
<p>??спользуемые в системах хранения диски сегодня имеют различную производительность, емкость и цену, причем производительность и емкость обычно величины обратно пропорциональные: больше емкость – меньше производительность (SATA), меньше емкость – выше производительность&#160; (SAS), еще выше производительность и цена, и меньше емкость – Flash. Система хранения-же целом характеризуется соотношением <strong>IOPS/$</strong>, то есть количества единиц производительности с “вложенного доллара”.&#160; Повысить этот параметр стремится любой вендор и любой покупатель системы хранения.</p>
<p>Согласно широкоизвестному <a href="http://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%9F%D0%B0%D1%80%D0%B5%D1%82%D0%BE">Закону Парето</a> (“20 процентов сотрудников делает 80 процентов всей работы”) сравнительно небольшая часть данных ответственна за значительную долю быстродействия системы. Напротив, значительная часть данных относится к так называемым “холодным”данным, скорость доступа к которым в принципе не критична.     <br />Было бы неплохо, если бы эти 20% активных данных, скорость доступа к которым напрямую влияет на общую производительность, располагались на максимально быстром разделе хранилища, пусть он дорогой, но его емкость будет всего 20% от емкости хранилища, зато прирост мы получим во все 80%!</p>
<p>??менно эта идея лежала в основе идеи tiering-а. Если этот тип данных – активный, то автоматически перенесем его на более дорогие и быстродействующие диски, и повысим производительность доступа к ним, а менее активные данные – перенесем на менее производительные дешевый тип дисков. ?? настанет счастье, в виде повысившегося соотношения IOPS/$.</p>
<p>К такому, “классическому” типу tiering-а относится продукт EMC FAST – (Fully-Automated Storage Tiering). Он позволяет прозрачно для пользователя переносить фрагменты его данных между “уровнями” хранилища, например между разделами на дисках SAS и SATA.</p>
<p><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image99.png" width="186" height="318" /></p>
<p>Увы, дьявол кроется в деталях, и “всегда читайте написанное мелким шрифтом”. Первая версия FAST была весьма сырой, например, позволяла переносить только LUN-ы целиком, что, очевидно, неприемлемый на практике уровень “гранулярности” данных. Только в FASTv2 появилась возможность суб-LUN-овой миграции, впрочем, по-прежнему, мигрируемый минимальный фрагмент данных весьма велик (1GB!), да и еще окружен множеством ограничений использования (не поддерживается компрессия для данных, которые подлежат миграции, например).</p>
<p>К тому же, к сожалению, принадлежность данных к той или иной группе не фиксирована. Так, бухгалтерские проводки за прошлый квартал могут лежать “холодными”, до момента составления квартального или годового отчета, когда обращение к ним резко увеличится. Значит пока мы не распознаем и не смигрируем ставшие активными данные на быстрое хранилище, мы рискуем значительно терять в быстродействии, а если наш алгоритм выявления активных данных не обладает достаточным быстродействием, мы рискуем и вовсе не получить прироста, если “пики” доступа окажутся менее “разрешающей способности” анализирующего алгоритма, и не будут им распознаны как активность. Это же относится и к “устойчивости к помехам”, так, например, регулярный бэкап может свести с ума алгоритм анализа активности, ведь к данным обращаются каждую ночь!</p>
<p>Как вы видите, внешне очевидная и тривиальная задача анализа активности данных и переноса в соответствии с ними на те или иные типы дисков, обрастает сложностями.</p>
<p>Однако, почему задачу повышения быстродействия доступа к данным можно решать только лишь физическим перемещением данных с диска на диск?&#160; Вспомним про механизм, который называется “кэширование”. При кэшировании, копии данных, к которым осуществляется активный доступ, накапливаются в специальном высокоскоростном пространстве, доступ куда значительно быстрее (впрочем, см. выше, оно и значительно дороже, этим ограничивается расширение его размера), когда же активность доступа к данным падает (читай: снижается необходимость в этих данных), эти копии удаляются из кэша, при этом собственно содержимое данных по прежнему сохраняется в сравнительно недорогом и&#160; малопроизводительном, по сравнению с кэшем, “основном хранилище”.</p>
<p>Большой плюс метода кэширования заключается, во-первых, в полной “автоматизации процесса”, ни пользователю, ни его программе не надо предпринимать какие-то дополнительные действия при его использовании. Если они часто читают этот участок данных, он оказывается в кэше, и скорость доступа к нему повышается, если они перестают его читать, то он вытесняется более активными данными, и остается только в основном хранилище.</p>
<p><img style="background-image: none; border-right-width: 0px; padding-left: 0px; padding-right: 0px; display: inline; border-top-width: 0px; border-bottom-width: 0px; border-left-width: 0px; padding-top: 0px" title="image" border="0" alt="image" src="/pics//image100.png" width="182" height="343" /></p>
<p>Во-вторых, в область дорогого, но высокопроизводительного хранения – “кэша” – попадают “по определению” только “горячие” данные. Место в кэше всегда занимают только данные, к которым сейчас идет активное обращение. Все 100% дорогостоящего объема кэша используются для повышения производительности системы, а не просто “хранения”. Следствием этого является более высокая эффективность работы кэша. Процессор Intel Xeon имеет всего 8Mb кэша, что не мешает ему работать с в тысячи раз большими объемами ОЗУ на сервере, и, за счет этого, сравнительно небольшого, по относительной величине, кэша, эффективно ускорять доступ к размещенным в обширной памяти данным.</p>
<p>В третьих – процесс ускорения доступа и улучшения результата IOPS/$ путем кэширования есть, с алгоритмической точки зрения, процесс тривиальный. А раз так, он имеет минимум побочных эффектов и ограничений использования, а работать (и давать результат в виде повышения производительности) начинает сразу же, а не когда накопится статистика использования и, наконец, в соответствии с ней произойдет миграция данных с одних дисков на другие.</p>
<p>Минусы? Ну как же в нашей жизни и без минусов. Эффективность кэша, действительно высокая при чтении, сильно падает на операциях записи. Ведь если мы изменяем данные в их копиях в кэше, нам надо регулярно и своевременно обновлять их непосредственно по месту их размещения (этот процесс имеет название “сброса содержимого кэша” или “flush”). Значит, если мы изменяем содержимое блока, нам надо это изменение распознать и переписать его содержимое в основном хранилище, чтобы, когда блок будет вытеснен по неактивности из кэша, его новое, измененное содержимое не потерялось. Кэширование записи, хоть и ускоряет собственно процесс записи, сильно усложняет ситуацию алгоритмически, и, как следствие, может вести к значительному снижению эффективности работы.</p>
<p>Таким образом, взглянув на задачу “со стороны пользователя”, мы видим два эквивалентных по результатам решения. Оба решают задачу в нужном пользователю русле, а именно, распределяют данные по “уровням” хранения с различной ценой и производительностью, в зависимости от их активности, улучшая, в результате, экономическую эффективность хранилища, повышая производительность и снижая затраты, то есть улучшая уже названный выше ключевой экономический&#160; параметр хранилища – <strong>IOPS/$</strong>.</p>
<p>Да, действительно, “метод кэширования” имеет в основе другой механизм, чем “метод переноса”, но если результат для пользователя – тот же самый: изменение характеристик доступа к данным, ведущее к улучшению параметра IOPS/$ и эффективности хранилища, то <em>“неважно, какого цвета кошка, если она хорошо ловит мышей”</em>. Будет ли это tiering как перенос данных между двумя типами дисков, или в виде переноса между диском и кэшем, для пользователя и его задач это, по большому счету, безразлично, если это дает эквивалентный прирост эффективности.</p>
<p>Вот почему я считаю, что как EMC FAST, так и EMC FASTcache и NetApp FlashCache, все они являются формами организации tiering-а, и их можно рассматривать вместе, как <em>формы</em> одного решения.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="emc/index.html" rel="tag">emc</a>, <a href="fast.html" rel="tag">fast</a>, <a href="fastcache.html" rel="tag">fastcache</a>, <a href="flashcache.html" rel="tag">flashcache</a>, <a href="netapp/index.html" rel="tag">netapp</a>, <a href="tiering.html" rel="tag">tiering</a><br />					Раздел: <a href="../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../926/trackback.html#comments" title="Комментарий к записи Про tiering: EMC FAST, FASTcache, NetApp Flash Cache">Комментарии (38)</a>									 </div>
			</div>
	
		
		<div class="navigation">
			<div class="alignleft"><a href="emc/page/2.html">&laquo; Previous Entries</a></div>
			<div class="alignright"></div>
		</div>
		
	
	</div>
	<div id="sidebar">
		<ul>
			
			
			<!-- Author information is disabled per default. Uncomment and fill in your details if you want to use it.
			<li><h2>Автор</h2>
			<p>A little something about you, the author. Nothing lengthy, just an overview.</p>
			</li>
			-->

			<li class="pagenav"><h2>Страницы</h2><ul><li class="page_item page-item-153"><a href="../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul></li>
			<li><h2>Рубрики</h2>
				<ul>
					<li class="cat-item cat-item-89"><a href="../category/commands/index.html" title="Просмотреть все записи в рубрике &laquo;commands&raquo;">commands</a>
</li>
	<li class="cat-item cat-item-37"><a href="../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;">howto</a>
</li>
	<li class="cat-item cat-item-52"><a href="../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;">justread</a>
</li>
	<li class="cat-item cat-item-51"><a href="../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;">review</a>
</li>
	<li class="cat-item cat-item-3"><a href="../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;">techtalk</a>
</li>
	<li class="cat-item cat-item-71"><a href="../category/tricks/index.html" title="Просмотреть все записи в рубрике &laquo;tricks&raquo;">tricks</a>
</li>
	<li class="cat-item cat-item-95"><a href="../category/utilities/index.html" title="Просмотреть все записи в рубрике &laquo;utilities&raquo;">utilities</a>
</li>
	<li class="cat-item cat-item-44"><a href="../category/whoisho/index.html" title="Просмотреть все записи в рубрике &laquo;whoisho&raquo;">whoisho</a>
</li>
	<li class="cat-item cat-item-1"><a href="../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;">новости</a>
</li>
	<li class="cat-item cat-item-387"><a href="../category/opros.html" title="Просмотреть все записи в рубрике &laquo;опрос&raquo;">опрос</a>
</li>
	<li class="cat-item cat-item-8"><a href="../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;">переводы</a>
</li>
	<li class="cat-item cat-item-40"><a href="../category/citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;">цитаты</a>
</li>
				</ul>
			</li>

			<li><h2>Архивы</h2>
				<ul>
					<li><a href='../date/2018/01.html' title='Январь 2018'>Январь 2018</a></li>
	<li><a href='../date/2015/10.html' title='Октябрь 2015'>Октябрь 2015</a></li>
	<li><a href='../date/2015/04.html' title='Апрель 2015'>Апрель 2015</a></li>
	<li><a href='../date/2015/03.html' title='Март 2015'>Март 2015</a></li>
	<li><a href='../date/2015/01.html' title='Январь 2015'>Январь 2015</a></li>
	<li><a href='../date/2014/12.html' title='Декабрь 2014'>Декабрь 2014</a></li>
	<li><a href='../date/2014/11.html' title='Ноябрь 2014'>Ноябрь 2014</a></li>
	<li><a href='../date/2014/10.html' title='Октябрь 2014'>Октябрь 2014</a></li>
	<li><a href='../date/2014/09.html' title='Сентябрь 2014'>Сентябрь 2014</a></li>
	<li><a href='../date/2014/08.html' title='Август 2014'>Август 2014</a></li>
	<li><a href='../date/2014/07.html' title='Июль 2014'>Июль 2014</a></li>
	<li><a href='../date/2014/06.html' title='Июнь 2014'>Июнь 2014</a></li>
	<li><a href='../date/2014/05.html' title='Май 2014'>Май 2014</a></li>
	<li><a href='../date/2014/04.html' title='Апрель 2014'>Апрель 2014</a></li>
	<li><a href='../date/2014/03.html' title='Март 2014'>Март 2014</a></li>
	<li><a href='../date/2014/02.html' title='Февраль 2014'>Февраль 2014</a></li>
	<li><a href='../date/2014/01.html' title='Январь 2014'>Январь 2014</a></li>
	<li><a href='../date/2013/12.html' title='Декабрь 2013'>Декабрь 2013</a></li>
	<li><a href='../date/2013/11.html' title='Ноябрь 2013'>Ноябрь 2013</a></li>
	<li><a href='../date/2013/10.html' title='Октябрь 2013'>Октябрь 2013</a></li>
	<li><a href='../date/2013/09.html' title='Сентябрь 2013'>Сентябрь 2013</a></li>
	<li><a href='../date/2013/08.html' title='Август 2013'>Август 2013</a></li>
	<li><a href='../date/2013/07.html' title='Июль 2013'>Июль 2013</a></li>
	<li><a href='../date/2013/06.html' title='Июнь 2013'>Июнь 2013</a></li>
	<li><a href='../date/2013/05.html' title='Май 2013'>Май 2013</a></li>
	<li><a href='../date/2013/04.html' title='Апрель 2013'>Апрель 2013</a></li>
	<li><a href='../date/2013/03.html' title='Март 2013'>Март 2013</a></li>
	<li><a href='../date/2013/02.html' title='Февраль 2013'>Февраль 2013</a></li>
	<li><a href='../date/2013/01.html' title='Январь 2013'>Январь 2013</a></li>
	<li><a href='../date/2012/12.html' title='Декабрь 2012'>Декабрь 2012</a></li>
	<li><a href='../date/2012/11.html' title='Ноябрь 2012'>Ноябрь 2012</a></li>
	<li><a href='../date/2012/10.html' title='Октябрь 2012'>Октябрь 2012</a></li>
	<li><a href='../date/2012/09.html' title='Сентябрь 2012'>Сентябрь 2012</a></li>
	<li><a href='../date/2012/08.html' title='Август 2012'>Август 2012</a></li>
	<li><a href='../date/2012/07.html' title='Июль 2012'>Июль 2012</a></li>
	<li><a href='../date/2012/06.html' title='Июнь 2012'>Июнь 2012</a></li>
	<li><a href='../date/2012/05.html' title='Май 2012'>Май 2012</a></li>
	<li><a href='../date/2012/04.html' title='Апрель 2012'>Апрель 2012</a></li>
	<li><a href='../date/2012/03.html' title='Март 2012'>Март 2012</a></li>
	<li><a href='../date/2012/02.html' title='Февраль 2012'>Февраль 2012</a></li>
	<li><a href='../date/2012/01.html' title='Январь 2012'>Январь 2012</a></li>
	<li><a href='../date/2011/12.html' title='Декабрь 2011'>Декабрь 2011</a></li>
	<li><a href='../date/2011/11.html' title='Ноябрь 2011'>Ноябрь 2011</a></li>
	<li><a href='../date/2011/10/index.html' title='Октябрь 2011'>Октябрь 2011</a></li>
	<li><a href='../date/2011/09/index.html' title='Сентябрь 2011'>Сентябрь 2011</a></li>
	<li><a href='../date/2011/08.html' title='Август 2011'>Август 2011</a></li>
	<li><a href='../date/2011/07/index.html' title='Июль 2011'>Июль 2011</a></li>
	<li><a href='../date/2011/06/index.html' title='Июнь 2011'>Июнь 2011</a></li>
	<li><a href='../date/2011/05/index.html' title='Май 2011'>Май 2011</a></li>
	<li><a href='../date/2011/04/index.html' title='Апрель 2011'>Апрель 2011</a></li>
	<li><a href='../date/2011/03/index.html' title='Март 2011'>Март 2011</a></li>
	<li><a href='../date/2011/02.html' title='Февраль 2011'>Февраль 2011</a></li>
	<li><a href='../date/2011/01.html' title='Январь 2011'>Январь 2011</a></li>
	<li><a href='../date/2010/12.html' title='Декабрь 2010'>Декабрь 2010</a></li>
	<li><a href='../date/2010/11/index.html' title='Ноябрь 2010'>Ноябрь 2010</a></li>
	<li><a href='../date/2010/10/index.html' title='Октябрь 2010'>Октябрь 2010</a></li>
	<li><a href='../date/2010/09/index.html' title='Сентябрь 2010'>Сентябрь 2010</a></li>
	<li><a href='../date/2010/08.html' title='Август 2010'>Август 2010</a></li>
	<li><a href='../date/2010/07/index.html' title='Июль 2010'>Июль 2010</a></li>
	<li><a href='../date/2010/06.html' title='Июнь 2010'>Июнь 2010</a></li>
	<li><a href='../date/2010/05.html' title='Май 2010'>Май 2010</a></li>
	<li><a href='../date/2010/04/index.html' title='Апрель 2010'>Апрель 2010</a></li>
	<li><a href='../date/2010/03/index.html' title='Март 2010'>Март 2010</a></li>
	<li><a href='../date/2010/02.html' title='Февраль 2010'>Февраль 2010</a></li>
	<li><a href='../date/2010/01.html' title='Январь 2010'>Январь 2010</a></li>
	<li><a href='../date/2009/12/index.html' title='Декабрь 2009'>Декабрь 2009</a></li>
	<li><a href='../date/2009/11/index.html' title='Ноябрь 2009'>Ноябрь 2009</a></li>
	<li><a href='../date/2009/10.html' title='Октябрь 2009'>Октябрь 2009</a></li>
	<li><a href='../date/2009/09.html' title='Сентябрь 2009'>Сентябрь 2009</a></li>
	<li><a href='../date/2009/08/index.html' title='Август 2009'>Август 2009</a></li>
	<li><a href='../date/2009/07/index.html' title='Июль 2009'>Июль 2009</a></li>
	<li><a href='../date/2009/06.html' title='Июнь 2009'>Июнь 2009</a></li>
	<li><a href='../date/2009/05.html' title='Май 2009'>Май 2009</a></li>
	<li><a href='../date/2009/04.html' title='Апрель 2009'>Апрель 2009</a></li>
	<li><a href='../date/2009/03.html' title='Март 2009'>Март 2009</a></li>
	<li><a href='../date/2009/02.html' title='Февраль 2009'>Февраль 2009</a></li>
	<li><a href='../date/2009/01.html' title='Январь 2009'>Январь 2009</a></li>
	<li><a href='../date/2008/12.html' title='Декабрь 2008'>Декабрь 2008</a></li>
	<li><a href='../date/2008/11.html' title='Ноябрь 2008'>Ноябрь 2008</a></li>
	<li><a href='../date/2008/10.html' title='Октябрь 2008'>Октябрь 2008</a></li>
	<li><a href='../date/2008/09.html' title='Сентябрь 2008'>Сентябрь 2008</a></li>
	<li><a href='../date/2008/08.html' title='Август 2008'>Август 2008</a></li>
	<li><a href='../date/2008/03.html' title='Март 2008'>Март 2008</a></li>
	<li><a href='../date/2008/02.html' title='Февраль 2008'>Февраль 2008</a></li>
	<li><a href='../date/2007/12.html' title='Декабрь 2007'>Декабрь 2007</a></li>
	<li><a href='../date/2007/11.html' title='Ноябрь 2007'>Ноябрь 2007</a></li>
	<li><a href='../date/2007/10.html' title='Октябрь 2007'>Октябрь 2007</a></li>
	<li><a href='../date/2007/09.html' title='Сентябрь 2007'>Сентябрь 2007</a></li>
	<li><a href='../date/2007/08.html' title='Август 2007'>Август 2007</a></li>
	<li><a href='../date/2007/07/index.html' title='Июль 2007'>Июль 2007</a></li>
	<li><a href='../date/2007/06.html' title='Июнь 2007'>Июнь 2007</a></li>
	<li><a href='../date/2007/05.html' title='Май 2007'>Май 2007</a></li>
				</ul>
			</li>

			
					</ul>
	</div>

</div> <!-- wrapper -->
<div id="footer">
	<a href="../../feed">Entries (RSS)</a> and <a href="../../comments/feed">Comments (RSS)</a>. Valid <a href="http://validator.w3.org/check/referer" title="This page validates as XHTML 1.0 Transitional"><abbr title="eXtensible HyperText Markup Language">XHTML</abbr></a> and <a href="http://jigsaw.w3.org/css-validator/check/referer"><abbr title="Cascading Style Sheets">CSS</abbr></a>.<br />
	Powered by <a href="http://wordpress.org/" title="Powered by WordPress.">WordPress</a> and <a href="http://srinig.com/wordpress/themes/fluid-blue/">Fluid Blue theme</a>.<br />
	<!-- 15 queries. 0.116 seconds. -->
	</div>
</div> <!-- page -->
</body>
</html>
