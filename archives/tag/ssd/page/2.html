<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="ru-RU">

<head profile="http://gmpg.org/xfn/11">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

<title>about NetApp   &raquo; ssd</title>

<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/style.css" type="text/css" media="screen" />
<link rel="stylesheet" href="../../../../wp-content/themes/fluid-blue/print.css" type="text/css" media="print" />
<link rel="alternate" type="application/rss+xml" title="about NetApp RSS Feed" href="../../../../feed" />
<link rel="pingback" href="../../../../xmlrpc.php.html" />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="../../../../xmlrpc.php%3Frsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="../../../../wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 2.6" />

</head>

<body>
<div id="page">
<div id="header">
	<div id="headertitle">
		<h1><a href="../../../../index.html" title="about NetApp: Системы хранения данных как предмет разговора">about NetApp</a></h1>
		<p>Системы хранения данных как предмет разговора</p>
	</div> 
	<!-- Search box (If you prefer having search form as a sidebar widget, remove this block) -->
	<div class="search">
		<form method="get" id="searchform" action="../../../../index.html">
<input type="text" size="20" name="s" id="s" value="Поиск..."  onblur="if(this.value=='') this.value='Поиск...';" onfocus="if(this.value=='Поиск...') this.value='';"/>
</form>
	</div> 
	<!-- Search ends here-->
		
</div>

<div id="navbar">
<ul id="nav">
	<li><a href="../../../../index.html">Home</a></li>
	<li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul>
</div>
<div id="wrapper">

	<div id="content">

	
			<p>Posts tagged &#8216;ssd&#8217;</p>

	 		
		<div class="navigation">
			<div class="alignleft"></div>
			<div class="alignright"><a href="../index.html">Next Entries &raquo;</a></div>
		</div>

						
			<div class="post" id="post-1197">
				<h2 class="posttitle"><a href="../../../1197/trackback.html" rel="bookmark" title="Permanent Link to Flash Pool: некоторые тонкости применения">Flash Pool: некоторые тонкости применения</a></h2>
				<div class="postmetadata">27 Август 2012, 11:00 <!-- от  --></div>
				<div class="postentry">
					<p>Долгожданный Flash Pool (AKA Hybrid Aggregate) наконец-то вышел в релиз с появлением версии Data ONTAP 8.1.1. О том, что это такое я уже писал, но для тех, кто все пропустил, и не желает посмотреть в поиске по этому блогу, вкратце: это технология, которая позволяет создать комбинированный aggregate на системе хранения NetApp, в которй добавленные в aggregate диски SSD (на flash memory) используются для кэширования данных, читающихся, а также пишущихся на тома этого aggregate. Эта технология расширяет и дополняет уже имеющуюся несколько лет у NetApp для систем его midrange&#160; и highend линейки, технологию Flash Cache, выполненную в виде карты flashmemory, и устанавливаемую внутри контроллера системы хранения.</p>
<p>Однако, как слюбой новой технологией, у нее существуют некоторые тонкости применения, неочевидные места, а то и просто засады. Вот о них мы сегодня поговорим подробнее.</p>
<p>   <a href="../../../1197/trackback.html#more-1197" class="more-link"><small>Continue reading &#8216;Flash Pool: некоторые тонкости применения&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../flash/index.html" rel="tag">flash</a>, <a href="../../flash-cache.html" rel="tag">flash cache</a>, <a href="../../flash-pool.html" rel="tag">flash pool</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1197/trackback.html#comments" title="Комментарий к записи Flash Pool: некоторые тонкости применения">Комментарии (13)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1190">
				<h2 class="posttitle"><a href="../../../1190/trackback.html" rel="bookmark" title="Permanent Link to Создание Flash Pool">Создание Flash Pool</a></h2>
				<div class="postmetadata">16 Август 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>С выходом Data ONTAP 8.1.1 (сейчас он, для самых нетерпеливых, находится в состоянии Release Candidate) появляется долгожданная фича под названием <a href="../../../1158/trackback.html">Flash Pool </a>(он же, ранее, <a href="../../../1096/trackback.html">Hybrid Aggregate</a>)</p>
<p>??так, давайте посмотрим, как можно создать Flash Pool, то есть aggregate с дополнительными дисками SSD для кэширования данных.<br />
Создадим для начала простой aggregate:</p>
<p><code>fas01> aggr create flashpool -B 64 -t raid_dp -T SATA -r 16 16</code></p>
<p>??мя aggregate: flashpool, формат: 64-bit, тип RAID: RAID-DP, тип дисков: SATA, размер RAID: 16</p>
<p>После успешного создания aggregate по имени &#8216;flashpool&#8217; разрешим на нем собственно flashpool:</p>
<p><code>fas01> aggr options flashpool hybrid_enabled on</code></p>
<p>Несмотря на то, что коммерческое название фичи было изменено в релизе с &#8216;hybrid aggregate&#8217; на &#8216;flash pool&#8217;, опция по прежнему называется &#8216;hybrid&#8217;. Аналогично с дедупликацией, которая когда-то называлась A-SIS (Advanced Single Instance Storage), и до сих пор так называется соответствующая опция в параметрах.</p>
<p>Теперь можно добавить к aggregate диски SSD в количестве 6 штук:</p>
<p><code>fas01> aggr add flashpool -T SSD 6</code></p>
<p>??з 6 штук два будет забрано под RAID parity (RAID-DP), а оставшиеся 4 - будут использованы как кэш. Обратите внимание, что сам aggregate не увеличится в емкости хранения! Добавленные SSD недоступны для непосредственного использования и записи на них данных, они будут использованы как кэш.</p>
<p>А теперь просто создадим на получившемся aggregate том (myvol) для хранения данных, емкостью 500GB:</p>
<p><code>fas01> vol create myvol flashpool 500g</code></p>
<p>Теперь получившийся том myvol, размером 500GB, можно использовать под данные, причем записываемые и считываемые данные будут автоматически использовать кэш на SSD.<br />
В следующем посте мы посмотрим, какие средства есть для тонкой настройки режимов кэширования томов на Flash Pool.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../811.html" rel="tag">8.1.1</a>, <a href="../../flash-pool.html" rel="tag">flash pool</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1190/trackback.html#comments" title="Комментарий к записи Создание Flash Pool">Комментарии (5)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1158">
				<h2 class="posttitle"><a href="../../../1158/trackback.html" rel="bookmark" title="Permanent Link to Hybrid Aggregate теперь Flash Pool!">Hybrid Aggregate теперь Flash Pool!</a></h2>
				<div class="postmetadata">14 Май 2012, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Ну, так как до выхода 8.1.1 уже совсем немного времени, давайте я уже расскажу вам, что же такое Flash Pool, который появится у NetApp начиная с этой версии.</p>
<p>Я ранее уже несколько раз упоминал о новой идее NetApp – включении нескольких SSD непосредственно в дисковый aggregate системы хранения, и использования их под кэш “уровня aggregate”, в том числе и для записи. Эта конструкция дополняет возможности Flash Cache, может работать как с ним вместе, так и сама по себе, причем, отметьте, также и для систем, на которых Flash Cache, по тем или иным причинам, использовать <strike>уже</strike> нельзя, например FAS3210, 3140, и даже 2240.</p>
<p>К моменту выпуска, реализация <a href="../../../1096/trackback.html">Hybrid Aggregate</a> в системах NetApp получила собственное, коммерческое имя-торговую марку Flash Pool, и далее я буду пользоваться именно им. Вы же знайте, что Flash Pool это название реализации NetApp Hybrid Aggregate в Data ONTAP 8.1.1 и новее.</p>
<p>К сожалению, вокруг Hybrid Aggregate/Flash Pool уже начало образовываться облако недопониманий и мифов, а моя задача в очередной раз внести ясность в тему.</p>
<p>??так, начнем.</p>
<p>Прежде всего, я бы хотел сказать, что, вопреки домыслам, Flash Pool это <strong>НЕ</strong> <strong>tiering</strong>, в классическом его понимании (например в том виде, в каком он представлен в EMC FAST), это кэш. Этот момент понятен? НЕ disk tiering, not, nicht, nie. :) <strong>Это КЭШ</strong>.</p>
<p>Появление Flash Pool также не означает отказа от Flash Cache. Это независимое, но дополняющее решение. Он может работать с Flash Cache, может работать сам по себе. В случае работы с Flash Cache, кэширование не дублируется. Тома, работающие с Flash Pool (находящиеся в аггрегейте с SSD) не кэшируются в Flash Cache. Помните, что Flash Cache может работать со всеми aggregates и volumes системы в целом, а кэширование Flash Pool распространяется только на тома одного aggregate. Если у вас несколько aggregates, вам понадобится добавлять SSD для создания Flash Pool в каждый aggregate, который вы хотите кэшировать в Flash.</p>
<p>В гибридный aggregate, то есть Flash Pool вы можете преобразовать любой 64-bit aggregate, добавив в него несколько SSD NetApp, объединенных в RAID-группу, и указав для aggregate соответстующую опцию, также его можно создать “с нуля” обычным способом, как любой aggregate. Но в создании Flash Pool есть несколько тонких моментов, именно на них я хочу остановится подробнее.</p>
<p>Так как Flash Pool это кэш, то есть SSD, как таковые, не доступны для непосредственного хранения на них каких-то конкретных данных, а лишь кэшируют поступаюшие на и считываемые с томов aggregate данные, добавление в aggregate SSD <strong>не увеличивает его емкость</strong>. Есть и “побочный эффект” – если вы имеете aggregate, достигший максимального возможного для данного типа контроллеров размера, например 50TB для FAS3210, то вы все равно можете добавить в этот 50TB-аггрегейт диски SSD для Flash Pool.</p>
<p>Тип RAID-группы для дисков, добавляемых в aggregate должен быть одинаков для всего aggregate. Если вы используете RAID-DP, то добавляемые SSD тоже должны быть в RAID-DP. Нельзя в aggregate из HDD в RAID-DP добавить SSD в RAID-4, например.</p>
<p>Обратите внимание, что возможность добавления в aggregate дисков SSD <strong>НЕ</strong> означает возможности добавления в aggregate дисков HDD другого типа. Flash Pool може быть (по вашему выбору) из SAS/FC и SSD, или из SATA и SSD, но НЕ из SAS и SATA.</p>
<p>После добавления SSD в aggregate вы, как и в случае обычных дисков, добавленных в aggregate, не можете “вынуть” их оттуда (например чтобы использовать их позже в другом, более нуждающемся aggregate) не уничтожив aggregate. </p>
<p>Наверняка у многих уже вертится на языке вопрос: “Как же нам воспользоваться Flash Pool, если NetApp продает SSD только в составе полки на 24 диска?” Отвечаем: С появлением Flash Pool SSD NetApp будут продаваться паками по 4 штуки, что дает вам во Flash Pool 142GB кэша из 4 SSD. Диски имеют размер 100GB [84574 MiB], и когда они включаются в aggregate, построенный на RAID-DP, вы получите из 4 дисков два диска parity и два – data. Конечно, вы можее включить в Flash Pool и больше SSD.</p>
<p>Однако помните, что SSD имеют интерфейс SATA. Это значит, что вы <strong>НЕ МОЖЕТЕ</strong> добавить SSD непосредственно в полку с дисками SAS. Но <strong>можете</strong> – в полку с дисками SATA. Смешивать физические интерфейсы дисков в составе одной полки нельзя. Таким образом, если у вас система с “только-SAS/FC”, вам понадобится для установки SSD, даже всего 4 штук, например, дополнительная полка “только-SATA”. Не забывайте об этой сложности.</p>
<p>Вопрос, который я уже тоже слышу :) “Вы говорите – SSD работает на запись? А как же <em>с исчерпанием ресурса на перезапись для SSD</em>?”</p>
<p>Ну, это тема. Да, безусловно, с этой точки зрения Flash Cache был принципиально более надежен, так как работал только на чтение, а записи (заполнение кэша) в него делались сравнительно (по меркам компьютера) редко, и большими “порциями”, которые flash memory как раз обрабатывает довольно хорошо, это не random write мелкими блоками. Однако <a href="../../../669/trackback.html">практика использования SSD enterprise-class показывает</a>, что проблема пресловутого “исчерпания ресурсов SSD при записи” в значительной мере надумана, преувеличена, и присуща, в основном, “бытовым” SSD. Тем не менее, эта проблема возможна, так как Flash Pool действительно пишется, работая на запись (хотя, вы не забыли, записи в WAFL не рандомны, а секвентальны). Для защиты данных в случае выхода SSD из строя вы как раз и используете объединение SSD в RAID, а сами SSD, как устройства, покрыты общей трехлетней warranty на систему.</p>
<p>На самом деле в отношении записи вы можете столкнуться с другой, более важной, чем мифическое “<em>исчерпание ресурса на запись</em>” неприятностью. Дело в том, что устройство flash таково (это так для любого flash-устройства”), что его производительность на запись падает, по мере активной записи (и пере-записи) данных на нем. Производительность SSD на запись максимальна, когда он полностью пуст и только пришел с завода.&#160; После того, как данные на SSD записываются, перезаписываются, и он постепенно заполняется данными, его производительность постепенно снижается, и стабилизируется на более низком, чем начальный, уровне, после того, как все его ячейки будут перезаписаны. С этим эффектом знакомы все владельцы SSD. Так что не экстраполируйте результаты первого испытания пустых SSD на всю его работу.</p>
<p>Отвечая на третий вопрос ;) : Да, <a href="http://en.wikipedia.org/wiki/TRIM">TRIM</a> для SSD поддерживается Data ONTAP на уровне системы. </p>
<p>Напомню, Flash Pool, новое название Hybrid Aggregate, появится в Data ONTAP 8.1.1, которая ожидается к выпуску в ближайшем месяце.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../811.html" rel="tag">8.1.1</a>, <a href="../../flash/index.html" rel="tag">flash</a>, <a href="../../flash-pool.html" rel="tag">flash pool</a>, <a href="../../hybrid-aggregate.html" rel="tag">hybrid aggregate</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;" rel="category tag">новости</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1158/trackback.html#comments" title="Комментарий к записи Hybrid Aggregate теперь Flash Pool!">Комментарии (22)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-1096">
				<h2 class="posttitle"><a href="../../../1096/trackback.html" rel="bookmark" title="Permanent Link to Hybrid Aggregate">Hybrid Aggregate</a></h2>
				<div class="postmetadata">26 Ноябрь 2011, 9:37 <!-- от  --></div>
				<div class="postentry">
					<p>Некоторые новости с прошедшего в Риме NetApp Insight 2011, главного мирового мероприятия NetApp, на котором, обычно, представляются все новинки года. На этот раз я обратил внимание на объявленную любопытную технологию, которая появится в Data ONTAP 8.1.1, под названием Hybrid Aggregate.    <br />Вот что это такое.</p>
<p>Как вы знаете, NetApp использует flash, прежде всего, в форме “расширения кэш-памяти”, а не как “эмуляцию диска” – SSD – <em>Solid State Disk</em>, у такого решения множество преимуществ на практике, и широкий успех Flash Cache, устройства для систем хранения NetApp, представляющего собой плату с 256/512/1024GB flash на ней, и устанавливаемую внутрь контроллера FAS3200/6200, тому подтверждение.</p>
<p>Однако у NetApp есть и “классические” SSD, а теперь появилась и новая интересная разработка в области работы с “традиционными” SSD. Ведь не секрет, что не существует волшебной “серебряной пули”, разом решающей абсолютно все проблемы пользователя, что бы там по этому поводу ни утверждали в отделе маркетинга. Вот для областей, где Flash Cache работает недостаточно эффективно, и предназначены новые разработки.</p>
<p>Hybrid Aggregate это, как нетрудно догадаться из названия, “составной” aggregate, состоящий из обычных “врашающихся” дисков и SSD. В такой структуре диски SSD обеспечивают “кэш на запись” для традиционных, “вращающихся” дисков в составе этого aggregate. </p>
<p>Основные отличия Hybrid Aggregate от Flash Cache следующие:</p>
<ul>
<li>Работает в том числе и на запись. В тех случаях, когда характер нагрузки системы пользователя представляет собой, преимущественно, большое количество операций записи сравнительно небольшими блоками, Flash Cache неэффективен, так как практически не ускоряет записи (в определенной степени он их ускоряет, но только за счето того, что, высвобождая ресурсы дисков за счет кэширования чтения, оставляет больше на обработку операций записи). Следует отметить, что рабочая нагрузка, представляющая собой, главным образом, объемный random write мелкими блоками, это довольно специфическая и не слишком массовая задача.</li>
<li>Hybrid Aggregate может быть несколько на одной системе, каждый из них может быть настроен индивидуально и независимо от других. Flash Cache работает на все aggregate разом, хотя и можно политиками FlexCache настроить приоритеты и политики кэширования для разных томов.</li>
</ul>
<p>Как мне видится, преимущества от Hybrid Aggregate получат прежде всего большие и очень большие системы, с сотнями дисков, с большими aggregates и <em>очень большим</em> трафиком ввода-вывода, или же системы со специфической рабочей нагрузкой (большой объем преимущественно записей мелкими блоками), кроме того следует помнить, что Hybrid Aggregate не исключает использование и Flash Cache, он вполне может использоваться вместе с Hybrid Aggregate на той же системе. Таким образом, появление Hybrid Aggregate не стоит трактовать как “отказ от Flash Cache” (я специально делаю это замечание для “паникеров” ;) или признание его неэффективности, это, скорее дополнение в ранее незакрытом сегменте рабочих нагрузок и специфических решений.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../flash/index.html" rel="tag">flash</a>, <a href="../../hybrid-aggregate.html" rel="tag">hybrid aggregate</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;" rel="category tag">review</a>,  <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../1096/trackback.html#comments" title="Комментарий к записи Hybrid Aggregate">Комментарии (10)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-951">
				<h2 class="posttitle"><a href="../../../951/trackback.html" rel="bookmark" title="Permanent Link to Еще немного про autotiering">Еще немного про autotiering</a></h2>
				<div class="postmetadata">13 Июль 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Проглядывая <a href="http://community.netapp.com">community.netapp.com</a> обнаружил <a href="http://communities.netapp.com/message/51884">дискуссию о autotiering-е</a>, откуда выдернул интересное мнение уже известного вам Dimitris K. (recoverymonkey). Хотя в оригинале это были три реплики-ответа в дискуссии в форуме, я слил их оформил их как отдельную “статью”.</p>
<p>Дискуссия идет о реализации autotiering в EMC FAST, а также о системах хранения Compellent, которые, до недавнего времени, были главным игроком на рынке tiering-а, и реализация прозрачного tiering-а в них была сделана ранее всех. В России они почти неизвестны, хотя сейчас, как я понимаю, они могут начать попадать в страну через каналы Dell.</p>
<p><em>Dimitris Kriekouvkas (<a href="http://www.recoverymonkey.org">recoverymonkey</a>), сотрудник NetApp:</em></p>
<p>Autotiering это отличная концепция, но она также очень новая, и результаты ее работы не проверены и не подтверждены на подавляющем большинстве реальных нагрузок.</p>
<p>Посмотрите на опубликованные бенчмарки EMC – там нигде нет autotiering.</p>
<p>Вы также не найдете и показателей FASTcache. Все бенчмарки у EMC делаются на традиционных RAID-группах, без пулов.</p>
<p>Если вы посмотрите на руководство наилучших практик по производительности и доступности для EMC CLARiiON (ссылку на этот документ я давал в прошлом посте про &#8220;матчасть&#8221;), то вы увидите следующее:</p>
<ul>
<li>Вместо RAID-5 для больших пулов на SATA рекомендуется RAID-6 с размером группы в 10-12 дисков. </li>
<li>Thin Provisioning снижает производительность </li>
<li>Storage pools снижают производительность по сравнению с Traditional RAID </li>
<li>Данные и ввод-вывод не распределяются на все диски пула, как вы, возможно, предполагали (см <a href="http://virtualeverything.wordpress.com/2011/03/05/emc-storage-pool-deep-dive-design-considerations-caveats/">ссылку</a>). </li>
<li>Рекомендуется использовать drive ownership на только один контроллер </li>
<li>Нельзя смешивать разные типы RAID в одном пуле </li>
<li>Существуют ограничения по расширению пула дисками, в идеале расширение должно производиться увеличивая емкость пула вдвое (или, хотя бы, кратно количеству дисков в RAID-группе пула) </li>
<li>Для пула недоступны reallocate/rebalancing (для MetaLUN вы можете сделать restripe) </li>
<li>Процесс Tresspassing pool LUN (обычно при переключении LUN-а с одного контроллера на другой, например при выходе одного из них из строя, но, часто, и при других операциях) может приводить к снижению производительности, так как оба контроллера будут пытаться совершать операции ввода-вывода на LUN. Поэтому для pool LUN-ов важно оставаться закрепленными за тем контроллером, на котором они начали работать, иначе потребуется затратная миграция. </li>
<li>Не рекомендуется использовать thin LUN для задач, требующих высокой производительности по bandwidth. </li>
</ul>
<p>Я хочу еще раз привлечь внимание к простому факту: дьявол кроется в деталях. Читайте мелкий шрифт.</p>
<p>Вам говорят: Autotiering волшебным образом, автоматически, решит ваши проблемы, без вашего участия, не беспокойтесь об этом. В реальности все не совсем так.</p>
<p>При работе autotiering, значительная часть вашего working set, рабочего набора данных, находящегося в активном использовании в данный момент, должно быть перемещено на быстрое хранилище.</p>
<p>Допустим у вас есть хранилище ваших данных, емкостью 50TB. Правило оценки, которым руководствуются инженеры EMC,&#160; что 5% рабочего набора данных пользователя – “горячие данные”. Они перемещаются на SSD (в виде tier или cache). Таким образом вам нужно 2,5TB usable space на SSD, или примерно одна полку дисками SSD по 200GB, может быть больше, в зависимости от типа использованного RAID.</p>
<p>Принято считать, что объем “средней” нагрузки составляет 20% от объема, то есть 10TB, который размещается на дисках SAS или FC.</p>
<p>Остальное размещается на дисках SATA.</p>
<p><strong>Вопрос на 10 миллионов долларов:</strong></p>
<p>Что обойдется дешевле, autotiering и софт кэширования (он небесплатен) плюс 2,5TB SSD, плюс 10TB SAS, плюс 37,5TB SATA, или…</p>
<p>50TB SATA плюс NetApp FlashCache,или, например, 50TB SAS и Flash Cache?</p>
<p><strong>Вопрос на 20 миллионов долларов:</strong></p>
<p>Какая из этих двух конфигураций будет иметь более предсказуемую производительность?</p>
<p>&#160;</p>
<p>Compellent – это еще одна интересная история.</p>
<p>Большинство обсуждающих Compellent не задумывается о том, что tiering-у у него подвергаются “снэпшоты”, а не непосредственно рабочие данные!</p>
<p>То есть принцип там такой: берется снэпшот, данные делятся на страницы, размеров 2MB (по умолчанию, может быть меньше, но тогда нельзя будет увеличить емкость хранилища). Далее, если оценивается, что обращений на чтение с данной страницы мало, то она переносится на уровень SATA.</p>
<p>О чем не знают большинство интересующихся Compellent-ом:</p>
<p>Если вы изменяете содержимое данных на странице, то происходит это следующим образом:</p>
<ol>
<li>Перенесенная на SATA страница, содержащая данные, которые мы изменяем, остается на SATA.</li>
<li>Новая страница, объемом 2MB создается на Tier1 (SAS/FC), куда реплицируется содержимое страницы с SATA, и где делается запись изменения. Даже если меняется в данных один байт.</li>
<li>Когда с этой страницы будет сделан снэпшот, то он, в свою очередь, также может быть впоследствии перенесен на SATA, заменив прежнюю.</li>
<li>??того: 4MB занятого места для того, чтобы сохранить 2MB данных и один измененный байт.</li>
</ol>
<p>?? снова укажу: дьявол кроется в деталях. Если вы изменяете свои данные произвольным образом (random), вы получите множество “заснэпшоченных” страниц, и очень неэффективное использование пространства. Вот почему я всегда предупреждаю пользователей, которые интересуются Compellent-ом, задавать эти вопросы, и уяснить себе эти моменты, получив ясное описание от инженеров того, как используется пространство снэпшотов.</p>
<p>На NetApp мы имеем предельно гранулярное пространство, благодаря WAFL. Минимально возможный снэпшот по своему объему очень мал (это указатель, немного метаданных, плюс один измененный блок, размером 4KB). Поэтому на NetApp некоторые наши пользователи могут хранить сотни тысяч&#160; снэпшотов на одной системе (например именно так делает один всем известный банк, использующий наши системы хранения).</p>
<p>&#160;</p>
<p>Гранулярность, на самом деле, это только часть проблемы (производительность – другая ее часть). Сейчас страница у Compellent имеет размер 2MB (можно уменьшить до 512K, но это не позволит изменять размер стораджа). Если они перейдут, как обещают, на 64-битную арифметику в ПО, то они смогут получит&#160; размер страницы 64K (это пока не подтверждено), однако тут есть вот какая проблема. Запись этой страницы на RAID может быть двумя способами.</p>
<p>Если это RAID-1, то тогда мы записываем две копии страницы, размером 64KB на каждый из дисков.</p>
<p>Если это RAID-5/6, то тогда нам надо поделить объем записываемой страницы, размером 64KB, поровну между всеми дисками данных, входящих в RAID. Допустим используется RAID-5 в варианте 4d+1p. Тогда на каждый диск в операции записи получится всего 16KB (и меньше). ?? если для RAID-1 размер записи в 64KB это довольно типичный размер записи сегмента в RAID, и запись таким размером достаточно производительна, то для RAID-5/6 это очень маленький кусочек для операции записи, что будет неизбежно отражаться на производительности.</p>
<p>В Data ONTAP мы не перемещаем снэпшоты, поэтому у нас нет такой проблемы, у других же вендоров она очень остра. Получить предсказуемую производительность при использовании autotiering это очень, очень сложная задача. Всякий раз когда мы у клиентов меняем нашими системами сторадж от Compellent, это происходит не потому что им не хватает каких-то фич, а только оттого, что у клиентов с ними проблемы с производительностью. Всякий раз.</p>
<p>Мы ставим 1-2 вида дисков плюс Flash Cache, и проблема решена (в большинстве случаев производительность становится в 2-3 раза выше, как минимум). Часто это получается даже не дороже.</p>
<p>Вот такие дела.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../autotiering.html" rel="tag">autotiering</a>, <a href="../../compellent.html" rel="tag">compellent</a>, <a href="../../fast.html" rel="tag">fast</a>, <a href="../../fastcache.html" rel="tag">fastcache</a>, <a href="../../flash-cache.html" rel="tag">flash cache</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>,  <a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;" rel="category tag">переводы</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../951/trackback.html#respond" title="Комментарий к записи Еще немного про autotiering">Комментарий</a>									 </div>
			</div>
	
						
			<div class="post" id="post-936">
				<h2 class="posttitle"><a href="../../../936/trackback.html" rel="bookmark" title="Permanent Link to EMC FASTcache и NetApp Flash Cache">EMC FASTcache и NetApp Flash Cache</a></h2>
				<div class="postmetadata">23 Июнь 2011, 7:00 <!-- от  --></div>
				<div class="postentry">
					<p>Как мне тут не раз уже попеняли, некорректно сравнивать <em>tiering-as-datamoving</em> и <em>tiering-as-caching</em>, то есть, например, NetApp Flash Cache и EMC FAST VP. Допустим, как я ни старался в <a href="../../../926/trackback.html">соответствующей статье</a>, я вас не убедил, что обе эти формы повышения эффективности системы хранения для пользователя есть суть одно. </p>
<p>Хорошо, давайте рассмотрим отдельно особенности, достоинства и недостатки только Flash Cache (PAM-II) и FASTcache.</p>
<p>Во первых, конечно, вы бы могли вместе со мной поехдничать о извилистом пути достижения цели у EMC. Сперва превратить flash-память в &quot;диски&quot; в форме SSD, а затем из этих дисков эмулировать &quot;память&quot; кэша. Ну ладно, допустим не смогли напрямую, оказалось проще через &quot;двойную эмуляцию&quot;.</p>
<p>Во-вторых если мы посмотрим на то, как у EMC предполагается использовать SSD диски под FASTcache, вы, уверен, вместе со мной поразитесь неффективности.</p>
<p>Допустим, мы разворачиваем систему хранения для 1000 рабочих мест под десктопную виртуализацию на XenDesktop. <a href="http://itzikr.wordpress.com/2011/06/08/citrix-xendesktopp-5-on-emc-vnx-match-made-in-heaven-part1/">Рекомендуемая схема</a> включает в себя три диска SSD, из которых один - hotspare, а два других образуют &quot;зеркало&quot; RAID-1. Таким образом, очевидно, что эффективность использования flash в такой конструкции будет примерно 33%, то есть одна треть от купленной емкости flash. Да, при увеличении объема FASTcache, кроме роста цены будет расти и эффективность использования, но она никогда не превысит 50%, за счет использования RAID-1 (плюс hotspare). Больше половины затраченных на SSD денег будут простаивать. По контрасту, если вы покупаете 256GB Flash Cache, вы можете использовать под кэширование и ускорение работы с данными все 256GB, сто процентов от затраченных на них денег.</p>
<p>В третьих, стоит обратит внимание, что использование SSD как дисков вынуждает EMC разместить этот кэш &quot;снаружи&quot; контроллера, в &quot;петле&quot; дискового ввода-вывода на интерфейсе SAS. В то время, как у NetApp плата Flash Cache располагается непосредственно на системной шине контроллера, на PCIe (PCIe v2.0 x8 в моделях 3200/6200, пропускная способность 32Gbit/s в каждом направлении). То есть взаимодействие контроллера с кэшем в случае FASTcache такое: данные пришли через ввод-вывод на контроллер, по каналу SAS к дискам, вышли через другой порт и записались на SSD по интерфейсу SAS. Затем, если к данным кэша обращение, они должны считаться через дисковый канал ввода-вывода по SAS обратно в контроллер, и отдаться через третий канал ввода-вывода, собственно инициатору запроса, по FC или iSCSI, или NFS/CIFS. Все это, безусловно, нагружает и так не бесконечные возможности дискового канала ввода-вывода к полкам, и, потенциально, может привести к ограничению производительности.</p>
<p>Наконец, стоит помнить, что, хотя в FASTcache удалось значительно снизить размер оперируемого &quot;чанка&quot; до 64KB, против гигабайта в FAST-&quot;просто&quot;, все же этот размер достаточно велик для многих задач, работающих в random read/write, размер блока кэша, значительно превышающий рабочий для соответствующей файловой системы или задачи, например блока базы данных, также снижает эффективность использования такого кэша, если, допустим, только 4KB из блока в 64KB нам действительно нужны (при 100% random это довольно обычная ситуация), то это значит, что эффективность кэша составляет лишь 1/16 от своего фактического объема.</p>
<p>Что же в плюсах? Очевидно, что это активно &quot;педалируемая&quot; EMC возможность работы такого кэша на запись. Особенно на это нажимают в сравнении с NetApp Flash Cache, который на запись не работает, и эта возможность действительно производит впечатление на тех людей, которые не особенно разбираются как там у NetApp все устроено, и знают только то, что &quot;что-то иметь это гораздо лучше чем не иметь&quot;, и уж все знают, что запись надо кэшировать, без кэширования запись на диски очень медленная, это знает даже начинающий пользователь, впервые покупающий кэш-контроллер в сервер.</p>
<p>Чем прежде всего занимается кэш на запись?   <br />Давайте рассмотрим на примере.</p>
<p>Когда клиент записывает блок данных на систему хранения, то, при наличии кэша на запись, этот блок максимально быстро в него помещается, и клиенту отдается сообщение “OK, принято”, клиент не ждет физического помещения блока в сектор жесткого диска, а может продолжать работать, словно записываемый блок уже записан, например отправить следующий блок.    <br />Запись ускоряется путем ее буферизации, и, в дальнейшем, сортировкой и упорядочением процессов записи.</p>
<p>Хотя современный жесткий диск и позволяет обращаться к произвольным фрагментам записанной на него информации, чем отличается от, например, магнитной ленты, которая позволяет считывать и записывать ее только последовательно, однако не позволяет эту произвольно размещенную (random) информацию считывать и записывать _одновременно_, поскольку это физически ограничено возможностями магнитных головок диска.</p>
<p>Если у нас есть жесткий диск, и мы не используем кэширование, то три клиента, пишущих на этот диск, будут вынуждены выстроиться в очередь. Сперва диск спозиционирует головки и дождется подхода нужного сектора для записи блока данных клиента A, а пославшие на запись свой блок клиенты B и C будут вынуждены ждать, затем головки будут переставлены в новое место, диск дождется, когда мимо головок проедет блок, который требуется перезаписать клиенту B, и перезапишет его, а если за это время у клиента A появился еще один блок на запись, то он поставится в очередь следом за блоком клиента C, и будет ожидать, пока выполнятся все операции перед ним.</p>
<p>Все эти процессы, как вы понимаете, механические, неспешны, и занимают не микросекунды, как операции в памяти, а миллисекунды, иногда десятки миллисекунд. Несмотря на то, что жесткий диск - устройство произвольной записи, эти произвольные записи не могут быть осуществлены одновременно.</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fca970c-pi" /></p>
<p>Ради создания хотя бы иллюзии одновременности и организуется кэш на запись. Приложения записывают свои данные в кэш записи, получают сразу же ответ &quot;готово&quot; и идут сочинять новые блоки на запись, не дожидаясь того, что данные физически поместятся на диск.</p>
<p>Кэш же внутри себя удерживает блок, ожидая подходящего момента на запись, а также оптимизирует записи, с тем, чтобы уменьшить &quot;пробег&quot; магнитных головок по диску, и возможно оптимальным образом перекомпонует записи так, чтобы уложить их максимально эффективным, с точки зрения head seek-а, способом.</p>
<p>Принципиальное отличие WAFL тут состоит в том, что WAFL не перезаписывает блоки уже записанные на диске, и значит при записи клиенты гораздо меньше ожидают seek-а. Вы помните, что в WAFL записи можно провести &quot;чохом&quot;, в выделенный сегмент, а не переписывать по одному блоку, мечась по всему диску, здесь и там, ожидая, когда подъедет под головку тот или иной блок. Поэтому даже без традиционного кэширования записи на WAFL клиентские записи быстро оказываются на дисках.</p>
<p>Строго говоря, большой кэш означает, что, используя транспортную аналогию, записи быстро встают в очередь на посадку, но совсем не значит, что при этом они быстро сядут и поедут.</p>
<p>WAFL оптимизирован именно на минимальное время от момента прихода данных &quot;на остановку&quot; и до входа их &quot;в автобус&quot; жесткого диска, превращая записи в записи последовательного порядка (sequental) из поступающих записей в произвольном порядке (random).</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef0133f2735a39970b-pi" width="467" height="307" /></p>
<p>Результат хорошо иллюстрируется <a href="http://blogs.netapp.com/dropzone/2010/07/does-your-data-wait-in-line.html">экспериментом</a>, в котором один aggregate, состоящий из трех дисков SATA 1TB 7200rpm, в RAID-DP (2p+1d), то есть, фактически, из одного действующего диска, показывает при random write блоком 4KB не типичные для SATA 70-80 IOPS, а более 4600!</p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485986fe9970c-pi" /></p>
<p><img src="http://blogs.netapp.com/.a/6a00d8341ca27e53ef013485987007970c-pi" /></p>
<p>Объяснение этому простое - записи, поступающие на диск теряют свою &quot;рандомность&quot;, и &quot;секвентализируются&quot;. Около четырех с половиной тысяч IOPS random-записи на один диск SATA - это как раз то, отчего в системах NetApp нет свойственной для &quot;классических систем&quot; острой необходимости в кэше записи на уровне контроллера.</p>
<p>Таким образом запись действительно надо кэшировать для &quot;классических&quot; систем, да, безусловно, это так. Но это совсем не так безусловно для &quot;неклассических&quot; дисковых структур, используемых, например, в NetApp.</p>
<p>Вот почему NetApp Flash Cache не используется на запись данных, работая всем своим объемом только на чтение. Вот почему необходимость кэширования записи для WAFL не столь безоговорочна, как для <em>&quot;классических&quot;</em> дисковых структур.</p>
<p>Потому, что в WAFL необходимость обязательного кэширования данных при записи существенно ниже, чем для &quot;традиционных&quot; систем. </p>
<p>Это позволило, среди прочего, кстати, значительно упростить алгоритмическую составляющую процесса кэширования, ведь не секрет, что правильная обработка кэширования на запись часто создает значительные проблемы, особенно в наиболее эффективном режиме write-back.</p>
<p>А это, в свою очередь, снизило количество возможных ошибок (как программного кода реализации, так и ошибок при использовании) удешевило устройство и упростило использование.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../cache.html" rel="tag">cache</a>, <a href="../../emc/index.html" rel="tag">emc</a>, <a href="../../fastcache.html" rel="tag">fastcache</a>, <a href="../../flash/index.html" rel="tag">flash</a>, <a href="../../flashcache.html" rel="tag">flashcache</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../936/trackback.html#comments" title="Комментарий к записи EMC FASTcache и NetApp Flash Cache">Комментарии (25)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-669">
				<h2 class="posttitle"><a href="../../../669/trackback.html" rel="bookmark" title="Permanent Link to Частота отказов в SSD: реальные данные из жизни">Частота отказов в SSD: реальные данные из жизни</a></h2>
				<div class="postmetadata">30 Август 2010, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>В группе LinkedIn “Storage Professionals” (кстати рекомендую обратить внимание на существование дискуссионных групп в LinkedIn, бывает интересно) вот уже которую неделю обсуждается тема:   <br /><strong>SSD drives failure rates</strong></p>
<p>Некоторые цитаты оттуда, которые я приведу без перевода, благо все понятно (каждый абзац – цитата-фрагмент из сообщения отдельного человека в данном треде).</p>
<p><em>I&#8217;m working as a contractor at a bank in the midwest and we have SSD&#8217;s in EMC VMAX&#8217;s for about 9 months. We haven&#8217;t seen any failures yet</em></p>
<p><em>I once ran a multi week attempt to burn out various vendors&#8217; SSDs. I ran them flat out 100% random writes for about a month. Fusion IOs at something like 30k IOPs per drive, STECs / Intels around 7k. Never was able to get any of them to fail.     <br />The Fusion IO did as many writes that month as a single SAS drive could do in over a decade. </em></p>
<p><em>We have approximately 150 SSD drives and have seen 1 failure during the past 12 months.</em></p>
<p><em>I&#8217;ve been using SSDs in a cx4-960 clariion for just under 12 months with no failures ( covering large ms sql tempdb).      <br />From my own experience ( first shipped SSD systems 2 and half years ago), SLC SSD failure rate is in the same range as rotating drives.</em></p>
<p>Вот такие дела. Есть над чем подумать тем кто до сих пор считает, что ресурс SSD на запись <em>ужасно ограничен</em>, что SSD ненадежен, и при работе Enterprise Flash Drives дохнет как паленая китайская USB-флешка <em>Kinqston.</em></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../index.html" rel="tag">ssd</a><br />					Раздел: <a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;" rel="category tag">justread</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../669/trackback.html#comments" title="Комментарий к записи Частота отказов в SSD: реальные данные из жизни">Комментарии (10)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-599">
				<h2 class="posttitle"><a href="../../../599/trackback.html" rel="bookmark" title="Permanent Link to PAM - Performance Acceleration Module">PAM - Performance Acceleration Module</a></h2>
				<div class="postmetadata">10 Май 2010, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Вот уже пару лет как у NetApp в номенклатуре продуктов находится интересный, но все еще не слишком известный широкому кругу пользователей продукт – <strong>PAM – Performance Acceleration Module</strong>, а в прошлом году к нему в компанию добавился еще один вариант – <strong>PAM-II</strong> (ныне <strong>Flash Cache</strong>). </p>
<p>Давайте разберемся подробнее что это, чем полезно и как применяется. </p>
<p>Первое, что следует понимать, чтобы разобраться в том, что есть PAM и как его применяют:    <br /><strong>PAM это не SSD!</strong></p>
<p><strong><img style="border-bottom: 0px; border-left: 0px; display: inline; border-top: 0px; border-right: 0px" title="PAMII" border="0" alt="PAMII" src="/pics//pamii1.jpg" width="470" height="246" /> </strong></p>
<p>Широкое распространение SSD (даже этот пост пишется на ноутбуке с SSD) привело к тому, что любой продукт так или иначе использующий память для хранения данных называется “SSD”.</p>
<p>Давайте разберемся, что такое SSD, и чем PAM <em><strong>не SSD</strong></em>.</p>
<p>  <a href="../../../599/trackback.html#more-599" class="more-link"><small>Continue reading &#8216;PAM - Performance Acceleration Module&#8217; &raquo;</small></a></p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../flash-cache.html" rel="tag">flash cache</a>, <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../pam.html" rel="tag">pam</a>, <a href="../index.html" rel="tag">ssd</a>, <a href="../../techtalk/index.html" rel="tag">techtalk</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../599/trackback.html#comments" title="Комментарий к записи PAM - Performance Acceleration Module">Комментарии (16)</a>									 </div>
			</div>
	
						
			<div class="post" id="post-291">
				<h2 class="posttitle"><a href="../../../291/trackback.html" rel="bookmark" title="Permanent Link to Почему NetApp до сих пор не использует SSD?">Почему NetApp до сих пор не использует SSD?</a></h2>
				<div class="postmetadata">4 Июнь 2009, 8:00 <!-- от  --></div>
				<div class="postentry">
					<p>Несомненно одна из &#8220;горячих тем&#8221; 2008-09 года это SSD - Solid State Disks - &#8220;твердотельные&#8221; диски на технологии Flash. Они появились повсеместно, от недорогих нетбуков &#8220;до 300$&#8221; до дорогих серверных систем. В прошлом году использование SSD в системах хранения данных было анонсировано EMC для их линейки Symmetrix.<br />
Часто приходится отвечать на вопросы: &#8220;А что же NetApp не реагирует, и не поддержит свое реноме передовой инновационной инженерной компании? Где же у NetApp SSD?&#8221;</p>
<p>А NetApp, как всегда, движется своим путем. </p>
<p>Что есть SSD? SSD это flash, знакомый нам уже много лет, но организованный таким образом, чтобы &#8220;обманывать&#8221; прочие устройства, чтобы те думали, что они работают с обычным HDD.<br />
Этакий аппаратный &#8220;эмулятор HDD&#8221;. Кроме этого чем SSD отличается от знакомых нам, уже сто лет как, USB-&#8221;брелков&#8221;? Да по сути ничем. Ну да, SATA это в принципе более производительная шина, чем USB2.0. Да, в современных контроллерах Flash используется чередование и <a href="../../../204/trackback.html">wear-leveling</a>, но все то же самое используется и в современных высокоскоростных USB-&#8221;флешках&#8221;.</p>
<p>То есть в чем инновационность SSD? Только в том, что мы можем ставить его в те, ранее выпущенные устройства, которые знают и умеют работать только с жесткими дисками.<br />
Некая аналогия с <noindex><nofollow><a href="http://www.netapp.com/us/products/storage-systems/netapp-vtl/">VTL - Virtual Tape Library</a></nofollow></noindex>. Мы можем поставить их там, где софт умеет работать только с ленточными библиотеками, как, например, какние-нибудь запрограммированные на Коболе мэйнфреймы 70-80-х годов. ?? при этом нам не надо ничего менять на стороне остальной системы.<br />
Но в том случае, когда нас не заботит &#8220;обратная совместимость&#8221; с прежним оборудованием, если мы можем создать IT-систему с нуля, тогда нам, скорее всего, незачем эмулировать поведение ленточных библиотек, мы можем поддерживать диски нативно.<br />
По моему наблюдению, именно это причина плохих продаж VTL в России, по сравнению со всем миром, где VTL совершенно явные фавориты, выпускаемые многими вендорами дисковых систем.<br />
В России просто незначительна проблема &#8220;унаследованного оборудования&#8221; и &#8220;унаследованных решений&#8221; (legacy solutions), и проблема совместимости для &#8220;начисто&#8221; создаваемой IT-инфраструктуры минимальна.<br />
Конечно конкретно NetApp VTL это не только эмуляция, но также множество других, зачастую уникальных фич, таких как Direct Tape Creation и дедупликация, но в основном все так.</p>
<p>Таким образом, EMC решило эту задачу минимально затратным и наименее &#8220;умным&#8221; способом, просто сэмулировав на высокоскоростных Flash-устройствах обычные диски, подобно тому, как VTL эмулирует &#8220;ленточные библиотеки&#8221; на дисковых массивах.<br />
Если здоровый человек хорошо размахнется и ударит отбойным молотком, то, конечно, сможет использовать его в качестве обычной кувалды, но нативное его применение может быть гораздо эффективнее. </p>
<p>Возможен ли другой путь? Очевидно, что да.<br />
Эмуляция дисков не единственный способ использования flash-памяти.</p>
<p>Вот, что <noindex><nofollow><a href="http://blogs.netapp.com/extensible_netapp/2008/12/flash-and-the-n.html">пишет </a></nofollow></noindex>у себя в блоге уже знакомый вам инженер-разработчик NetApp Костадис Руссос:</p>
<blockquote><p>
&#8220;Существует три возможных варианта ситуации с доступом к датасету:<br />
1. Низкий уровень IOPS, малая нагрузка<br />
2. Высокая нагрузка по IOPS, когда датасет помещается в кэш<br />
3. Высокая нагрузка по IOPS, когда датасет НЕ помещается в кэш</p>
<p>Очевидно, что из этих трех сценариев только третий - кандидат на использование Flash-дисков.&#8221;
</p>
</blockquote>
<p>Однако NetApp выбрал иной путь, создав <noindex><nofollow><a href="http://www.netapp.com/us/products/storage-systems/performance-acceleration-module/">PAM - Performance Acceleration Module</a></nofollow></noindex>, модуль расширения кэша. Своеобразный &#8220;SSD&#8221;, но не в виде эмуляции дисков, как принято сейчас делать, а на уровне архитектуры системы в целом.</p>
<p><noindex><nofollow><a href="http://blogs.netapp.com/extensible_netapp/2009/06/15k-rpm-drives-not-dead-yet.html">Он же</a></nofollow></noindex>:</p>
<blockquote><p>
Практика показывает, что большинство данных это так называемые &#8220;холодные данные&#8221; (то есть данные, уровень обращений к которым невысок). Таким образом платить за высокую производительность для &#8220;холодных данных&#8221; есть очень дорогостоящее решение. Но если система хранения обеспечивает высокую производительность работы для &#8220;горячих&#8221; данных, даже если они при этом лежат на медленном хранилище, то цена Flash, как среды хранения данных, в таком случае значительно уменьшается.<br />
Другими словами, да, жесткие диски 15KRPM не обеспечивают исключительно высокой производительности, но они обеспечивают достаточный уровень, для достаточного объема данных, оставляя для SSD Flash нишу.
</p>
</blockquote>
<p>Я довольно давно собираюсь развернуто написать про PAM и его устройство, те, кто был в этом году на NetApp Innovation 2009 наверняка слышал рассказ специалиста московского отделения компании, Романа Ройфмана, о Performance Acceleration Module.</p>
<p>Надеюсь в скором времени и я напишу подробный рассказ про PAM для читателей этого блога.</p>
				</div>
		
				<div class="postmetadata">
					Метки: <a href="../../netapp/index.html" rel="tag">netapp</a>, <a href="../../performance/index.html" rel="tag">performance</a>, <a href="../index.html" rel="tag">ssd</a>, <a href="../../techtalks.html" rel="tag">techtalks</a><br />					Раздел: <a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;" rel="category tag">techtalk</a>&nbsp;&nbsp;|&nbsp;
					<a href="../../../291/trackback.html#comments" title="Комментарий к записи Почему NetApp до сих пор не использует SSD?">Комментарии (3)</a>									 </div>
			</div>
	
		
		<div class="navigation">
			<div class="alignleft"></div>
			<div class="alignright"><a href="../index.html">Next Entries &raquo;</a></div>
		</div>
		
	
	</div>
	<div id="sidebar">
		<ul>
			
			
			<!-- Author information is disabled per default. Uncomment and fill in your details if you want to use it.
			<li><h2>Автор</h2>
			<p>A little something about you, the author. Nothing lengthy, just an overview.</p>
			</li>
			-->

			<li class="pagenav"><h2>Страницы</h2><ul><li class="page_item page-item-153"><a href="../../../../about/trackback.html" title="about">about</a></li>
<li class="page_item page-item-215"><a href="../../../../distributory-v-rossii/trackback.html" title="Дистрибуторы в России">Дистрибуторы в России</a></li>
<li class="page_item page-item-1327"><a href="../../../../disti-ua/trackback.html" title="Дистрибуторы в Украине">Дистрибуторы в Украине</a></li>
</ul></li>
			<li><h2>Рубрики</h2>
				<ul>
					<li class="cat-item cat-item-89"><a href="../../../category/commands/index.html" title="Просмотреть все записи в рубрике &laquo;commands&raquo;">commands</a>
</li>
	<li class="cat-item cat-item-37"><a href="../../../category/howto/index.html" title="Просмотреть все записи в рубрике &laquo;howto&raquo;">howto</a>
</li>
	<li class="cat-item cat-item-52"><a href="../../../category/justread/index.html" title="Просмотреть все записи в рубрике &laquo;justread&raquo;">justread</a>
</li>
	<li class="cat-item cat-item-51"><a href="../../../category/review/index.html" title="Просмотреть все записи в рубрике &laquo;review&raquo;">review</a>
</li>
	<li class="cat-item cat-item-3"><a href="../../../category/techtalk/index.html" title="Просмотреть все записи в рубрике &laquo;techtalk&raquo;">techtalk</a>
</li>
	<li class="cat-item cat-item-71"><a href="../../../category/tricks/index.html" title="Просмотреть все записи в рубрике &laquo;tricks&raquo;">tricks</a>
</li>
	<li class="cat-item cat-item-95"><a href="../../../category/utilities/index.html" title="Просмотреть все записи в рубрике &laquo;utilities&raquo;">utilities</a>
</li>
	<li class="cat-item cat-item-44"><a href="../../../category/whoisho/index.html" title="Просмотреть все записи в рубрике &laquo;whoisho&raquo;">whoisho</a>
</li>
	<li class="cat-item cat-item-1"><a href="../../../category/news/index.html" title="Просмотреть все записи в рубрике &laquo;новости&raquo;">новости</a>
</li>
	<li class="cat-item cat-item-387"><a href="../../../category/opros.html" title="Просмотреть все записи в рубрике &laquo;опрос&raquo;">опрос</a>
</li>
	<li class="cat-item cat-item-8"><a href="../../../category/translations/index.html" title="Просмотреть все записи в рубрике &laquo;переводы&raquo;">переводы</a>
</li>
	<li class="cat-item cat-item-40"><a href="../../../category/citaty/index.html" title="Просмотреть все записи в рубрике &laquo;цитаты&raquo;">цитаты</a>
</li>
				</ul>
			</li>

			<li><h2>Архивы</h2>
				<ul>
					<li><a href='../../../date/2018/01.html' title='Январь 2018'>Январь 2018</a></li>
	<li><a href='../../../date/2015/10.html' title='Октябрь 2015'>Октябрь 2015</a></li>
	<li><a href='../../../date/2015/04.html' title='Апрель 2015'>Апрель 2015</a></li>
	<li><a href='../../../date/2015/03.html' title='Март 2015'>Март 2015</a></li>
	<li><a href='../../../date/2015/01.html' title='Январь 2015'>Январь 2015</a></li>
	<li><a href='../../../date/2014/12.html' title='Декабрь 2014'>Декабрь 2014</a></li>
	<li><a href='../../../date/2014/11.html' title='Ноябрь 2014'>Ноябрь 2014</a></li>
	<li><a href='../../../date/2014/10.html' title='Октябрь 2014'>Октябрь 2014</a></li>
	<li><a href='../../../date/2014/09.html' title='Сентябрь 2014'>Сентябрь 2014</a></li>
	<li><a href='../../../date/2014/08.html' title='Август 2014'>Август 2014</a></li>
	<li><a href='../../../date/2014/07.html' title='Июль 2014'>Июль 2014</a></li>
	<li><a href='../../../date/2014/06.html' title='Июнь 2014'>Июнь 2014</a></li>
	<li><a href='../../../date/2014/05.html' title='Май 2014'>Май 2014</a></li>
	<li><a href='../../../date/2014/04.html' title='Апрель 2014'>Апрель 2014</a></li>
	<li><a href='../../../date/2014/03.html' title='Март 2014'>Март 2014</a></li>
	<li><a href='../../../date/2014/02.html' title='Февраль 2014'>Февраль 2014</a></li>
	<li><a href='../../../date/2014/01.html' title='Январь 2014'>Январь 2014</a></li>
	<li><a href='../../../date/2013/12.html' title='Декабрь 2013'>Декабрь 2013</a></li>
	<li><a href='../../../date/2013/11.html' title='Ноябрь 2013'>Ноябрь 2013</a></li>
	<li><a href='../../../date/2013/10.html' title='Октябрь 2013'>Октябрь 2013</a></li>
	<li><a href='../../../date/2013/09.html' title='Сентябрь 2013'>Сентябрь 2013</a></li>
	<li><a href='../../../date/2013/08.html' title='Август 2013'>Август 2013</a></li>
	<li><a href='../../../date/2013/07.html' title='Июль 2013'>Июль 2013</a></li>
	<li><a href='../../../date/2013/06.html' title='Июнь 2013'>Июнь 2013</a></li>
	<li><a href='../../../date/2013/05.html' title='Май 2013'>Май 2013</a></li>
	<li><a href='../../../date/2013/04.html' title='Апрель 2013'>Апрель 2013</a></li>
	<li><a href='../../../date/2013/03.html' title='Март 2013'>Март 2013</a></li>
	<li><a href='../../../date/2013/02.html' title='Февраль 2013'>Февраль 2013</a></li>
	<li><a href='../../../date/2013/01.html' title='Январь 2013'>Январь 2013</a></li>
	<li><a href='../../../date/2012/12.html' title='Декабрь 2012'>Декабрь 2012</a></li>
	<li><a href='../../../date/2012/11.html' title='Ноябрь 2012'>Ноябрь 2012</a></li>
	<li><a href='../../../date/2012/10.html' title='Октябрь 2012'>Октябрь 2012</a></li>
	<li><a href='../../../date/2012/09.html' title='Сентябрь 2012'>Сентябрь 2012</a></li>
	<li><a href='../../../date/2012/08.html' title='Август 2012'>Август 2012</a></li>
	<li><a href='../../../date/2012/07.html' title='Июль 2012'>Июль 2012</a></li>
	<li><a href='../../../date/2012/06.html' title='Июнь 2012'>Июнь 2012</a></li>
	<li><a href='../../../date/2012/05.html' title='Май 2012'>Май 2012</a></li>
	<li><a href='../../../date/2012/04.html' title='Апрель 2012'>Апрель 2012</a></li>
	<li><a href='../../../date/2012/03.html' title='Март 2012'>Март 2012</a></li>
	<li><a href='../../../date/2012/02.html' title='Февраль 2012'>Февраль 2012</a></li>
	<li><a href='../../../date/2012/01.html' title='Январь 2012'>Январь 2012</a></li>
	<li><a href='../../../date/2011/12.html' title='Декабрь 2011'>Декабрь 2011</a></li>
	<li><a href='../../../date/2011/11.html' title='Ноябрь 2011'>Ноябрь 2011</a></li>
	<li><a href='../../../date/2011/10/index.html' title='Октябрь 2011'>Октябрь 2011</a></li>
	<li><a href='../../../date/2011/09/index.html' title='Сентябрь 2011'>Сентябрь 2011</a></li>
	<li><a href='../../../date/2011/08.html' title='Август 2011'>Август 2011</a></li>
	<li><a href='../../../date/2011/07/index.html' title='Июль 2011'>Июль 2011</a></li>
	<li><a href='../../../date/2011/06/index.html' title='Июнь 2011'>Июнь 2011</a></li>
	<li><a href='../../../date/2011/05/index.html' title='Май 2011'>Май 2011</a></li>
	<li><a href='../../../date/2011/04/index.html' title='Апрель 2011'>Апрель 2011</a></li>
	<li><a href='../../../date/2011/03/index.html' title='Март 2011'>Март 2011</a></li>
	<li><a href='../../../date/2011/02.html' title='Февраль 2011'>Февраль 2011</a></li>
	<li><a href='../../../date/2011/01.html' title='Январь 2011'>Январь 2011</a></li>
	<li><a href='../../../date/2010/12.html' title='Декабрь 2010'>Декабрь 2010</a></li>
	<li><a href='../../../date/2010/11/index.html' title='Ноябрь 2010'>Ноябрь 2010</a></li>
	<li><a href='../../../date/2010/10/index.html' title='Октябрь 2010'>Октябрь 2010</a></li>
	<li><a href='../../../date/2010/09/index.html' title='Сентябрь 2010'>Сентябрь 2010</a></li>
	<li><a href='../../../date/2010/08.html' title='Август 2010'>Август 2010</a></li>
	<li><a href='../../../date/2010/07/index.html' title='Июль 2010'>Июль 2010</a></li>
	<li><a href='../../../date/2010/06.html' title='Июнь 2010'>Июнь 2010</a></li>
	<li><a href='../../../date/2010/05.html' title='Май 2010'>Май 2010</a></li>
	<li><a href='../../../date/2010/04/index.html' title='Апрель 2010'>Апрель 2010</a></li>
	<li><a href='../../../date/2010/03/index.html' title='Март 2010'>Март 2010</a></li>
	<li><a href='../../../date/2010/02.html' title='Февраль 2010'>Февраль 2010</a></li>
	<li><a href='../../../date/2010/01.html' title='Январь 2010'>Январь 2010</a></li>
	<li><a href='../../../date/2009/12/index.html' title='Декабрь 2009'>Декабрь 2009</a></li>
	<li><a href='../../../date/2009/11/index.html' title='Ноябрь 2009'>Ноябрь 2009</a></li>
	<li><a href='../../../date/2009/10.html' title='Октябрь 2009'>Октябрь 2009</a></li>
	<li><a href='../../../date/2009/09.html' title='Сентябрь 2009'>Сентябрь 2009</a></li>
	<li><a href='../../../date/2009/08/index.html' title='Август 2009'>Август 2009</a></li>
	<li><a href='../../../date/2009/07/index.html' title='Июль 2009'>Июль 2009</a></li>
	<li><a href='../../../date/2009/06.html' title='Июнь 2009'>Июнь 2009</a></li>
	<li><a href='../../../date/2009/05.html' title='Май 2009'>Май 2009</a></li>
	<li><a href='../../../date/2009/04.html' title='Апрель 2009'>Апрель 2009</a></li>
	<li><a href='../../../date/2009/03.html' title='Март 2009'>Март 2009</a></li>
	<li><a href='../../../date/2009/02.html' title='Февраль 2009'>Февраль 2009</a></li>
	<li><a href='../../../date/2009/01.html' title='Январь 2009'>Январь 2009</a></li>
	<li><a href='../../../date/2008/12.html' title='Декабрь 2008'>Декабрь 2008</a></li>
	<li><a href='../../../date/2008/11.html' title='Ноябрь 2008'>Ноябрь 2008</a></li>
	<li><a href='../../../date/2008/10.html' title='Октябрь 2008'>Октябрь 2008</a></li>
	<li><a href='../../../date/2008/09.html' title='Сентябрь 2008'>Сентябрь 2008</a></li>
	<li><a href='../../../date/2008/08.html' title='Август 2008'>Август 2008</a></li>
	<li><a href='../../../date/2008/03.html' title='Март 2008'>Март 2008</a></li>
	<li><a href='../../../date/2008/02.html' title='Февраль 2008'>Февраль 2008</a></li>
	<li><a href='../../../date/2007/12.html' title='Декабрь 2007'>Декабрь 2007</a></li>
	<li><a href='../../../date/2007/11.html' title='Ноябрь 2007'>Ноябрь 2007</a></li>
	<li><a href='../../../date/2007/10.html' title='Октябрь 2007'>Октябрь 2007</a></li>
	<li><a href='../../../date/2007/09.html' title='Сентябрь 2007'>Сентябрь 2007</a></li>
	<li><a href='../../../date/2007/08.html' title='Август 2007'>Август 2007</a></li>
	<li><a href='../../../date/2007/07/index.html' title='Июль 2007'>Июль 2007</a></li>
	<li><a href='../../../date/2007/06.html' title='Июнь 2007'>Июнь 2007</a></li>
	<li><a href='../../../date/2007/05.html' title='Май 2007'>Май 2007</a></li>
				</ul>
			</li>

			
					</ul>
	</div>

</div> <!-- wrapper -->
<div id="footer">
	<a href="../../../../feed">Entries (RSS)</a> and <a href="../../../../comments/feed">Comments (RSS)</a>. Valid <a href="http://validator.w3.org/check/referer" title="This page validates as XHTML 1.0 Transitional"><abbr title="eXtensible HyperText Markup Language">XHTML</abbr></a> and <a href="http://jigsaw.w3.org/css-validator/check/referer"><abbr title="Cascading Style Sheets">CSS</abbr></a>.<br />
	Powered by <a href="http://wordpress.org/" title="Powered by WordPress.">WordPress</a> and <a href="http://srinig.com/wordpress/themes/fluid-blue/">Fluid Blue theme</a>.<br />
	<!-- 15 queries. 0.099 seconds. -->
	</div>
</div> <!-- page -->
</body>
</html>
